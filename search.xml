<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>解决LaTeX不显示参考文献问题</title>
      <link href="/2022/04/30/VScode-LaTex-no-bib/"/>
      <url>/2022/04/30/VScode-LaTex-no-bib/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本人使用的LaTeX环境为：<br>VScode 1.66.2<br>LaTeX-workshop v8.25.0<br>MikTex </p></blockquote><p>由于直接复制网络上他人的教程导致无法适用于自己的环境，bib的reference始终无法正确识别 [在正文中以？出现]，经过不断的尝试，终于找到原因：<br><strong>编译链设置不对</strong><br>网上的普遍配置是大多如下：</p><pre><code class="json">// 用于配置编译链&quot;latex-workshop.latex.recipes&quot;: [    &#123;        &quot;name&quot;: &quot;xelatex&quot;,        &quot;tools&quot;: [            &quot;xelatex&quot;        ]    &#125;,    &#123;        &quot;name&quot;: &quot;latexmk&quot;,        &quot;tools&quot;: [            &quot;latexmk&quot;        ]    &#125;,    &#123;        &quot;name&quot;: &quot;pdflatex-&gt;bibtex-&gt;pdflatex*2&quot;,        &quot;tools&quot;: [            &quot;pdflatex&quot;,            &quot;bibtex&quot;,            &quot;pdflatex&quot;,            &quot;pdflatex&quot;        ]    &#125;],</code></pre><p>当默认以第一种方式编译时，我的环境下是无法识别bib的，解决办法很简单就是使用<code>pdflatex-&gt;bibtex-&gt;pdflatex*2</code>为默认编译链。</p>]]></content>
      
      
      <categories>
          
          <category> VScode使用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VScode.issue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>希尔伯特空间</title>
      <link href="/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4/"/>
      <url>/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4/</url>
      
        <content type="html"><![CDATA[<h3 id="等量变换"><a href="#等量变换" class="headerlink" title="等量变换"></a>等量变换</h3><p><strong>概率场景：</strong></p><p>由于概率分布的积分对应面积为1，所以在微分角度上，两个分布的变化量是相等的，则有：<br>$$<br>p(x)dx&#x3D;p(y)dy \<br>p(y)&#x3D;p(x)\left|\frac{dx}{dy}\right|<br>$$</p><h3 id="Hilbert-Space-希尔伯特空间"><a href="#Hilbert-Space-希尔伯特空间" class="headerlink" title="Hilbert Space 希尔伯特空间"></a>Hilbert Space 希尔伯特空间</h3><p>线性空间（向量空间）关注的是向量的位置，对于一个线性空间，知道<code>基</code>（相当于三维空间中的坐标系）便可确定空间中元素的坐标（即位置）；线性空间只定义了<code>加法和数乘</code>运算。</p><ul><li><p>赋范线性空间：如果我们想知道向量的长度，定义<code>范数</code></p></li><li><p>度量空间：如果我们想知道向量间的距离，定义<code>距离</code></p></li><li><p>内积空间：如果我们想知道向量的夹角，定义<code>内积</code></p></li><li><p>欧式空间：定义了内积的有限维实线性空间</p><blockquote><p>有限维：设$A$是线性空间$E$的一个线性无关子集，我们设$A$的维度为$D_E$。当$D_E&lt; +\infty$时，称$E$为有限维的，否则称$E$为无限维的，即欧式空间中没有无限维的计算的概念  </p></blockquote></li><li><p>完备空间：如果我们想研究收敛性（极限），定义<code>完备</code></p><blockquote><p>完备性：是在极限的基础上衍生的概念。例如在有理数集上的一个序列{1，1.4，1.41，1.414，1.4142…}，可知此序列极限为2根号2，而根号2为无理数，不属于有理数集，即有理数集不具备完备性，也就是有理数集不具备极限的概念，因为有理数集上的数都是确定的</p></blockquote></li><li><p>Banach空间：完备的赋范线性空间</p></li><li><p><kbd>Hilbert空间</kbd>：完备的内积空间（极限运算中不能跑出度量的范围；是欧式空间的一种推广；由于极限计算针对的一般是函数，所以Hilbert空间一般是指函数空间，可以定义函数的內积）</p><blockquote><p>函数的內积：我们有两个函数$f(x)$与$g(x)$与区间$[a,b]$，且两函数在该区间上可积且平方可积。则积分：$\int_a^b f(x)g(x)dx$，我们称之为函数的内积，函数的内积常记作$&lt;f(x),g(x)&gt;$，如果是离散的函数则我们可以直接：$\sum{f(x)\times g(x)}$，用矩阵表示就是$F(X)G(X)$</p></blockquote></li></ul><p>希尔伯特空间是一个完备的空间，其上所有的<code>柯西列</code>等价于<code>收敛列</code>，从而微积分中的大部分概念都可以无障碍地推广到希尔伯特空间中。</p><blockquote><p><strong>柯西序列</strong><br>定义：在具有度量$d$的度量空间$S$中，一个序列为柯西序列，若其符合以下条件：<br>对于任意的实数$\epsilon &gt; 0$，存在一正整数$N$，使得每当$m,n&gt;N$时都有$d(a_m, a_n)&lt;\epsilon$</p><p>在数学中，一个柯西列是指一个这样一个序列，它的元素随着序数的增加而愈发靠近。更确切地说，在去掉有限个元素后，可以使得余下的元素中任何两点间的距离的最大值不超过任意给定的正的常数。</p><p>柯西列的定义依赖于距离的定义，所以只有在度量空间(metric space)中柯西列才有意义。在更一般的一致空间(uniform space)中，可以定义更为抽象的柯西滤子(Cauchy filter)和柯西网(Cauchy net)。</p><p>柯西序列的重要作用是定义“完备空间”。完备空间是指一种度量空间，它的所有柯西序列（如果有的话），都收敛在这个空间自己里面。</p><p>在完备空间（complete space）中，所有的柯西列都有极限，这就让人们可以在不求出这个极限（如果存在）的情况下，利用柯西列的判别法则证明该极限是存在的。柯西列在构造具有完备性的代数结构的过程中也有重要价值，如构造实数。</p><p>1.对于在某度量空间内的柯西序列，它的极限不一定在相同的度量空间内。如有理柯西序列可导出无理极限。（事实上，一种实数构造就是用这种方法）</p><p>2.任何收敛列必然是柯西列，任何柯西列必然是有界序列。</p></blockquote><p>希尔伯特空间为基于任意正交系上的多项式表示的傅立叶级数和傅立叶变换提供了一种有效的表述方式，而这也是泛函分析的核心概念之一。</p><img src="/images/v2-be26b2ba1df2edc9636647a28b22238d_1440w.jpg" alt="空间关系图" style="zoom:80%;" /><p><strong>Reproducing Kernel Hilbert Space再生核希尔伯特空间：</strong></p><p><kbd>Kernel</kbd>任何半正定的函数都可以作为核函数（Merrcer定理：充要条件）</p><blockquote><p><strong>Merrcer定理：</strong>所谓半正定函数$f(x_i, x_j)$，是指拥有训练数据集合$(x_1, x_2,\dots,x_n)$，我们定义一个矩阵的元素$a_{ij}&#x3D;f(x_i, x_j)$，这个矩阵是半正定的，那么$f(x_i, x_j)$就成为半正定的函数。</p></blockquote><p>再生核希尔伯特空间是支持监督学习（SVM）等监督学习模型的理论基础，实际上再生核希尔伯特空间就是是由核函数构成的希尔伯特空间，这里的再生指的是再生性，这里的核函数比如LibSVM中自带的几类：</p><ol><li>线性：$K(v_1,v_2)&#x3D;&lt;v_1,v_2&gt;$</li><li>多项式：$K(v_1,v_2)&#x3D;(\gamma&lt;v_1,v_2&gt;+c)^n$</li><li>高斯核：$K(v_1,v_2)&#x3D;\exp(-\gamma|v_1-v_2|^2)$</li><li>Sigmoid：$K(v_1,v_2)&#x3D;\tanh(\gamma&lt;v_1,v_2&gt;+c)$</li></ol><blockquote><p>设$\mathcal{X}$是输入空间(欧式空间$R^n$的子集或离散集合)，又设$\mathcal{H}$是特征空间(希尔伯特空间)，如果存在一个$\mathcal{X}$到$\mathcal{H}$的映射$\phi(x):\mathcal{X}\rightarrow\mathcal{H}$使得对所有$x,z\in\mathcal{X}$，函数$K(x,z)$满足条件$K(x,z)&#x3D;\phi(x)\cdot\phi(z)$则称$K(x,z)$为核函数，$\phi(x)$为映射函数，式中$\phi(x)\cdot\phi(z)$为$\phi(x)$和$\phi(z)$的內积。</p></blockquote><p>再生性指的就是<strong>原本函数之间计算内积需要算无穷维的积分（也就是这个映射函数可以映射到高维甚至无穷维（高斯核），而计算无穷维的积分是非常复杂的），但是现在只需要算核函数可以。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 专题学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>科学空间笔记</title>
      <link href="/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E7%A7%91%E5%AD%A6%E7%A9%BA%E9%97%B4%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E7%A7%91%E5%AD%A6%E7%A9%BA%E9%97%B4%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="关于VAE"><a href="#关于VAE" class="headerlink" title="关于VAE"></a>关于VAE</h2><h2 id="关于Flow"><a href="#关于Flow" class="headerlink" title="关于Flow"></a>关于Flow</h2><p><strong>缺点</strong></p><p><code>由于必须保证逆变换简单和雅可比行列式容易计算，那么每一层的非线性变换能力都很弱。所以为了保证充分的拟合能力，模型就必须堆得非常深，计算量非常大。</code></p><h2 id="余弦相似度的假设"><a href="#余弦相似度的假设" class="headerlink" title="余弦相似度的假设"></a><a href="https://kexue.fm/archives/8069">余弦相似度的假设</a></h2><p>$$<br>cos(x,y)&#x3D;\frac{\sum^d_{i&#x3D;1}{x_iy_i}}{\sqrt{\sum^d_{i&#x3D;1}{x^2_i}}\sqrt{\sum^d_{i&#x3D;1}{y^2_i}}}<br>$$</p><blockquote><p>上式等号只在<strong>标准正交基下成立</strong>。向量的“夹角余弦”本身是具有鲜明的几何意义的，但上式右端只是坐标的运算，坐标依赖于所选取的坐标基，基底不同，内积对应的坐标公式就不一样，从而余弦值的坐标公式也不一样。</p></blockquote><p><strong>如果用公式算余弦值来比较句子相似度时表现不好，那么原因可能就是此时的句向量所属的坐标系并非标准正交基。</strong></p><p>原则上我们无法确定此时向量所属坐标系，但是我们在给向量集合选择基底时，可以依据猜测：<code>会尽量地用好每一个基向量，从统计学的角度看，这就体现为每个分量的使用都是独立的、均匀的，如果这组基是标准正交基，那么对应的向量集应该表现出“各项同性”来。</code>【<strong>如果一个向量的集合满足各向同性，那么我们可以认为它源于标准正交基</strong>】</p><p><strong>标准化协方差矩阵</strong></p><p>标准正态分布的均值为0、协方差矩阵为单位阵。假设向量集合${x_i}^N_{i&#x3D;1}$执行变换$\hat{x}<em>i&#x3D;(x_i-\mu)W$使得${\hat{x}<em>i}^N</em>{i&#x3D;1}$的均值为0，协方差矩阵为单位阵，这个操作对应于数据挖掘中的白化操作(Whitening)，具体如下：<br>$$<br>均值为0则：\mu&#x3D; \frac{1}{N}\sum</em>{i&#x3D;1}^Nx_i\<br>原始协方差：\Sigma&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N(x_i-\mu)^T(x_i-\mu)&#x3D;\left(\frac{1}{N}\sum_{i&#x3D;1}^N{x^T_ix_i}\right)-\mu^T\mu \<br>变换后协方差：\hat\Sigma&#x3D;W^T\Sigma W&#x3D;I\rightarrow\Sigma&#x3D;(W^T)^{-1}W^{-1}&#x3D;(W^{-1})^TW^{-1}\<br>协方差矩阵是半正定对称矩阵，可以被SVD分解：\Sigma&#x3D;U\Lambda U^T\rightarrow W^{-1}&#x3D;\sqrt{\Lambda}U^T\rightarrow W&#x3D;U\sqrt{\Lambda^{-1}}<br>$$</p><h2 id="关于Attention"><a href="#关于Attention" class="headerlink" title="关于Attention"></a>关于Attention</h2><pre><code>RNN因其本质是马尔科夫决策过程，无法很好的学习全局信息。CNN方便并行，而且容易捕捉到一些全局的结构信息。RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好：$y_t=f(y_&#123;t-1&#125;,x_t)$CNN事实上只能获取局部信息，是通过层叠来增大感受野：$y_t=f(x_&#123;t-1&#125;,x_t,x_&#123;t+1&#125;)$ [3x的kernel]</code></pre><p>Attention的思路最为粗暴，它一步到位获取了全局信息！它的解决方案是：$y_t&#x3D;f(x_t,A,B)$ A和B是额外引入的序列，如果$A&#x3D;B&#x3D;X$就是self-attention。Attention的意思是直接将$x_t$与原来的每个词进行比较，最后算出$y_t$。<br>$$<br>Attention(Q,K,V)&#x3D;softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>$Q\in R^{n\times d_k}$，$K\in R^{m\times d_k}$，$V\in R^{m\times d_v}$。如果忽略激活函数softmax的话，那么事实上它就是三个矩阵相乘，最后的结果就是一个$n\times d_v$的矩阵。于是我们可以认为：这是一个Attention层，将序列Q编码成了一个新的的序列。事实上$Q,K,V$分别是$query,key,value$的简写，那么上式的意思就是通过$query$与各个$key$内积的并softmax的方式，来得到$query$与各个$value$的相似度，然后加权求和，得到一个向量。其中因子$d_k$起到调节作用，使得内积不至于太大（太大的话softmax后就非0即1了，不够“soft”了）。</p><pre><code>50维的词向量，将每一维打乱重新排个序（当然整体要按同样的顺序来重新排序），它还是等价于原来的词向量。既然相加的对象（词向量）都没有局部结构，我们也没必要强调被加的对象（Position_Embedding）的局部结构（也就是交叉连接）了。</code></pre><h2 id="一些观点"><a href="#一些观点" class="headerlink" title="一些观点"></a>一些观点</h2><pre><code>数据扩增是将我们的先验知识融入到模型中的一种方案。mixup相当于一个正则项，它希望模型尽可能往线性函数靠近，也就是说，既保证模型预测尽可能准确，又让模型尽可能简单。</code></pre>]]></content>
      
      
      <categories>
          
          <category> 专题学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高斯分布</title>
      <link href="/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/"/>
      <url>/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<h3 id="单变量高斯分布-univariate-Gaussian"><a href="#单变量高斯分布-univariate-Gaussian" class="headerlink" title="单变量高斯分布(univariate Gaussian )"></a>单变量高斯分布(univariate Gaussian )</h3><p>$$<br>f(x)&#x3D;\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)<br>$$</p><h3 id="单高斯分布"><a href="#单高斯分布" class="headerlink" title="单高斯分布"></a>单高斯分布</h3><p>$$<br>N(x ; u, \Sigma)&#x3D;\frac{1}{\sqrt{2 \pi}|\Sigma|} \exp \left[-\frac{1}{2}(x-u)^{T}\Sigma^{-1} (x-u)\right]<br>$$</p><p>单高斯与单变量高斯的区别在于前者维度为$d$，后者维度为$1$。对于单高斯模型，$\mu$通常代表训练样本的均值，$\Sigma$代表样本的方差。</p><h3 id="高斯混合模型（GMM）"><a href="#高斯混合模型（GMM）" class="headerlink" title="高斯混合模型（GMM）"></a>高斯混合模型（GMM）</h3><p>$$<br>\operatorname{Pr}(x)&#x3D;\Sigma_{k&#x3D;1}^{K} \pi_{k} N\left(x ; u_{k}, \Sigma_{k}\right)<br>$$</p><p>依据全概率公式，$\pi_{k}$是选中参数$u_{k}$和$\Sigma_{k}$的概率，又称权重因子。EM算法用于优化GMM。</p>]]></content>
      
      
      <categories>
          
          <category> 专题学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第5章 决策树</title>
      <link href="/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC5%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/"/>
      <url>/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC5%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<p>决策树（decision tree）是一种基本的分类和回归方法，它可以任务是if-then规则（<strong>互斥且完备</strong>）的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。<br>因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题，得到的决策树是sub-optimal的。<br>决策树的生成对应于模型的局部选择<code>局部最优</code>，决策树的剪枝对应于模型的全局选择<code>全局最优</code>。</p><h3 id="1-特征选择"><a href="#1-特征选择" class="headerlink" title="1. 特征选择"></a>1. 特征选择</h3><p>特征现在在于选取对训练数据具有分类能力的特征，通常特征选择的准则是信息增益或信息增益比。<br><strong>熵</strong><br>熵（entropy）是表示随机变量不确定性的度量：$H(X)&#x3D;-\sum_i^N{p_i\log{p_i}},\ 0\le{H(p)\le\log{n}}$<br>条件熵（conditional entropy）表示在已知随机变量X的条件下随机变量Y的不确定性：$H(Y|X)&#x3D;\sum_i^N{p_iH(Y|X&#x3D;x_i)}$</p><p><strong>信息增益</strong>（Information gain）表示得知特征X的信息而使得类Y的信息的确定性减少的程度：<br>$g(D,A)&#x3D;H(D)-H(D|A)$<br>上式中$H(D)[empirical\ entropy]$表示对数据集D进行分类的不确定性，$H(D|A)[empirical\ comditional\ entropy]$表示在特征A给定的条件下对数据集D进行分类的不确定性。一般地，熵与条件熵的差称为互信息（mutual Information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p><p><strong>信息增益比</strong>（Information gain ratio）信息增益存在偏向于选择取值较多的特征，ratio可以避免此问题。<br>$g_R(D,A)&#x3D;\frac{g(D,A)}{H_A(D)},\ H_A(D)&#x3D;-\sum_{i&#x3D;1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|},\ n是特征A的取值个数$</p><h3 id="2-决策树的生成"><a href="#2-决策树的生成" class="headerlink" title="2. 决策树的生成"></a>2. 决策树的生成</h3><p><strong>ID3：</strong>核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。ID3相当于用极大似然法进行概率模型的选择。<strong>C4.5：</strong>改进ID3，并使用信息增益比。</p><h3 id="3-决策树的剪枝（pruning）"><a href="#3-决策树的剪枝（pruning）" class="headerlink" title="3. 决策树的剪枝（pruning）"></a>3. 决策树的剪枝（pruning）</h3><p>决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现。<br>设树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该结点有$N_t$个样本点，其中$k$类样本点有$N_{tk}$个，则决策树学习的损失函数可以定义为：<br>$C_\alpha(T)&#x3D;\sum_t^{|T|}N_tH_t(T)+\alpha|T|&#x3D;C(T)+\alpha|T|\ H_t(T)&#x3D;-\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}\C(T)&#x3D;\sum_t^{|T|}{N_tH_t(T)&#x3D;-\sum_t^{|T|}\sum_k^K{N_{tk}}\log\frac{N_{tk}}{N_t}}$<br>$C(T)$表示模型对训练数据的预测误差（模型与训练数据的拟合程度），$|T|$表示模型复杂度，$\alpha\ge0$控制两者间的影响。较大的$\alpha$促使选择较简单的模型，较小的$\alpha$促使选择较复杂的模型，$\alpha&#x3D;0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。剪枝就是当$\alpha$确定时，选择损失函数最小的模型。损失函数的极小化等价于正则化的极大似然估计。</p><blockquote><p><strong>输入：</strong>生成算法产生的整个树$T$，参数$\alpha$<br><strong>输出：</strong>修剪后的子树$T_{\alpha}$<br>1-计算每个结点的经验熵<br>2-递归地从树的叶结点向上回缩<br>3-根据回缩前B后A损失函数值$C_{\alpha}(T_A)\le C_{\alpha}(T_B)$，剪掉后损失减少则剪枝（重复）</p></blockquote><h3 id="4-CART算法"><a href="#4-CART算法" class="headerlink" title="4. CART算法"></a>4. CART算法</h3><p>分类与回归树（CART）是在给定输入随机变量X条件下输出随机变量Y的条件概率分布。（左是右否的二叉树）</p><blockquote><p><strong>生成：</strong>基于训练数据集生成决策树，生成的树要尽量大。<br><strong>剪枝：</strong>用验证集对已生成的树进行剪枝并选择最优子树，用损失函数最小作为剪枝的标准。</p></blockquote><p><strong>CART生成</strong><br><code>回归树：</code>平方误差最小化准则<br><code>分类树：</code>基尼指数最小化准则</p><blockquote><p><strong>最小二乘回归树生成算法</strong><br><strong>输入：</strong>训练数据集$D$<br><strong>输出：</strong>回归树$f(x)$<br>在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉树：1-选择最优切分变量$j$与切分点$s$，求解$\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$遍历变量$j$对固定的切分变量$j$扫描切分点$s$选择使式达到最小值的对$(j,s)$。2-用选定的$(j,s)$划分区域并决定相应的输出值$R_1(j,s)&#x3D;{x|x^{(j)}\le s},\ \ R_2(j,s)&#x3D;{x|x^{(j)}\gt s},\ \ \hat{c}<em>m&#x3D;\frac{1}{N_m}\sum</em>{x_i\in R_m(j,s)}y_i,x\in R_m$。3-重复1、2。4-将输入空间划分为$M$个区域$R_1,R_2,\cdots,R_M$生成决策树：$f(x)&#x3D;\sum_m^M\hat{c}_mI(x\in R_m)$</p><p><strong>基尼指数分类树生成算法</strong><br><strong>输入：</strong>训练数据集$D$，停止计算的条件<br><strong>输出：</strong>CART决策树<br>从根结点开始构建二叉树：1-设结点的训练数据集为$D$，计算现有特征对数据集的基尼指数$Gini(D)$。 根据特征$A&#x3D;a$将数据集分成$D_1$和$D_2$计算基尼指数$Gini(D,A)$。2-在所有特征与切分点对$&lt;A,a&gt;$中选择基尼指数最小的作为最优特征与最优切分点，由此生成两个子结点。3-重复1，2，直到满足停止条件。4-生成CART树。</p><p><code>基尼指数：</code>$K$类中样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数为：$Gini(p)&#x3D;\sum_k^K{p_k(1-p_k)}&#x3D;1-\sum_k^K{p_k^2}$。对于给定样本集合$D$，基尼指数为$Gini(D)&#x3D;1-\sum_k^K(\frac{|C_k|}{|D|})^2$，对于在特征$A$条件下的集合$D$的基尼指数为$Gini(D,A)&#x3D;\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$。基尼指数越大，样本集合的不确定性也越大（与熵类似）。<br><code>排列组合：</code>排列$A_n^m&#x3D;\frac{n!}{(n-m)!}$， 组合$C_n^m&#x3D;\pmatrix{\begin{matrix}n\m\end{matrix}}&#x3D;\frac{A_n^m}{m!}&#x3D;\frac{n!}{m!(n-m)!}&#x3D;C(n,m)&#x3D;C(n,n-m)$<br><code>二项式定理：</code>$(a+b)^n&#x3D;\sum_i^nC_n^i a^{n-i}b^i$，$C_n^i$杨辉三角<br><code>伯努利分布：</code>关于布尔变量$x\in{0,1}$的概率分布，其连续参数$p\in[0,1]$表示变量$x&#x3D;1$的概率：$p(x)&#x3D;p^x(1-p)^{1-x},E(x)&#x3D;p,\sigma&#x3D;p(1-p)$<br><code>二项分布：</code>1-实验次数固定为$n$；2-每次事件只有两种可能；3-每次成功的概率固定为$p$；4-表示成功$x$次的概率：$p(x)&#x3D;C_n^x p^x(1-p)^{n-x},E(x)&#x3D;np,\sigma&#x3D;\sqrt{np(1-p)}$<br><code>几何分布：</code>0-与二项分类相似，唯一不同在于4-表示进行$x$次尝试，取得第一次成功的概率：$p(x)&#x3D;(1-p)^{x-1}p,E(x)&#x3D;1&#x2F;p,\sigma&#x3D;(1-p)&#x2F;p^2$<br>&#96;&#96;泊松分布：&#96;1-事件独立；2-在任意相同的时间范围内，事件发生的概率相同；3-在某个时间范围内，发生某件事情$x$的概率：$p(x)&#x3D;\frac{u^xe^{-u}}{x!},E(x)&#x3D;\sigma&#x3D;u\ [p&#x3D;\frac{u}{n},p(x)&#x3D;\lim_{n\rightarrow\infty}\pmatrix{\begin{matrix}n\m\end{matrix}}p^x(1-p)^{n-x}]即在p上的极限$</p><p><code>伯努利扔一次硬币；二项分布是多次伯努利；泊松分布是p很小的二项，即无数次扔硬币且正面概率极小；正态分布是n很大的二项，即无数次扔硬币且硬币完全相同。</code></p></blockquote><p><strong>CART剪枝</strong></p><blockquote><p><strong>输入：</strong>CART算法完全生成的决策树$T_0$<br><strong>输出：</strong>最优决策树$T_\alpha$<br>1-设$k&#x3D;0,T&#x3D;T_0,\alpha&#x3D;+\infty$<br>2-自下而上地对各内部结点$t$计算$C(T_t)$，$|T_t|$以及$g(t)&#x3D;\frac{C(t)-C(T_t)}{|T_t|-1},\alpha&#x3D;\min(\alpha,g(t))$。以$|T_t|$为叶结点个数$t$为根节点的子树$T_t$，对训练数据的预测误差$C(T_t)$。<br>3-对$g(t)&#x3D;\alpha$的内部结点$t$进行剪枝，并对叶结点以多数表决法决定其类，得到树$T$。<br>4-设$k&#x3D;k+1,T_k&#x3D;T,\alpha_k&#x3D;\alpha$<br>5-如果$T_k$不是由根节点及两个叶结点构成的树，则回到1-$\alpha&#x3D;+\infty$；否则令$T_k&#x3D;T_n$<br>6-采用交叉验证法在子树序列$T_0,T_1,\cdots,T_n$中选取最优子树$T_{\alpha}$。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 《统计学习方法》（第二版） </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第3章 K近邻法</title>
      <link href="/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC3%E7%AB%A0%20K%E8%BF%91%E9%82%BB%E6%B3%95/"/>
      <url>/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC3%E7%AB%A0%20K%E8%BF%91%E9%82%BB%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="3-1-k-近邻算法-（-k-nearest-neighbor-KNN）"><a href="#3-1-k-近邻算法-（-k-nearest-neighbor-KNN）" class="headerlink" title="3.1 $k$近邻算法 （$k$-nearest neighbor, KNN）"></a>3.1 $k$近邻算法 （$k$-nearest neighbor, KNN）</h3><blockquote><p><strong>输入：</strong>训练集$T&#x3D;{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$<br><strong>输出：</strong>实例$x$所属类别$y$<br>1-根据给定的距离度量，在训练集$T$中找出与$x$最近邻的$k$个点，涵盖这$k$个点的$x$的领域记作$N_k(x)$;<br>2-在$N_k(x)$中根据分类决策规则决定$x$的类别$y$：$y&#x3D;\arg\max_{c_j}\sum_{x_i\in N_k(x)}{I(y_i&#x3D;c_j)}$，$I$为指示函数。</p></blockquote><p>是一种基本分类与回归方法，没有显式的学习过程</p><h3 id="3-2-k-近邻模型"><a href="#3-2-k-近邻模型" class="headerlink" title="3.2 $k$近邻模型"></a>3.2 $k$近邻模型</h3><p>$k$近邻算法使用的模型实际上对应于对特征空间的划分，模型有三个基本要素：距离度量、$k$值得选择和分类决策规则。</p><p><strong>距离度量</strong></p><blockquote><p>欧式距离、$L_p$距离（Minkowski距离）</p><p>$L_p(x_i,y_i)&#x3D;(\sum_{l&#x3D;1}^n|x_i^{(l)}-x_j^{(l)}|^p)^\frac{1}{p},p\ge1$<br>$p&#x3D;2$，欧式距离；$p&#x3D;1$，Manhattan距离；$p&#x3D;\infty$，各个坐标距离的最大值</p></blockquote><p><strong>$k$值的选择</strong></p><blockquote><p><strong>较小值：</strong>相当于在较小邻域中训练实例进行预测，近似误差会减小，估计误差会增大，对近邻点非常敏感。<em>k值减小意味着整体模型变得复杂，容易过拟合。</em></p><p><strong>较大值：</strong>相当于在较大邻域中训练实例进行预测，估计误差会减小，近似误差会增大，对远邻点依然有效，容易误检。<em>k值增大意味着整体模型变得简单，容易欠拟合。</em></p><p>$\mathbf{k&#x3D;N}$：无论什么输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略了实例中大量有用信息。</p></blockquote><h3 id="3-3-k-近邻法的实现：-kd-树"><a href="#3-3-k-近邻法的实现：-kd-树" class="headerlink" title="3.3 $k$近邻法的实现：$kd$树"></a>3.3 $k$近邻法的实现：$kd$树</h3><p>以特征$x$的第一维为坐标轴，以中位数为切分点，然后是第二维，第三维，划分得到最后的二叉树。</p>]]></content>
      
      
      <categories>
          
          <category> 《统计学习方法》（第二版） </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第4章 朴素贝叶斯法</title>
      <link href="/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC4%E7%AB%A0%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"/>
      <url>/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC4%E7%AB%A0%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="1-基本方法"><a href="#1-基本方法" class="headerlink" title="1.基本方法"></a>1.基本方法</h3><p>朴素贝叶斯法（naive Bayes）是基于贝叶斯定理与特征条件独立假设的分类法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$.</p><p>先学习先验概率分布$P(Y&#x3D;c_k)$及条件概率分布$P(X&#x3D;x|Y&#x3D;c_k)&#x3D;P(X^{(1)}&#x3D;x^{(1)},X^{(2)}&#x3D;x^{(2)},\cdots,X^{(N)}&#x3D;x^{(n)}|Y&#x3D;c_k)$从而得到联合概率分布$P(X,Y)$<br>由于条件概率分布有指数级数量的参数，所以对条件概率分布做了<strong>条件独立性假设</strong><code>用于分类的特征在类确定的条件下都是条件独立的</code>：$P(X&#x3D;x|Y&#x3D;c_k)&#x3D;P(X^{(1)}&#x3D;x^{(1)},X^{(2)}&#x3D;x^{(2)},\cdots,X^{(N)}&#x3D;x^{(n)}|Y&#x3D;c_k)&#x3D;\prod_{j&#x3D;1}^n{P(X^j&#x3D;x^j|Y&#x3D;c_k)}$<br>分类器：$y&#x3D;f(x)&#x3D;\arg\max_{c_k}\frac{P(Y&#x3D;c_k)\Pi_{j}P(X^{j}&#x3D;x^j|Y&#x3D;c_k)}{\sum_k{P(Y&#x3D;c_k)\Pi_{j}P(X^{j}&#x3D;x^j|Y&#x3D;c_k)}}$</p><p><strong>后验概率最大化的含义</strong></p><p>期望风险最小化准则变为了后验概率最大化准则<br>$$<br>\begin{align}<br>f(x)&amp;&#x3D;\arg\min_{y\in\mathcal{Y}}\sum_{k&#x3D;1}^K{L(c_k,y)P(c_k|X&#x3D;x)}\<br>&amp;&#x3D;\arg\min_{y\in\mathcal{Y}}\sum_{k&#x3D;1}^K{P(y\neq c_k|X&#x3D;x)}\<br>&amp;&#x3D;\arg\min_{y\in\mathcal{Y}}{(1-P(y&#x3D;c_k|X&#x3D;x))}\<br>&amp;&#x3D;\arg\max_{y\in\mathcal{Y}}{P(y&#x3D;c_k|X&#x3D;x)}\<br>\end{align}<br>$$</p><h3 id="2-朴素贝叶斯法的参数估计"><a href="#2-朴素贝叶斯法的参数估计" class="headerlink" title="2.朴素贝叶斯法的参数估计"></a>2.朴素贝叶斯法的参数估计</h3><p><strong>极大似然估计</strong></p><p>估计先验概率：$P(Y&#x3D;c_k)&#x3D;\frac{\sum_{i&#x3D;1}^N{I(y_i&#x3D;c_k)}}{N},I为指示函数$<br>估计条件概率：$P(X^j&#x3D;a_{jl}|Y&#x3D;c_k)&#x3D;\frac{\sum_{i&#x3D;1}^{N}{I(x_i^j&#x3D;a_{jl},y_i&#x3D;c_k)}}{\sum_{i&#x3D;1}^{N}{I(y_i&#x3D;c_k)}}$<br>会出现所有估计概率值为0的情况</p><p><strong>贝叶斯估计</strong></p><p>估计先验概率：$P_\lambda(Y&#x3D;c_k)&#x3D;\frac{\sum_{i&#x3D;1}^N{I(y_i&#x3D;c_k)}+\lambda}{N+K\lambda},I为指示函数$<br>估计条件概率：$P_\lambda(X^j&#x3D;a_{jl}|Y&#x3D;c_k)&#x3D;\frac{\sum_{i&#x3D;1}^N{I(x_i^j&#x3D;a_{jl},y_i&#x3D;c_k)+\lambda}}{\sum_{i&#x3D;1}^N{I(y_i&#x3D;c_k)+S_j\lambda}}$<br>$\lambda&#x3D;0$就是<code>极大似然估计</code>，$\lambda&#x3D;1$就是<code>拉普拉斯平滑</code></p>]]></content>
      
      
      <categories>
          
          <category> 《统计学习方法》（第二版） </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第2章 感知机</title>
      <link href="/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC2%E7%AB%A0%20%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC2%E7%AB%A0%20%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h3 id="2-1-感知机模型"><a href="#2-1-感知机模型" class="headerlink" title="2.1 感知机模型"></a>2.1 感知机模型</h3><p>$$<br>f(x)&#x3D;sign(w\cdot x+b),\ sign(x)&#x3D;\begin{cases}+1,x\ge0\-1,x\lt0\end{cases}<br>$$</p><p>对于特征空间$\mathbf{R}^n$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距。</p><h3 id="2-2-感知机学习策略"><a href="#2-2-感知机学习策略" class="headerlink" title="2.2 感知机学习策略"></a>2.2 感知机学习策略</h3><ul><li>数据集的线性可分性</li><li>损失函数是参数$w$和$b$连续可导函数：点到超平面的距离 $\frac{1}{|w|}|w\cdot x_i+b|$</li><li>误分类数据满足：$-y_i(w\cdot x_i+b)\gt0$，$y(w\cdot x+b)$称为样本点的函数间隔</li></ul><h3 id="2-3-感知机学习算法"><a href="#2-3-感知机学习算法" class="headerlink" title="2.3 感知机学习算法"></a>2.3 感知机学习算法</h3><p><strong>原始形式</strong></p><blockquote><p><strong>输入：</strong>数据集、学习率$\eta\ (0\lt\eta\le1)$<br><strong>输出：</strong>$w,b,f(x)&#x3D;sign(w\cdot x+b)$</p><ol><li><p>选取初始值$w_0,b_0$</p></li><li><p>在训练集中选取数据$(x_i,y_i)$</p></li><li><p>如果$y_i(w\cdot x_i+b)\le0$</p><p>$$w\leftarrow w+\eta y_i x_i\<br>b\leftarrow b+\eta y_i$$</p></li><li><p>转至2，直到无误分类点</p></li></ol></blockquote><p>首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数，在一个过程中一次随机选取一个误分类点使其梯度下降。</p><p><strong>Novikoff定理</strong></p><ol><li><p>$\exists:y_i(\hat{W}<em>{opt}\cdot\hat{x}<em>i)&#x3D;y_i(W</em>{opt}\cdot x_i+b</em>{opt})\ge\gamma,|\hat{W}_{opt}|&#x3D;1$</p></li><li><p>在训练集上的误分类次数$k$满足不等式：$k\le(\frac{R}{\gamma})^2,R&#x3D;\max_{1\le i\le N}|\hat{x}_i|^2$</p></li></ol><p>说明误分类有上界；训练集可分时，原始形式收敛，不可分时，不收敛；2可以近似看成线段划分，即特征空间最多可以划分成K个。</p><p><code>当训练集可分时，感知机学习算法存在无穷多解，由于不同的初始值或不同的迭代顺序而有所不同。</code></p><p><strong>对偶形式</strong></p><blockquote><p><strong>输入：</strong>数据集、学习率$\eta\ (0\lt\eta\le1)$<br><strong>输出：</strong>$\alpha,b,f(x)&#x3D;sign(\sum_{j&#x3D;1}^N\alpha_j y_j x_j\cdot x+b)$</p><ol><li><p>选取初始值$a\leftarrow0,b\leftarrow0$</p></li><li><p>在训练集中选取数据$(x_i,y_i)$</p></li><li><p>如果$y_i(\sum_{j&#x3D;1}^N\alpha_j y_j x_j\cdot x_i+b)\le0$</p></li></ol><p>  $$\alpha_i\leftarrow\alpha_i+\eta\<br>  b\leftarrow b+\eta y_i$$</p><ol start="4"><li>转至2，直到无误分类点</li></ol></blockquote><p><strong>基本思想：</strong>将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合，通过求解其系数而得到$w$和$b$。实例点更新次数越多，意味着它距离分类超平面越近，也就越难正确分类。对偶形式中训练实例仅以內积$x_j\cdot x_i$的形式出现（Gram矩阵）。</p><h3 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h3><p><strong>1. 为什么感知机不能表示异或</strong></p><p><strong>2. 样本集线性可分的充要条件是正实例点击所构成的凸壳$^1$与负实例所构成的凸壳互不相交</strong></p><p>$^1$设集合$S\subset\mathbf{R}^n$是由$\mathbf{R}^n$中的$k$个点组成的集合，即$S&#x3D;{x_1,x_2,\cdots,x_k}$ 定义$S$的凸壳$conv(S)$为：<br>$$<br>conv(S)&#x3D;\left{x&#x3D;\sum_{i&#x3D;1}^k{\lambda_i x_i}\mid\sum_{i&#x3D;1}^k{\lambda_i&#x3D;1},\lambda_i\ge0,i&#x3D;1,2,\cdots,k\right}<br>$$<br><code>最小凸多边形、点集合的边界所围成的区域、点集的线性组合</code></p>]]></content>
      
      
      <categories>
          
          <category> 《统计学习方法》（第二版） </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo搭建GitHub个人博客</title>
      <link href="/2022/04/10/Hexo%E6%90%AD%E5%BB%BAGitHub%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2022/04/10/Hexo%E6%90%AD%E5%BB%BAGitHub%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><pre><code>- 安装Node.js- 安装Git- 远程连接GitHub- 安装Hexo- Hexo的本地配置- 博客相关指令</code></pre><h3 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h3><blockquote><p>Node.js就是一个用于创建服务器端应用程序的运行系统，它可以轻松构建网络或其他事件驱动的应用程序服务器。</p></blockquote><p>进入<a href="https://nodejs.org/en/download/">Node.js官网</a>下载对应版本的安装包，默认设置安装。<br>安装完成后进入终端检查是否安装成功，按<code>Win+R</code>进入<code>cmd</code>输入<code>node -v</code>和<code>npm -v</code>，如果出现版本号，则证明安装成功。</p><blockquote><p>npm (Node Package Manager) NodeJS包管理和分发工具<br>(<em>可选</em>) - 添加镜像（进入cmd，以阿里为例）<br>npm config set registry <a href="https://registry.npm.taobao.org/">https://registry.npm.taobao.org</a></p></blockquote><h3 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h3><blockquote><p>Git是一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理，帮助我们把本地网页上传到Github。</p></blockquote><p>进入<a href="https://git-scm.com/download/win">Git官网</a>下载，默认设置安装。<br>安装完成之后在<code>cmd</code>中使用<code>git --version</code>验证是否安装成功。</p><h3 id="远程连接GitHub"><a href="#远程连接GitHub" class="headerlink" title="远程连接GitHub"></a>远程连接GitHub</h3><p><strong>生成密钥</strong></p><pre><code class="bash"># git bash中设置user.name和user.emailgit config --global user.name &quot;GitHub用户名&quot;git config --global user.email &quot;GitHub注册邮箱&quot;# 生成密钥ssh-keygen -t rsa -C &quot;注册邮箱&quot;# 将公钥id_rsa.pub添加到GitHubgithub -&gt; settings -&gt; ssh and gpg keys# 测试本地连接ssh -T git@github.com</code></pre><h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><blockquote><p>Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Heroku，是搭建博客的首选框架。</p></blockquote><p>进入<code>cmd</code>输入<code>npm</code>安装命令</p><pre><code class="bash"># 安装指令 -g 表示全局安装npm install hexo-cli -g# 验证指令hexo -v</code></pre><h3 id="Hexo的本地配置"><a href="#Hexo的本地配置" class="headerlink" title="Hexo的本地配置"></a>Hexo的本地配置</h3><p><strong>初始化</strong><br>在<code>Git Bash</code>中执行</p><pre><code class="bash"># 创建存放博客文件夹mkdir myBlog# 初始化hexo init myBlog</code></pre><p>看到<code>Start blogging with Hexo</code>则表示初始化成功。<br><strong>部署到GitHub</strong><br>安装必要插件：</p><pre><code class="bash">npm install hexo-deployer-git --save  #安装自动部署Github插件</code></pre><p>修改配置文件<code>_config.yml</code></p><pre><code class="yml"># Deployment## Docs: https://hexo.io/docs/one-command-deploymentdeploy:  type: git  repo: git@github.com:seeyourmind/seeyourmind.github.io  branch: master</code></pre><p><strong>更换主题</strong><br>以aomori为例</p><pre><code class="bash">git clone https://github.com/lh1me/hexo-theme-aomori.git themes/aomori</code></pre><h3 id="博客相关指令"><a href="#博客相关指令" class="headerlink" title="博客相关指令"></a>博客相关指令</h3><blockquote><p><strong>常用指令</strong><br>hexo new post “article title”<br>hexo s<br>hexo clean &amp;&amp; hexo g -s<br>hexo clean &amp;&amp; hexo g -d</p></blockquote><p><strong>文章书写</strong></p><pre><code class="bash">hexo new [layout] &lt;title&gt; 或 hexo n [layout] &lt;title&gt;# layout: #  post(默认，存于source/_posts)#  draft(草稿，存于source/_drafts，可以使用publish指令将其推送到_posts)#  page(页面)-p --path 自定义新文章-r --replece 替换同名文章-s --slug 文章的slug，作为新文章的文件名和发布后的URL# 示例hexo new page --path about/me &quot;About me&quot;</code></pre><p><strong>生成静态文件</strong></p><pre><code class="bash">hexo generate 或 hexo g-d --deploy 文件生成后部署网站-w --watch 监视文件变动-b --bail 生成过程中出现异常时抛出-f --force 强制重新生成文件-c --concurrency 最大同时生成文件数量，默认无限制</code></pre><p><strong>发布草稿</strong></p><pre><code class="bash">hexo publish [layout] &lt;filename&gt;</code></pre><p><strong>启动服务器</strong><br><code>hexo server 或 hexo s</code> 启动服务器，<code>ctrl+c</code> 结束，默认地址为：<a href="http://localhost:4000/">http://localhost:4000/</a><br><strong>部署网站</strong></p><pre><code class="bash">hexo deploy 或 hexo d-g 或--generate 部署之前写成静态文件</code></pre><p><strong>渲染文件</strong></p><pre><code class="bash">hexo render &lt;file1&gt; [file2]-o或--output 设置输出路径</code></pre><p><strong>清除缓存文件</strong></p><pre><code class="bash">hexo clean</code></pre><p><strong>列出网站资料</strong></p><pre><code class="bash">hexo list &lt;type&gt;</code></pre><p><strong>显示草稿</strong></p><pre><code class="bash">hexo --deaft</code></pre><p><strong>自定义当前工作目录</strong></p><pre><code class="bash">hexo --cwd /path/to/cwd</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第1章 统计学习及监督学习概论</title>
      <link href="/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC1%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/"/>
      <url>/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC1%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<h3 id="1-统计学习基本分类"><a href="#1-统计学习基本分类" class="headerlink" title="1. 统计学习基本分类"></a>1. 统计学习基本分类</h3><p><strong>监督学习</strong>    <code>学习输入到输出的映射的统计规律</code></p><blockquote><p>1-输入空间、特征空间和输出空间：特征连续预测是回归，离散预测是分类<br>2-联合概率分布：假设输入与输出服从联合分布（关于数据的基本假设）<br>3-假设空间：模型属于由输入空间到输出空间的映射的集合，意味着学习范围的确定。模型可以是概率模型（$P(Y|X)$）或非概率模型$Y&#x3D;f(X)$</p></blockquote><p><strong>无监督学习</strong>    <code>学习数据中的统计规律或潜在结构</code></p><blockquote><p>旨在从假设空间中选出在给定评价标准下的最优模型<br>模型可以实现对数据的聚类、降维或概率估计</p></blockquote><p><strong>强化学习</strong>    <code>学习最优的序贯决策</code></p><blockquote><p>强化学习的马尔可夫决策过程是状态、奖励、动作序列上的随机过程，$&lt;S,A,P,r,\gamma&gt;$<br><strong>S</strong>tate、<strong>A</strong>ction、transition <strong>P</strong>robability ($P(s^\prime|s,a)&#x3D;P(s_{t+1}&#x3D;s^\prime|s_t&#x3D;s,a_t&#x3D;a)$)、<strong>r</strong>eward function ($r(s,a)&#x3D;E(r_{t+1}|s_t&#x3D;s,a_t&#x3D;a)$)、discount factor ($\gamma\in[0,1]$)<br>策略$\pi$定义为给定状态下动作的函数$a&#x3D;f(s)$或$P(a|s)$<br>状态价值函数定义为策略$\pi$从某一个状态$s$开始的长期累积奖励的数学期望<br>动作价值函数定义为策略$\pi$从某一个状态$s$和动作$a$开始的长期累积奖励的数学期望</p><p>强化学习方法有基于策略的(policy-based)、基于价值的(value-based)无模型(model-free)方法和有模型(model-based)方法。<br>model-based直接学习马尔科夫决策过程<br>policy-based model-free求解最优策略$\pi^*$<br>value-based model-free求解最优价值函数$q^*(s,a)$</p></blockquote><p><strong>半监督学习</strong>    <code>利用未标注数据中的信息辅助标注数据进行监督学习</code></p><p><strong>主动学习</strong>     <code>找出对学习最有帮助的实例让teacher标注</code></p><h3 id="2-按模型分类"><a href="#2-按模型分类" class="headerlink" title="2. 按模型分类"></a>2. 按模型分类</h3><p><strong>概率模型与非概率模型</strong></p><blockquote><p><strong>基本概率公式</strong><br>加法规则：$P(x)&#x3D;\sum_yP(x,y)$<br>乘法规则：$P(x,y)&#x3D;P(x)P(y|x)$</p></blockquote><p><strong>线性模型与非线性模型</strong></p><p><strong>参数化模型与非参数化模型</strong></p><p>参数化模型假设模型参数的维度固定，模型可以由有限维参数完全刻画；非参数化模型假设模型参数的维度不固定或者说无穷大，随着训练数据量的增加而不断增大。</p><h3 id="3-按算法分类"><a href="#3-按算法分类" class="headerlink" title="3. 按算法分类"></a>3. 按算法分类</h3><p><strong>在线学习与批量学习</strong></p><p>在线学习每次接受一个样本，进行预测，之后学习模型，不断重复；批量学习一次接受所有数据，学习模型，之后进行预测。<br>在线学习可以是监督学习，也可以是无监督学习，强化学习本身就拥有在线学习的特点。</p><h3 id="4-按技巧分类"><a href="#4-按技巧分类" class="headerlink" title="4. 按技巧分类"></a>4. 按技巧分类</h3><p><strong>贝叶斯学习</strong></p><p>在概率模型学习和推理中，利用贝叶斯定理计算在给定数据条件下模型的条件概率，即后验概率，并应用这个原理进行模型的估计，以及对数据的预测。</p><p>贝叶斯估计和极大似然估计代表着统计学中贝叶斯学派和频率学派对统计的不同认识。<br>假设先验分布是均匀分布，取后验概率最大，就能从<code>贝叶斯估计</code>得到<code>极大似然估计</code>。<br>$$<br>D\rightarrow{极大似然估计}\rightarrow\hat{\theta&#x3D;\arg\max_{\theta}P(D|\theta)}\ [图像是只在\hat{\theta}有值]\<br>D\rightarrow{贝叶斯估计}\rightarrow\hat{P}(\theta|D)&#x3D;\frac{P(\theta)P(D|\theta)}{P(D)}\ [图像是正态分布曲线]<br>$$<br><strong>核方法</strong></p><p>核方法是使用核函数表示和学习费线性模型的一种机器学习方法。有一些线性模型的学习方法基于相似度计算&lt;向量內积&gt;，核方法可以把他们扩展到非线性模型的学习中，使其应用更广泛。<br>线性转非线性直接做是显式的定义输入空间到特征空间的映射，在特征空间做內积；核方法不显式的定义这个映射，而是定义核函数&lt;映射之后在特征空间的內积&gt;。<br>$$<br>\frac{输入空间}{x_1,x_2}\stackrel{映射函数\psi}{\longrightarrow}\frac{特征空间}{\psi(x_1),\psi(x_2)}\<br>核函数定义在输入空间\frac{输入空间}{K(x_1,x_2)} \ K(x_1,x_2)&#x3D;&lt;\psi(x_1),\psi(x_2)&gt;<br>$$</p><h3 id="5-统计学习方法三要素"><a href="#5-统计学习方法三要素" class="headerlink" title="5. 统计学习方法三要素"></a>5. 统计学习方法三要素</h3><p><strong>模型</strong></p><p>在监督学习过程中，模型就是所要学习的条件概率分布或决策函数，模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数。</p><p><strong>策略</strong></p><p>损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。<br>损失函数是$f(X)$和$Y$的<strong>非负</strong>实值函数，记作$L(Y,f(X))$；损失函数的期望是模型$f(X)$关于联合分布$P(x,Y)$的平均意义下的损失，称为风险函数或期望损失$R_{exp}(f)$。<br>$$<br>R_{exp}(f)&#x3D;E_P[L(Y,f(X))]&#x3D;\int_{\mathcal{X}\times\mathcal{Y}}{L(y,f(x))P(x,y)dxdy}<br>$$<br>由于联合分布未知，监督学习就是为了学习联合分布，所以监督学习是一个ill-formed problem。根据大数定律，当样本容量$N$趋于无穷时，经验风险$R_{emp}$趋近于期望风险$R_{exp}$<br>$$<br>R_{emp}(f)&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N{L(y_i,f(x_i))}<br>$$<br><code>大数定律：说如果统计数据足够大,那么事物出现的频率就能无限接近它的期望值。</code></p><p>由于现实中训练样本数目有限，甚至很小，所以需要矫正。</p><blockquote><p><strong>经验风险最小化</strong><br>$\min_{f\in{F}}\frac{1}{N}\sum_{i&#x3D;1}^N{L(y_i,f(x_i))}$<br>当模型是条件概率分布、损失函数是对数损失函数时，经验风险最小化等价于<code>极大似然估计</code><br><strong>结构风险最小化</strong><br>$R_{srm}(f)&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N{L(y_i,f(xi))+\lambda J(f)},\ \lambda\ge0$<br>$J(f)$为模型的复杂度，是定义在假设空间的泛函；$\lambda$用以权衡经验风险和模型复杂度<br>当模型是条件概率分布、损失函数是对数损失函数时，模型复杂度由模型的先验概率表示时，结构风险最小化等价于<code>最大后验概率估计</code></p></blockquote><p><strong>算法</strong></p><h3 id="6-模型评估与模型选择"><a href="#6-模型评估与模型选择" class="headerlink" title="6.模型评估与模型选择"></a>6.模型评估与模型选择</h3><p>通常将学习方法对未知数据的预测能力称为泛化能力</p><p><strong>模型选择方法：</strong>正则化和交叉验证</p><p>正则化符合奥卡姆剃刀（Occam’s razor）原理<br><code>如无必要，勿增实体，即简单有效原理</code></p><p>交叉验证：简单交叉（分成两部分）、S折交叉（分成S分）、留一交叉（S&#x3D;N）</p><h3 id="7-泛化误差上界"><a href="#7-泛化误差上界" class="headerlink" title="7.泛化误差上界"></a>7.泛化误差上界</h3><p>$$<br>对二分类问题，泛化误差R(f)：R(f)\le\hat{R}(f)+\epsilon(d,N,\delta),\ \epsilon(d,N,\delta)&#x3D;\sqrt{\frac{1}{2N}(\log{d}+\log\frac{1}{\delta})}\<br>Hoeffding不等式：\forall{t\gt0},P(|\overline{X}-E[\overline{X}]|\ge t)\le2exp(-\frac{2n^2t^2}{\sum_{i&#x3D;1}^{n}(b_i-a_i)^2})<br>$$</p><h3 id="8-监督学习应用"><a href="#8-监督学习应用" class="headerlink" title="8.监督学习应用"></a>8.监督学习应用</h3><p>TP（正确（T）预测为正样例（P））——正预测为正<br>FN（错误（F）预测为负样本（N））——正预测为负<br>FP（错误（F）预测为正样例（P））——负预测为正<br>TN（正确（T）预测为负样本（N））——负预测为负<br>准确率：预测为正的里面真实为正的<br>召回率：真实为正的里面预测为正的</p>]]></content>
      
      
      <categories>
          
          <category> 《统计学习方法》（第二版） </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/04/10/hello-world/"/>
      <url>/2022/04/10/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><p><strong>To Do</strong></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
