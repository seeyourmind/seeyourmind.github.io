<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>LaTex写作实用技巧</title>
      <link href="/2022/08/17/LaTex-usetech/"/>
      <url>/2022/08/17/LaTex-usetech/</url>
      
        <content type="html"><![CDATA[<h3 id="multicolumn多列合并导致表格横线不显示"><a href="#multicolumn多列合并导致表格横线不显示" class="headerlink" title="\multicolumn多列合并导致表格横线不显示"></a>\multicolumn多列合并导致表格横线不显示</h3><p>两种解决方法</p><blockquote><ol><li>增加一个宽度为0的列</li><li><code>\vline</code>：由于线的粗细不同，补的横线并不是完美对齐</li></ol></blockquote><span id="more"></span><h3 id="Elsevier模板使用"><a href="#Elsevier模板使用" class="headerlink" title="Elsevier模板使用"></a>Elsevier模板使用</h3><p><strong>添加标题</strong></p><pre><code class="latex">\title[mode=title]&#123;标题，默认设置&#125;\title[mode=alt]&#123;备用标题&#125;\title[mode=sub]&#123;副标题&#125;\title[mode=trans]&#123;翻译标题&#125;\title[mode=transub]&#123;翻译副标题&#125;</code></pre><p><strong>作者信息相关</strong></p><pre><code class="latex">\author[label1]&#123;姓名 \corref&#123;cor1&#125;&#125;\ead&#123;XXX@XXX.com&#125;\address[label1]&#123;地址&#125;\cortext[cor1]&#123;Corresponding author&#125;</code></pre><blockquote><p>只给出最常用🌰<br>更丰富使用技巧请看参考链接</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Using Skills </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LaTeX </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第1章 一元函数微积分</title>
      <link href="/2022/06/21/Learning-MathematicsOfMachineLearning-chap1/"/>
      <url>/2022/06/21/Learning-MathematicsOfMachineLearning-chap1/</url>
      
        <content type="html"><![CDATA[<blockquote><p>微分学为研究函数的性质提供了统一的方法与理论，尤其是寻找函数的极值。积分则在机器学习中被用于计算某些概率分布的数字特征。</p></blockquote><span id="more"></span><h2 id="1-1-极限与连续"><a href="#1-1-极限与连续" class="headerlink" title="1.1 极限与连续"></a>1.1 极限与连续</h2><h3 id="1-1-1-可数集与不可数集"><a href="#1-1-1-可数集与不可数集" class="headerlink" title="1.1.1 可数集与不可数集"></a>1.1.1 可数集与不可数集</h3><ul><li>集合$A$的元素数量称为其<strong>基数或者势</strong>，记为$|A|$。</li><li>对于集合$A$和$B$，如果集合$A$中的任意元素$a$，在集合$B$中都有唯一的元素$b$通过某种映射关系与之对应，即存在如下<strong>双射函数</strong>（Bijection， 一对一映射函数）:$$b&#x3D;f(a),a\in A, b\in B$$则称这两个集合的基数相等。（例如，实数集$\mathbb{R}$与区间$[0,1]$是等价的）</li><li>无限集可进一步分为可数集（Countable set）与不可数集（Uncountable set），可数集中的每个元素可以用正整数进行编号。<strong>离散与可数等价</strong>，任意可数集在数轴上的“长度”为0，不可数集长度不为0的在数轴上是稠密或是连续的，<strong>连续与不可数等价</strong>。</li></ul><h3 id="1-1-2-数列的极限"><a href="#1-1-2-数列的极限" class="headerlink" title="1.1.2 数列的极限"></a>1.1.2 数列的极限</h3><ul><li>数列极限的四则运算：$$\begin{align}<br>  \lim_{n\rightarrow+\infty}(a_n \plusmn b_n)&amp;&#x3D;\lim_{n\rightarrow+\infty}{a_n}\plusmn\lim_{n\rightarrow+\infty}{b_n}\<br>  \lim_{n\rightarrow+\infty}(a_n \cdot b_n)&amp;&#x3D;\lim_{n\rightarrow+\infty}{a_n}\cdot\lim_{n\rightarrow+\infty}{b_n}\<br>  \lim_{n\rightarrow+\infty}(\frac{a_n}{b_n})&amp;&#x3D;\frac{\lim_{n\rightarrow+\infty}{a_n}}{\lim_{n\rightarrow+\infty}{b_n}}<br>\end{align}$$</li><li>数列的上下界：上界$a_n\leq U$不唯一，下界$an\geq L$。单调有界的数列收敛，此称为<strong>单调收敛定理</strong>。有界是数列收敛的必要条件而非充分条件，如果数列无界，则必定发散。</li><li><strong>夹逼定理</strong>：如果对于$\forall n\in\mathbb{N}$有$b_n\leq a_n\leq c_n$且$\displaystyle{\lim_{n\rightarrow+\infty}{b_n}&#x3D;\lim_{n\rightarrow+\infty}{c_n}&#x3D;c}$，则$\displaystyle{\lim_{n\rightarrow+\infty}{a_n}&#x3D;c}$。</li></ul><h3 id="1-1-3-函数的极限"><a href="#1-1-3-函数的极限" class="headerlink" title="1.1.3 函数的极限"></a>1.1.3 函数的极限</h3><ul><li>极限是由柯西给出的$\epsilon-\delta$定义，其中点$x_0$的$\delta$邻域是指满足不等式$|x-x_0|\lt\delta$的所有$x$的集合，$\delta$为邻域半径。</li><li>函数在某一点极限存在的条件是在该点处的左右极限均存在且相等。</li></ul><h3 id="1-1-4-函数的连续性与间断点"><a href="#1-1-4-函数的连续性与间断点" class="headerlink" title="1.1.4 函数的连续性与间断点"></a>1.1.4 函数的连续性与间断点</h3><ul><li>第一类间断点：$x_0$处左右极限存在，但不相等，$f(x_0^-)\neq{f(x_0^+)}$为跳跃间断点；$x_0$处左右极限相等，但不等于该点处的函数值，$f(x_0^-)&#x3D;f(x_0^+)\neq{f(x_0)}$为可去间断点。</li><li>第二类间断点：$x_0$处左极限或右极限至少有一个不存在。</li><li>介值定理：如果函数$f(x)$在闭区间$[a,b]$内连续，$c$是介于$f(a)$和$f(b)$之间的数，则存在$[a,b]$中某点$x$，使得$f(x)&#x3D;c$。</li></ul><h3 id="1-1-5-上确界与下确界"><a href="#1-1-5-上确界与下确界" class="headerlink" title="1.1.5 上确界与下确界"></a>1.1.5 上确界与下确界</h3><ul><li>上确界（Supremum，最小上界，$s\leq{t}$，记为$sup(S)$，<strong>存在则唯一</strong>）和下确界（Infimum，最大下界，$s\geq{t}$，记为$inf(S)$）可看作是集合最大值和最小值的推广。</li></ul><h3 id="1-1-6-利普希茨连续性"><a href="#1-1-6-利普希茨连续性" class="headerlink" title="1.1.6 利普希茨连续性"></a>1.1.6 利普希茨连续性</h3><ul><li>Lipschitz连续不但保证函数值不间断，还限定函数变化速度：给定函数$f(x)$，如果对于区间$D$内任意两点$a$、$b$都存在常数K使得$$|f(a)-f(b)|\leq{K|a-b|}$$则称函数在区间内满足利普希茨条件&#x2F;连续。如果$K\lt1$，则称函数为压缩映射。</li><li>Lipschitz连续要求函数在区间上不能有超过线性的变化速度，对于分析和确保机器学习算法的稳定性有重要作用。</li></ul><h3 id="1-1-7-无穷小量"><a href="#1-1-7-无穷小量" class="headerlink" title="1.1.7 无穷小量"></a>1.1.7 无穷小量</h3><p>假设$f(x)$和$g(x)$都是$x\rightarrow{x_0}$的无穷小量（极限为0）：</p><ul><li>$\displaystyle{\lim_{x\rightarrow{x_0}}{\frac{f(x)}{g(x)}&#x3D;0}}$，该比值也是无穷小量，$f(x)$为$g(x)$的高阶无穷小，记为$f(x)&#x3D;o(g(x))$。</li><li>$\displaystyle{\lim_{x\rightarrow{x_0}}{\frac{f(x)}{g(x)}&#x3D;c,c\neq0}}$，该比值的极限为非0有界变量，等价无穷小，记为$f(x)\sim g(x)$。</li><li>$\displaystyle{\lim_{x\rightarrow{x_0}}{\frac{f(x)}{g(x)}&#x3D;\infty}}$，该比值的极限为无界变量，低阶无穷小。</li><li>这些比值反映了无穷小量趋向于0的速度快慢。</li></ul><h2 id="1-2-导数与微分"><a href="#1-2-导数与微分" class="headerlink" title="1.2 导数与微分"></a>1.2 导数与微分</h2><h3 id="1-2-1-一阶导数"><a href="#1-2-1-一阶导数" class="headerlink" title="1.2.1 一阶导数"></a>1.2.1 一阶导数</h3><ul><li>导数的定义为函数的自变量变化值趋向于0时，函数变化量与自变量变化之间的比值：$$f^\prime(x)&#x3D;\displaystyle{\lim_{\Delta x\rightarrow0}{\frac{f(x+\Delta x)-f(x)}{\Delta x}}}$$</li><li>单侧差分公式近似（$\Delta{x}$的值接近于0）：$\displaystyle{f^\prime(x)\approx\frac{f(x+\Delta x)}{\Delta x}}$</li><li>中心差分公式近似（$\Delta{x}$的值为接近于0的正数）：$\displaystyle{f^\prime(x)\approx\frac{f(x+\Delta x)-f(x-\Delta x)}{2\Delta x}}$</li><li>四则运算的求导公式：<br>$$<br>(f(x)\plusmn g(x))^\prime&#x3D;f^\prime(x)\plusmn g^\prime(x)\<br>(cf(x))^\prime&#x3D;cf^\prime(x)\<br>(f(x)g(x))^\prime&#x3D;f^\prime(x)g(x)+f(x)g^\prime(x)\<br>\left(\frac{f(x)}{g(x)}\right)^\prime&#x3D;\frac{f^\prime(x)g(x)-f(x)g^\prime(x)}{g^2(x)}\<br>(f(g(x)))^\prime&#x3D;f^\prime(g(x))g^\prime(x)<br>$$</li></ul><h3 id="1-2-2-机器学习中的常用函数"><a href="#1-2-2-机器学习中的常用函数" class="headerlink" title="1.2.2 机器学习中的常用函数"></a>1.2.2 机器学习中的常用函数</h3><ul><li>softplus函数：$f(x)&#x3D;\ln(1+e^x)$，是ReLu函数在$\max(0,x)$的光滑近似。</li><li>如果一个函数所有不可导点的集合为有限集或无限可数集，则称该函数<strong>几乎处处可导</strong>。</li></ul><h3 id="1-2-4-微分"><a href="#1-2-4-微分" class="headerlink" title="1.2.4 微分"></a>1.2.4 微分</h3><ul><li>函数在某一区间有定义，关于$x$的增量$\Delta{x}$，如果函数的增量$\Delta{y}&#x3D;f(x_0+\Delta{x})-f(x_0)$可以表示成$\Delta{y}&#x3D;A\Delta{x}+o(\Delta{x})$，其中$A$是不依赖于$\Delta{x}$的常数，$o(\Delta{x})$是$\Delta{x}$的高阶无穷小，则称函数在$x_0$处可微。</li><li>如果函数可微，则导数与微分的关系为：$dy&#x3D;f^\prime(x)dx$。微分用一次函数近似代替邻域内的函数值而忽略了更高次的项，几何意义是在点$(x_0,f(x_0))$处自变量增加$\Delta{x}$时切线函数$y&#x3D;f^\prime(x_0)(x-x_0)+f(x_0)$的增量$f^\prime(x_0)\Delta{x}$。</li></ul><h3 id="1-2-5-导数与函数的单调性"><a href="#1-2-5-导数与函数的单调性" class="headerlink" title="1.2.5 导数与函数的单调性"></a>1.2.5 导数与函数的单调性</h3><ul><li>由于导数是函数变化率的极限，因此如果在$x$点处它的值为正，则在该点处自变量增大时函数值也增大；如果为负，则自变量增大时函数值减小。<code>拉格朗日中值定理可证明</code></li><li>利用导数可以证明某些不等式，其思路是证明函数在某一区间内单调，因此在区间端点处取得极值。</li></ul><h3 id="1-2-6-极值判别法则"><a href="#1-2-6-极值判别法则" class="headerlink" title="1.2.6 极值判别法则"></a>1.2.6 极值判别法则</h3><ul><li>邻域内$\geq$、$\leq$为极值，去心邻域内$\gt$、$\lt$为严格极值。</li><li><strong>费马（Fermat）定理</strong>：假设函数在$x_0$处可导，如果在$x_0$取得极值，必定存在$f^\prime(x_0)&#x3D;0$。<code>可导函数取极值的一阶必要条件</code></li><li><strong>驻点（Stationary point）</strong>：导数等于0的点。</li><li>驻点处二阶导大于0为严格极小值，小于0为严格极大值。如果等于0，则假设$f^\prime(x_0)&#x3D;\cdots&#x3D;f^{(n-1)}(x_0)&#x3D;0,f^{n}(x_0)\neq0$，当$n$为偶数时，$f^{n}(x_0)\gt0$为严格极小值，$f^{n}(x_0)\lt0$为严格极大值；当$n$为奇数，该点不是极值点。<code>二阶充分条件——可用泰勒公式证明</code></li><li><strong>鞍点（Saddle point）</strong>：该点不是极值点，会导致数值优化算法如梯度下降法无法找到真正的极值点。</li></ul><h3 id="1-2-7-导数与函数的凹凸性"><a href="#1-2-7-导数与函数的凹凸性" class="headerlink" title="1.2.7 导数与函数的凹凸性"></a>1.2.7 导数与函数的凹凸性</h3><ul><li>Mix定义域的值域$\left{f(\theta{x}+(1-\theta)y)\right}$与值域的Mix$\left{\theta f(x)+(1-\theta)f(y)\right}$之间的关系，连线在上为凸函数（$\leq$），连线在下为凹函数（$\geq$）（欧美标准）。<code>去掉等号的为严格凹凸函数</code></li><li>凸函数二阶导大于0，凹函数二阶导小于0，二阶导是凹凸函数的充分必要条件。</li><li><strong>拐点</strong>：函数凹凸性的分界点，在拐点处二阶导为0，且两侧二阶导异号。</li><li>凸函数有优良的性质，可以保证优化算法找到函数的极小值点。</li></ul><h2 id="1-3-微分中值定理-Mean-Value-Theorem"><a href="#1-3-微分中值定理-Mean-Value-Theorem" class="headerlink" title="1.3 微分中值定理 Mean Value Theorem"></a>1.3 微分中值定理 Mean Value Theorem</h2><h3 id="1-3-1-罗尔（Rolle）中值定理"><a href="#1-3-1-罗尔（Rolle）中值定理" class="headerlink" title="1.3.1 罗尔（Rolle）中值定理"></a>1.3.1 罗尔（Rolle）中值定理</h3><ul><li>如果函数$f(x)$在闭区间$[a,b]$内连续，在开区间$(a,b)$内可导，且在区间的两个端点处的值相等$f(a)&#x3D;f(b)$，则在区间$[a,b]$内至少存在一个点$\xi$使得$f^\prime(\xi)&#x3D;0$。<code>可以使用费马定理证明</code></li><li>对于区间两端点处的函数值相等的函数，在区间内至少存在一点的导数值为0，该点处的切线与$x$轴平行。</li></ul><h3 id="1-3-2-拉格朗日（Lagrange）中值定理"><a href="#1-3-2-拉格朗日（Lagrange）中值定理" class="headerlink" title="1.3.2 拉格朗日（Lagrange）中值定理"></a>1.3.2 拉格朗日（Lagrange）中值定理</h3><ul><li>如果函数$f(x)$在闭区间$[a,b]$内连续，在开区间$(a,b)$内可导，则在区间$[a,b]$内至少存在一个点$\xi$使得$\displaystyle{f^\prime(\xi)&#x3D;\frac{f(b)-f(a)}{b-a}}$。<code>可以将函数剪掉一个线性函数构造出两个端点值相等的函数来证明</code></li><li>在区间$(a,b)$内至少存在一个点$\xi$，在$(\xi,f(\xi))$处的切线与两点之间的割线平行。</li></ul><h3 id="1-3-3-柯西（Cauchy）中值定理"><a href="#1-3-3-柯西（Cauchy）中值定理" class="headerlink" title="1.3.3 柯西（Cauchy）中值定理"></a>1.3.3 柯西（Cauchy）中值定理</h3><p>函数$f(x),g(x)$在$[a,b]$内连续，在$(a,b)$内可导，且$\forall{x\in(a,b)},g^\prime(x)\neq0$，则存在$\xi\in(a,b)$使得$\displaystyle{\frac{f^\prime(\xi)}{g^\prime(\xi)}&#x3D;\frac{f(b)-f(a)}{g(b)-g(a)}}$。<code>可直接用Lagrange中值定理变形得到</code></p><h3 id="1-4-泰勒公式"><a href="#1-4-泰勒公式" class="headerlink" title="1.4 泰勒公式"></a>1.4 泰勒公式</h3><p>如果一个函数<strong>足够光滑</strong>且在某点处<strong>各阶导数均存在</strong>，以该点处的各阶导数作为系数，构造出多项式来近似函数在该点邻域中任意点处的函数值，此多项式被称为<strong>泰勒多项式</strong>（Taylor polynomial）。<br>$$<br>f(x)&#x3D;f(a)+\frac{f^\prime(a)}{1!}(x-a)+\frac{f^{\prime\prime}(a)}{2!}(x-a)^2+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n+R_n(x)<br>$$</p><ul><li>当$R_n(x)&#x3D;o((x-a)^n)$时，得到带皮亚诺余项的泰勒公式；</li><li>当$\displaystyle{R_n(x)&#x3D;\frac{f^{(n+1)}(\theta)}{(n+1)!}(x-a)^{(n+1)}},\theta\in(a,x)$时，得到带拉格朗日余项的泰勒公式；</li><li>函数在$x&#x3D;0$处的泰勒公式被称为麦克劳林（Maclaurin）公式。</li></ul><p>泰勒公式建立了可导函数与其各阶导数之间的联系，同时用多项式对函数进行逼近。</p><h2 id="1-5-不定积分"><a href="#1-5-不定积分" class="headerlink" title="1.5 不定积分"></a>1.5 不定积分</h2><h3 id="1-5-1-不定积分的定义与性质"><a href="#1-5-1-不定积分的定义与性质" class="headerlink" title="1.5.1 不定积分的定义与性质"></a>1.5.1 不定积分的定义与性质</h3><ul><li>不定积分是求导和微分的逆运算，记为$\int{f(x)dx}$，不定积分与原函数的关系为$\int{f(x)dx}&#x3D;F(x)+C$。如果函数$f(x)$的原函数存在，则称其可积。</li></ul><h3 id="1-5-2-换元积分法"><a href="#1-5-2-换元积分法" class="headerlink" title="1.5.2 换元积分法"></a>1.5.2 换元积分法</h3><p><code>基于复合函数求导公式推出</code></p><ul><li>凑微分法：$\int{f(u(x))u^\prime(x)dx}&#x3D;\int{f(u)du}&#x3D;F(u)$。</li><li>变量替换法：令$x&#x3D;u(t)$，则$\int{f(x)dx}&#x3D;\int{f(u(t))du(t)}$</li></ul><h3 id="1-5-3-分部积分法"><a href="#1-5-3-分部积分法" class="headerlink" title="1.5.3 分部积分法"></a>1.5.3 分部积分法</h3><p><code>基于乘法求导公式推出</code><br>$$\int{f(x)g^\prime(x)dx}&#x3D;f(x)g(x)-\int{f^\prime(x)g(x)dx}$$</p><blockquote><p>刘维尔定理指出，一个初等函数如果有初等的原函数，则它一定能写成同一个微分域的函数加上有限项该域上函数的对数的线性组合，否则不存在初等的原函数。</p></blockquote><h2 id="1-6-定积分"><a href="#1-6-定积分" class="headerlink" title="1.6 定积分"></a>1.6 定积分</h2><h3 id="1-6-1-定积分的定义与性质"><a href="#1-6-1-定积分的定义与性质" class="headerlink" title="1.6.1 定积分的定义与性质"></a>1.6.1 定积分的定义与性质</h3><p>定积分将函数映射成实数，是和式的极限：$\displaystyle{\lim_{\Delta{x}\rightarrow0}\sum_{i&#x3D;1}^{n}{f(\xi_i)\Delta{x_i}}}$，$n$表示将区间$[a,b]$分成$N$份，定积分被记为$\int_{b}^{a}f(x)dx$。</p><ul><li>定积分具有线性性、区间可加性，将积分上下限颠倒，积分值相反。</li></ul><h3 id="1-6-2-牛顿-莱布尼茨公式-（Newton-Leibniz）"><a href="#1-6-2-牛顿-莱布尼茨公式-（Newton-Leibniz）" class="headerlink" title="1.6.2 牛顿-莱布尼茨公式 （Newton-Leibniz）"></a>1.6.2 牛顿-莱布尼茨公式 （Newton-Leibniz）</h3><p>微积分基本定理，建立了定积分与原函数的关系。如果函数在区间$[a,b]$内可积，则在此区间内定积分的值等于其原函数在区间两个端点处函数值之差<code>可用Lagrange中值定理证明</code>：<br>$$\int_{b}^{a}f(x)dx&#x3D;F(x)|_a^b$$</p><h3 id="1-6-4-变上限积分"><a href="#1-6-4-变上限积分" class="headerlink" title="1.6.4 变上限积分"></a>1.6.4 变上限积分</h3><p>积分上限为自变量$x$，变上限积分函数是被积分函数的一个原函数，概率论中连续随机变量的分布函数是典型的变上限积分函数。</p><h3 id="1-6-6-广义积分"><a href="#1-6-6-广义积分" class="headerlink" title="1.6.6 广义积分"></a>1.6.6 广义积分</h3><p>用于积分区间为无限或是积分区间有限但被积分函数无界的情况，又称反常积分。前者为无穷限广义积分，后者为瑕积分。</p><h2 id="1-7-常微分方程"><a href="#1-7-常微分方程" class="headerlink" title="1.7 常微分方程"></a>1.7 常微分方程</h2><h3 id="1-7-1-基本概念"><a href="#1-7-1-基本概念" class="headerlink" title="1.7.1 基本概念"></a>1.7.1 基本概念</h3><p><code>微分方程（Differential Equation，DE）是含有自变量、函数与其导数的方程，方程的解是函数。</code><br>含有自变量、函数以及函数各阶导数的方程称为常微分方程（Ordinary DE，ODE），它的解为一元函数。<br>$$f(x,y^{(n)},\cdots,y^\prime,y)&#x3D;0$$</p><ul><li>如果微分方程式未知函数以及各阶导数的一次方程，则称为线性微分方程，否则非线性微分方程。</li><li>如果线性微分方程中未知函数项以及各阶导数项的系数都是常数，则称为常系数线性微分方程。</li><li>并非所有微分方程的解都存在。对于初值问题，<strong>Cauchy-Lipschitz定理</strong>给出了解的存在性和唯一性的判别条件。即使解存在，也只有少数简单的微分方程可以求得解析解。在无法求得解析解时，可以利用数值计算的方法近似求解，常用的有<strong>Runge-Kutta法和Richardson外推法</strong>。</li></ul><h3 id="1-7-2-一阶线性微分方程"><a href="#1-7-2-一阶线性微分方程" class="headerlink" title="1.7.2 一阶线性微分方程"></a>1.7.2 一阶线性微分方程</h3><ul><li>齐次方程：$y^\prime+ay&#x3D;0$</li><li>非齐次方程：$y^\prime+ay&#x3D;b(x)$<blockquote><p>利用指数函数的特性，方程两边同乘以$e^{ax}$</p></blockquote></li></ul><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="Appx-I-常用极限"><a href="#Appx-I-常用极限" class="headerlink" title="Appx-I 常用极限"></a>Appx-I 常用极限</h3><blockquote><ol><li>$\displaystyle{\lim_{n\rightarrow+\infty}(1+\frac{1}{n})^n&#x3D;e}$</li><li>当$x\rightarrow0$时，典型的等价无穷小：$sin(x)\sim{x},arcsin(x)\sim{x},tan(x)\sim{x},ln(1+x)\sim{x},e^x-1\sim{x},1-cos(x)\sim{\frac{x^2}{2}},\sqrt[n]{1+x}-1\sim{\frac{x}{n}},a^x-1\sim{x\ln(a)}$</li></ol></blockquote><h3 id="Appx-II-基本函数的求导公式"><a href="#Appx-II-基本函数的求导公式" class="headerlink" title="Appx-II 基本函数的求导公式"></a>Appx-II 基本函数的求导公式</h3><table><thead><tr><th align="center">基本函数</th><th>求导公式</th></tr></thead><tbody><tr><td align="center">幂函数</td><td>$(x^a)^\prime&#x3D;ax^{a-1}$</td></tr><tr><td align="center">指数函数</td><td>$(a^x)^\prime&#x3D;a^x\ln{a}$</td></tr><tr><td align="center">对数函数</td><td>$(\log_ax)^\prime&#x3D;\frac{1}{\ln{a}}\frac{1}{x}$</td></tr><tr><td align="center">三角函数</td><td>$(sin(x))^\prime&#x3D;cos(x)$</td></tr><tr><td align="center">三角函数</td><td>$(cos(x))^\prime&#x3D;-sin(x)$</td></tr><tr><td align="center">三角函数</td><td>$(tan(x))^\prime&#x3D;sec^2(x)$</td></tr><tr><td align="center">三角函数</td><td>$(cot(x))^\prime&#x3D;-csc^2(x)$</td></tr><tr><td align="center">反三角函数</td><td>$(arcsin(x))^\prime&#x3D;\frac{1}{\sqrt{1-x^2}}$</td></tr><tr><td align="center">反三角函数</td><td>$(arccos(x))^\prime&#x3D;-\frac{1}{\sqrt{1-x^2}}$</td></tr><tr><td align="center">反三角函数</td><td>$(arctan(x))^\prime&#x3D;\frac{1}{1+x^2}$</td></tr></tbody></table><h3 id="Appx-III-基本函数的麦克劳林公式"><a href="#Appx-III-基本函数的麦克劳林公式" class="headerlink" title="Appx-III 基本函数的麦克劳林公式"></a>Appx-III 基本函数的麦克劳林公式</h3><table><thead><tr><th align="center">函数</th><th>麦克劳林公式</th></tr></thead><tbody><tr><td align="center">$\frac{1}{1-x}$</td><td>$1+x+x^2+\cdots+x^n+o(x^n)$</td></tr><tr><td align="center">$e^x$</td><td>$1+x+\frac{x^2}{2!}+\cdots+\frac{x^n}{n!}+o(x^n)$</td></tr><tr><td align="center">$\sin x$</td><td>$x-\frac{x^3}{3!}+\frac{x^5}{5!}-\cdots+\frac{(-1)^{n-1}x^{2n-1}}{(2n-1)!}+o(x^{2n-1})$</td></tr><tr><td align="center">$\cos x$</td><td>$1-\frac{x^2}{2!}+\frac{x^4}{4!}-\cdots+\frac{(-1)^nx^{2n}}{(2n)!}+o(x^{2n})$</td></tr><tr><td align="center">$\ln(1+x)$</td><td>$x-\frac{x^2}{2}+\frac{x^3}{3}-\cdots+\frac{(-1)^{n+1}x^n}{n}+o(x^n)$</td></tr></tbody></table><h3 id="Appx-IV-基本函数的积分公式"><a href="#Appx-IV-基本函数的积分公式" class="headerlink" title="Appx-IV 基本函数的积分公式"></a>Appx-IV 基本函数的积分公式</h3><table><thead><tr><th align="center">函数</th><th>积分公式</th></tr></thead><tbody><tr><td align="center">常数函数</td><td>$\int{a}dx&#x3D;ax+C$</td></tr><tr><td align="center">幂函数</td><td>$\int{x^a}dx&#x3D;\frac{1}{a+1}x^{a+1}+C,a\neq-1$</td></tr><tr><td align="center">幂函数</td><td>$\int{\frac{1}{x}}dx&#x3D;\ln$&amp;#124;$x$&amp;#124;$+C$</td></tr><tr><td align="center">指数函数</td><td>$\int{e^x}dx&#x3D;e^x+C$</td></tr><tr><td align="center">指数函数</td><td>$\int{a^x}dx&#x3D;\frac{1}{\ln{a}}a^x+C,a&gt;0,a\neq1$</td></tr><tr><td align="center">三角函数</td><td>$\int{sin(x)}dx&#x3D;-cos(x)+C$</td></tr><tr><td align="center">三角函数</td><td>$\int{cos(x)}dx&#x3D;sin(x)+C$</td></tr><tr><td align="center">三角函数</td><td>$\int{tan(x)}dx&#x3D;-\ln$&amp;#124;$cos(x)$&amp;#124;$+C$</td></tr><tr><td align="center">三角函数</td><td>$\int{cot(x)}dx&#x3D;\ln$&amp;#124;$sin(x)$&amp;#124;$+C$</td></tr><tr><td align="center">三角函数</td><td>$\int{\frac{1}{cos^2(x)}}dx&#x3D;tan(x)+C$</td></tr><tr><td align="center">三角函数</td><td>$\int{\frac{1}{sin^2(x)}}dx&#x3D;-cot(x)+C$</td></tr><tr><td align="center">反三角函数</td><td>$\int{\frac{1}{\sqrt{1-x^2}}}dx&#x3D;arcsin(x)+C$</td></tr><tr><td align="center">反三角函数</td><td>$\int{\frac{1}{\sqrt{1-x^2}}}dx&#x3D;-arccos(x)+C$</td></tr><tr><td align="center">反三角函数</td><td>$\int{\frac{1}{1+x^2}}dx&#x3D;arctan(x)+C$</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 《机器学习的数学》-雷明 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git使用教程</title>
      <link href="/2022/05/15/Git-skils/"/>
      <url>/2022/05/15/Git-skils/</url>
      
        <content type="html"><![CDATA[<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><blockquote><p>在本地安装好Git</p></blockquote><h1 id="Git常用命令"><a href="#Git常用命令" class="headerlink" title="Git常用命令"></a>Git常用命令</h1><span id="more"></span><pre><code class="bash">git init            # 初始化git add .            # 上传所有文件到缓存区git status -s        # 查看缓存区文件状态git rm -r --cache &lt;文件名&gt;    # 清除缓冲区文件git commit -m &#39;&#39;    # 上传日志git branch -M main            # 转换branch</code></pre><h1 id="Git远程连接GitHub"><a href="#Git远程连接GitHub" class="headerlink" title="Git远程连接GitHub"></a>Git远程连接GitHub</h1><h2 id="ssh-keygen常用命令"><a href="#ssh-keygen常用命令" class="headerlink" title="ssh-keygen常用命令"></a>ssh-keygen常用命令</h2><pre><code class="bash">ssh-keygen -t rsa -C &#39;注释&#39; -f &#39;~/.ssh/自定义文件名&#39;# 命令选项-b：指定密钥长度-e：读取openssh的私钥或者公钥文件-C：添加注释-f：指定用来保存密钥的文件名-i：读取未加密的ssh-v2兼容的私钥/公钥文件，然后在标准输出设备上显示openssh兼容的私钥/公钥-l：显示公钥文件的指纹数据-N：提供一个新密语-P：提供（旧）密语-q：静默模式-t：指定要创建的密钥类型</code></pre><blockquote><p><code>ssh-keygen</code>用于在<code>.ssh</code>文件夹下生成私钥文件<code>id_rsa</code>和公钥文件<code>id_rsa.pub</code><br>GitHub似乎不能识别自定义的文件名，所以为GitHub生成密钥时请使用默认文件名，即不指定-f</p></blockquote><h2 id="在GitHub中配置SSH"><a href="#在GitHub中配置SSH" class="headerlink" title="在GitHub中配置SSH"></a>在GitHub中配置SSH</h2><ol><li>登录GitHub，进入<code>Setting</code></li><li>找到<code>SSH and GPG keys</code>，点击<code>New SSH key</code>，自定义title</li><li>将<code>id_rsa.pub</code>的内容复制到<code>key</code>内<br><img src="/../images/Snipaste_2022-05-15_17-43-21.png" alt="Github设置SSH界面"></li></ol><h2 id="远程连接命令"><a href="#远程连接命令" class="headerlink" title="远程连接命令"></a>远程连接命令</h2><pre><code class="bash">ssh -T git@github.com # 测试是否连接到GitHubgit remote add origin git@github.com:&lt;用户名&gt;/&lt;仓库名&gt;.git  # 链接git remote -v        # 查看已建立的链接git remote rm XX    # 删除链接XXgit push -u origin master    # 上传到GitHub</code></pre><h1 id="Git修改commit注释"><a href="#Git修改commit注释" class="headerlink" title="Git修改commit注释"></a>Git修改commit注释</h1><h2 id="修改最后一次提交的commit注释"><a href="#修改最后一次提交的commit注释" class="headerlink" title="修改最后一次提交的commit注释"></a>修改最后一次提交的commit注释</h2><pre><code class="bash">git commit --amend  # 打开nano/vim编辑commitgit log             # 修改完保存退出，然后打印log查看是否修改成功# nano编辑器：如同记事本一样直接删除插入，C^表示Ctrl+，M^表示Alt+# vim编辑器： i-insert模式，:wq-保存退出</code></pre><h2 id="修改任意一次commit注释"><a href="#修改任意一次commit注释" class="headerlink" title="修改任意一次commit注释"></a>修改任意一次commit注释</h2><p><strong>查看提交日志，确认要修改的提交历史</strong></p><pre><code class="bash">git log</code></pre><p><strong>变基操作</strong></p><pre><code class="bash">git rebase -i &lt;commit range&gt;# 示例git rebase -i HEAD~2    # 表示当前提交到2次以前的提交git rebase -i &lt;hash值&gt;  # 表示hash值对应的某次提交</code></pre><p><strong>编辑commit</strong></p><blockquote><ol><li>将pick修改为edit</li><li>将原始注释改为新的注释</li><li>保存退出</li></ol></blockquote><p><strong>执行commit修改</strong></p><pre><code class="bash">git commit --amendgit rebase --continue</code></pre><h2 id="修改已经Push到远程的commit注释"><a href="#修改已经Push到远程的commit注释" class="headerlink" title="修改已经Push到远程的commit注释"></a>修改已经Push到远程的commit注释</h2><p>首先把最新版本从远程pull到本地，然后使用上面👆的方法修改，在强制push到远程。</p><pre><code class="bash">git push --force origin master</code></pre><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>高频命令</p><pre><code class="bash">git log --oneline  # 精简在一行显示一条history记录git log --graph    # 可视化提交的历史git branch [new branch name] [commit id]   # 从历史提交处创建分支</code></pre>]]></content>
      
      
      <categories>
          
          <category> Using Skills </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VScode配置Python远程调试</title>
      <link href="/2022/05/03/VScode-Python-Remote/"/>
      <url>/2022/05/03/VScode-Python-Remote/</url>
      
        <content type="html"><![CDATA[<h1 id="必要插件安装"><a href="#必要插件安装" class="headerlink" title="必要插件安装"></a>必要插件安装</h1><blockquote><ol><li>安装Remote-SSH （或者Remote Development这个插件合集，其中包含了SSH）</li><li>安装Python Extensions</li></ol></blockquote><span id="more"></span><h1 id="远程服务器配置"><a href="#远程服务器配置" class="headerlink" title="远程服务器配置"></a>远程服务器配置</h1><p>点击<code>Remote Explorer</code>，在下拉框中选择<code>SSH Targets</code>，点击⚙️配置<code>C:\Users\Administrator\.ssh\config</code></p><pre><code class="php"># Read more about SSH config files: https://linux.die.net/man/5/ssh_configHost Deep-H    HostName 192.168.1.XX   # 服务器IP    User XXXXX              # 用户名</code></pre><p>如果报错，尝试在VScode设置中搜索<code>Show Login Terminal</code>，勾选下方<code>Always reveal the SSH login terminal</code>。<br><img src="/../images/Snipaste_2022-05-03_18-01-50.png" alt="远程服务器配置"><br>配置完成后，在需要连接的条目上右键，选择<code>Connect to Host in New Window</code>或者<code>Connect to Host in Current Window</code></p><h1 id="配置服务器免密登录"><a href="#配置服务器免密登录" class="headerlink" title="配置服务器免密登录"></a>配置服务器免密登录</h1><h2 id="在本地生成公-私钥对"><a href="#在本地生成公-私钥对" class="headerlink" title="在本地生成公-私钥对"></a>在本地生成公-私钥对</h2><pre><code class="bash">ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot; -f ~/.ssh/文件名# -t type 指定要创建的密钥类型。可以使用：“rsa1”(SSH-1) “rsa”(SSH-2) “dsa”(SSH-2)# -b bits 指定密钥长度。对于RSA密钥，最小要求768位，默认是2048位。DSA密钥必须恰好是1024位(FIPS 186-2 标准的要求)。# -C comment 提供一个新注释# -f ~/.ssh/文件名 公钥和私钥的输出位置和对应的文件名# 最简单常用的用法：ssh-keygen -t rsa -C &quot;your_email@example.com&quot;</code></pre><p>命令运行之后，会在<code>.ssh</code>文件夹下生成<code>id_rsa.pub</code>和<code>id_rsa</code></p><h2 id="在远程端添加公钥"><a href="#在远程端添加公钥" class="headerlink" title="在远程端添加公钥"></a>在远程端添加公钥</h2><p>进入<code>~/.ssh</code>将<code>id_rsa.pub</code>的内容追加到<code>authorized_keys</code>中</p><pre><code class="bash"># 如果没有.ssh文件夹则创建mkdir ~/.ssh# 如果没有authorized_keys文件则创建touch ~/.ssh/authorized_keys# 确认.ssh和authorized_keys的权限，必要时修改权限chmod 700 ~/.sshchmod 600 ~/.ssh/authorized_keys# 追加id_rsa.pub内容，此命令需确保.pub文件与authorized_keys在同一目录下cat id_rsa.pub &gt;&gt; authorized_keys</code></pre><h2 id="修改Remote-SSH配置"><a href="#修改Remote-SSH配置" class="headerlink" title="修改Remote-SSH配置"></a>修改Remote-SSH配置</h2><pre><code class="php"># Read more about SSH config files: https://linux.die.net/man/5/ssh_configHost Deep-H    HostName 192.168.1.XX   # 服务器IP    User XXXXX              # 用户名    IdentityFile XXXX\.ssh\id_rsa     # 本地生成的私钥文件</code></pre><h1 id="Python代码调试"><a href="#Python代码调试" class="headerlink" title="Python代码调试"></a>Python代码调试</h1><h2 id="配置Python环境"><a href="#配置Python环境" class="headerlink" title="配置Python环境"></a>配置Python环境</h2><p><code>Run</code>-&gt;<code>Add Configuration...</code>-&gt;<code>Python File</code>打开<code>launch.json</code>文件，在该文件中配置：</p><pre><code class="json">&#123;    // Use IntelliSense to learn about possible attributes.    // Hover to view descriptions of existing attributes.    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387    &quot;version&quot;: &quot;0.2.0&quot;,    &quot;configurations&quot;: [        &#123;            &quot;name&quot;: &quot;Python: Current File&quot;,            &quot;type&quot;: &quot;python&quot;,            &quot;request&quot;: &quot;launch&quot;,            &quot;program&quot;: &quot;$&#123;file&#125;&quot;,            &quot;console&quot;: &quot;integratedTerminal&quot;,            &quot;justMyCode&quot;: true,            &quot;cwd&quot;: &quot;$&#123;fileDirname&#125;&quot;  // 设置相对路径，在debug时可以切换到当前文件所在目录        &#125;,    ]&#125;</code></pre><blockquote><p><code>F1</code>-&gt;<code>Python: Select Interpreter</code>选择Python解释器</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> VScode使用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VScode.config </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>凯文·凯利的103条人生建议</title>
      <link href="/2022/05/01/PostReading-KK-lifestream/"/>
      <url>/2022/05/01/PostReading-KK-lifestream/</url>
      
        <content type="html"><![CDATA[<p><strong>硅谷精神之父在70岁生日之际送给大家的一份礼物🎁</strong></p><ul><li>大约 99% 的情况下，正确开始的时间就是现在。<span id="more"></span></li><li>别人不会像你一样对你的财产印象深刻。（Morgan Housel《金钱心理学》里面的一句话，可粗浅理解为禀赋效应）</li><li>永远不要为你不想成为的人工作。</li><li>培养 12 个爱你的人，他们比 1200 万个喜欢你的人更有价值。</li><li>不要一直犯同样的错误，做新的试错。</li><li>如果你驻足欣赏音乐家或街头艺人表演超过一分钟，你就欠他们一美元。</li><li>在「但是」一词之前说的任何话都不算数</li><li>原谅别人时，对方可能不会注意，但你自己会被治愈。宽恕不为他人，是给自己的礼物。</li><li>礼貌无价。用完马桶后把盖子放下，等电梯时先出后进，把购物车放回指定区域。借东西归还时要比你得到时的状态更好（装满，清洁）。</li><li>两方发生争执时，找第三方。</li><li>效率被高估了，偷懒被严重低估了。定期安排的休息日、假期、轮休、漫无目的的散步和休息时间对于任何类型的最佳表现都是必不可少的。最好的职业道德包括良好的休息道德。</li><li>当你领导群体时，真正的工作是创造更多领导者，而不是更多的追随者。</li><li>私下批评，公开表扬。</li><li>人生的每一门课程会随时间依次呈现给你。掌握所需的一切能力取决于你的内心。真正吸取一次教训，你就会面临下一个挑战。如果你生存了下来，那意味着你还将得到教训。</li><li>从老师那里得到一切知识是学生的责任，而老师也有责任从学生身上学到一切。</li><li>如果在一场比赛中获胜变得过于重要，那就改变规则并让它变得更有趣。改变规则本身也可以是新的比赛。</li><li>求人赞助，他们会给你建议；向别人寻求建议，他们会给你钱。</li><li>生产力通常会有误区。不要以更快完成任务为导向，而要以更好完成任务为目标，寻找你想一直做下去的事。</li><li>即时向供应商、工人、承包商支付你所欠的款项。他们下次才会不遗余力地优先与你合作。</li><li>我们对自己说的最大的谎是「不需要写下来，我能记住」。</li><li>作为意识体，成长是由你愿意进行的不舒服谈话的数量来衡量的。</li><li>说话要自信，就好像你是对的，但同时要仔细听，就好像你是错的。</li><li>简单的测量：伸平手臂与肩同高，两个指尖的距离就是你的身高。</li><li>努力（锻炼、陪伴、工作）的一致性比数量更重要。没有什么比每天做的小事更重要了，这比你偶尔做的事情更重要。</li><li>做艺术不是自私的；它是为他人服务的。如果你不做你的事，你就是在欺骗我们。</li><li>永远不要问一个女人是否怀孕。她愿意的话会主动告诉你。</li><li>你需要三种品质：不放弃某件事情直到最终成功，放弃做无用功，以及信任其他人来帮助你区分这两种情况的能力。</li><li>公开演讲时，要经常停顿。在你以新的方式说某件事情之前暂停，在你说了你认为重要的事情之后暂停，并将暂停作为一种解脱，让听众吸收细节。</li><li>没有所谓的「准时」之说，你要么迟到，要么提前。你需要做出选择。</li><li>问问那些你钦佩的人：他们的运气时常出现在远离主要目标的弯道上。所以接受走弯路。对任何人来说，生活都不是一条直线。</li><li>在互联网上获得正确答案的最好方法是发布一个明显错误的答案，并等待有人来纠正你。（以维基百科作者坎宁安命名的定律，维基百科可能是本定律的最佳例子。）</li><li>通过奖励好的行为而不是惩罚坏的行为，你会得到 10 倍的效果，特别是在儿童和动物身上。</li><li>花尽可能多的时间精心设计电子邮件的主题行，因为它往往是人们阅读的唯一内容。</li><li>生活不是等待暴风雨过去，而是要学会在雨中起舞。</li><li>查看求职者推荐信时要意识到，雇主可能不愿意或被禁止说任何负面的东西，所以留下或发送一条信息说：「如果你强烈推荐这个求职者，他是超级棒的，请给我回信。」如果他们不回复，就当作是一种否定。</li><li>使用密码管理器：它们更安全、更简单、更强大。</li><li>接受教育学到的一半技能，是让你学会可以忽略一些东西。</li><li>设立大目标的优势在于，制定非常高的标准，即使失败，也可能是普通人眼里的成功。</li><li>了解自己的一个好方法是认真地反思别人身上让你反感的一切。</li><li>在酒店房间里要把你所有的东西都放在显眼的位置，集中到一个地方，不要放在抽屉里。这样你就不会落下任何东西。如果你需要把充电器之类的东西放在一边，就在它旁边放几件其他的大件物品，拉下三件物品的可能性比只拉下一件小。</li><li>拒绝或回避赞美是不礼貌的。即使你认为它不值得，也要感谢地接受它。</li><li>始终记得阅读纪念碑旁边的牌匾。</li><li>当你有一点成绩的时候，自己是冒牌者的感觉可能是真的。我在愚弄谁呢？但当你创造出只有你—以独特才能和经验—才能做到的事情时，那么你绝对不是冒牌货。这是天命所归。在只有你能做的事情上努力是你的职责。</li><li>在逆境中的表现，比你在顺境里所做的更重要。</li><li>做对他人有益的事。</li><li>当你打开油漆，哪怕只用一丁点，无论你多么小心，它总会沾到你的衣服。准备好着装。</li><li>为了让小孩子们在汽车旅行中表现良好，准备一袋他们最喜欢的糖果，每当他们不听话时，就向窗外扔一块。</li><li>你无法让聪明人仅仅为了钱而奋力工作。</li><li>当你不知道该为某项任务付给某人多少钱时，问他「怎样才算公平？」，他们的回答通常就是该有的价格。</li><li>90% 的东西都没有意义。如果你认为自己不喜欢歌剧、爱情小说、TikTok、乡村音乐、素食、NFT，保持尝试，看看是否能找到有意义的 10%。</li><li>人们会根据你对那些和你无利益冲突的人的态度好坏来评价你。</li><li>我们倾向于高估自己一天内能做的事，又低估自己在十年内能取得的成就。十年时间足以完成奇迹般的大事。一场漫长的比赛包含众多小目标，积累之后形成量变。</li><li>感谢那位改变了你一生的老师。</li><li>你不可能用别人无法理解的逻辑给别人讲道理。</li><li>最好的工作可能是你不能胜任的工作，因为它会最大限度利用你的能力。所以请申请你不能胜任的工作。</li><li>买旧书。他们有和新书一样的字。也可以去图书馆。</li><li>你可以做任何你想做的事，所以做一个提前结束会议的人。</li><li>智者说：在你说话之前，让你的话通过三道门。在第一个大门问自己「这是真的吗？」 在第二道门前问「有必要吗？」在第三道门前问「是好的吗？」</li><li>走楼梯。</li><li>你为某样东西实际支付的费用至少是其标价的两倍，因为你需要花费时间精力和钱来设置、学习、维护、修理，以及最终的处置。不是所有的价格都体现在标签上。实际成本是标价的 2 倍。</li><li>当你到达酒店的房间时，确认紧急出口。这只需要一分钟时间。</li><li>回答「我现在应该做什么？」的唯一有效方式是首先解决「我应该成为谁？」的问题。</li><li>在高于平均水平的时间内持续的平均水平回报会产生非凡的效果。请买长线。</li><li>对无礼的陌生人保持极致风度是很令人感动的。</li><li>一个不太聪明但容易沟通的人，可能比一个超级聪明但难以沟通的人做得更好。这是件好事，因为提高你的沟通能力比提高你的智力要容易得多。</li><li>偶尔受骗是相信每个人的优点的小代价，因为当你相信别人的优点时，他们通常也会给你最好的回报。</li><li>艺术的表现形式是无穷无尽的。</li><li>要想在孩子身上取得最好的效果，只花你认为应该花的一半的钱，但花双倍的时间陪他们在一起。</li><li>购买最新的本地旅游指南。每年扮演一次游客，你会学到很多东西。</li><li>不要排队等候吃著名的东西，一般都不值得。</li><li>想要迅速弄清新认识的一个人的真实性格，让他们连接到一个慢得不能再慢的网络上。观察一下。</li><li>世俗化成功的处方：做一些奇怪的事情。让你的怪异成为一种习惯。</li><li>备份你的备份。至少要有一个物理备份和一个云端的备份，两者最好都有多于一个备份。如果你丢失了你的所有数据、照片、笔记，你会花多少钱来找回它们？与遗憾相比，备份很便宜。</li><li>不要相信你认为自己相信的一切。</li><li>想要发出紧急信号，使用三法则；3 声喊叫、3 声喇叭声或 3 声口哨声。</li><li>在餐厅，您会点一些你知道好吃的东西，还是尝试一些新的东西？探索新事物与利用新事物的最佳平衡是：1:3。将 1&#x2F;3 的时间用于探索，将 2&#x2F;3 的时间用于深化。随着年龄的增长，更难花时间去探索，因为它似乎没有成效，目标就成了剩下的 1&#x2F;3。</li><li>真正的好机会不容易找到，更不会明码标注出来。</li><li>与某人初次见面并作介绍时，要与对方有眼神接触并默数到 4，这样你们都会记住对方。</li><li>如果你发现自己在想 「我的好刀（或者我的好笔）在哪里」，这意味着你有其他坏的东西，把它们扔掉。</li><li>当你陷入困境时，尝试向别人解释你的问题。通常当你提出问题的时候就能找到一个解决方案。解释问题是解决困境的一种方法。</li><li>在购买花园水管、延长线或梯子时，要买一个比你认为需要的长得多的，那才是正确的尺寸。</li><li>不必费心地与旧事物抗争，只需要创造新事物。</li><li>只需要给别人以赞赏，他们就能取得超出能力范围的伟大成就。</li><li>说起一段历史，巅峰之年总是第十几年，就像每个人十多岁的时候，那是每个人最好的年华。</li><li>从让一个人生气的事情大小就能看出一个人的价值。（这是丘吉尔的名言）</li><li>当你面向听众表达自己的观点时，最好将目光集中在几个人身上，而不是扫向整个房间，你的眼神代表你是否相信自己所说的话。</li><li>习惯远比一时兴起可靠得多，你需要养成习惯才能获得进步。就像健身锻炼，不要专注于塑形，而是要成为从不错过锻炼的人。</li><li>谈判时，不要以争取更大的蛋糕为目标，要以创造一个更大的蛋糕为目标。</li><li>如果你把今天所做的事情重复 365 次，明年你会成为你想成为的人吗？</li><li>你只能看到一个人的 2%，同样他们也只能看到你的 2%，你需要隐藏自己的 98%。</li><li>你的时间和空间是有限的，移除、舍弃、扔掉那些已无法让你快乐的事物，以便为新的快乐腾出空间。</li><li>我们的后代将取得令我们震惊的突破，如果我们有足够的想象力，可以设想一下他们利用目前已有的条件能够创造出什么，可以大胆猜测一下。</li><li>如果你想获得丰厚的回报，那么即使是不感兴趣的事，你也要充满好奇。</li><li>专注于方向而不是目的地。没有人能知道自己的命运，但坚持朝着正确的方向努力，你就会到达想去的地方。</li><li>每一个突破最开始都是荒谬可笑的，事实上如果它最开始不荒谬，那它就算不上突破。</li><li>如果你借给别人 20 美元，而他为了不还钱选择不再见你，那你们的关系只值 20 美元。</li><li>复制他人的成功是一个好的开始，复制自己是一个令人失望的结束。</li><li>为一份新工作谈判薪资的最佳时机是对方想要你之后，而不是之前。然后，双方都说出一个数字就成了一场博弈，但在你工作之前让雇佣者给出一个薪资水平对你来说是有益的。</li><li>与其躲避生活中的意外，不如直接面对这些风险。</li><li>如果你使用信用卡租车，不要购买额外的保险。</li><li>如果你对一个问题的观点能够从你对另一个问题的观点中预测出来，那么你可能正处于某种意识形态的掌控之中。实际上当你真正独立地思考一个问题，你的结论很难被预测。</li><li>争取在离世之前花光自己所有的钱。相比于给你的受益人，自己全部花光更有趣也更有意义。你填的最后一张支票应该是给殡仪馆，还应该拒付。</li><li>防止变老的主要方法是对新鲜事物保持好奇和惊讶。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 读后感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Git在VScode中的配置</title>
      <link href="/2022/05/01/VScode-Git/"/>
      <url>/2022/05/01/VScode-Git/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本地安装Git，且已在环境变量中配置<br>Windows系统下的使用教程</p></blockquote><h1 id="在VScode中配置Git路径"><a href="#在VScode中配置Git路径" class="headerlink" title="在VScode中配置Git路径"></a>在VScode中配置Git路径</h1><p>在<code>settings.json</code>中添加配置</p><span id="more"></span><pre><code class="json">&quot;git.path&quot;: &quot;%Git安装目录%/Git/cmd/git.exe&quot;,</code></pre><h1 id="在VScode终端中添加Git-Bash"><a href="#在VScode终端中添加Git-Bash" class="headerlink" title="在VScode终端中添加Git Bash"></a>在VScode终端中添加Git Bash</h1><p>由于原有的Git Bash已经弃用，直接在<code>settings.json</code>中间中添加Git Bash路径是无法正确添加的，将得到如下错误信息：</p><blockquote><p>Value is not accepted. Valid values: “PowerShell”, “Windows PowerShell”, “Command Prompt”, “JavaScript Debug Terminal”.</p></blockquote><p>正确的做法是使用用新的规则配置：</p><pre><code class="json">&quot;terminal.integrated.profiles.windows&quot;: &#123;        &quot;Git-Bash&quot;: &#123;            // &quot;source&quot;: &quot;Git Bash&quot;, // 不可使用Git Bash            &quot;icon&quot;: &quot;logo-github&quot;,   // 设置图标，可选            &quot;path&quot;: [                &quot;%Git安装目录%/Git/bin/bash.exe&quot;            ],            &quot;args&quot;: []        &#125;,    &#125;,    &quot;terminal.integrated.defaultProfile.windows&quot;: &quot;Git-Bash&quot;, // 设置为默认，可选</code></pre><h1 id="文件改动无法显示Git的修改颜色标记"><a href="#文件改动无法显示Git的修改颜色标记" class="headerlink" title="文件改动无法显示Git的修改颜色标记"></a>文件改动无法显示Git的修改颜色标记</h1><p>这是由于VScode无法识别软连接文件路径造成的，只需在真实路径下打开文件就可以正常显示了。</p><h1 id="好用的插件"><a href="#好用的插件" class="headerlink" title="好用的插件"></a>好用的插件</h1><p><code>GitLens</code>: 强大的历史查看功能</p><h2 id="与历史版本对比修改差异"><a href="#与历史版本对比修改差异" class="headerlink" title="与历史版本对比修改差异"></a>与历史版本对比修改差异</h2><blockquote><p>使用<code>git reset</code>回退版本会将原本在回退版本之后的提交全部删除，如果仅是为了查看与某一次历史版本的修改差异可以使用如下方法：</p></blockquote><p><img src="/../images/Snipaste_2022-06-23_16-31-26.png" alt="查看单个文件的历史版本"><br>选择<code>Open Changes with Working File</code>查看当前工作目录下文件与某个历史文件之间的差异。</p>]]></content>
      
      
      <categories>
          
          <category> VScode使用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VScode.config </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解决LaTeX不显示参考文献问题</title>
      <link href="/2022/04/30/VScode-LaTex-no-bib/"/>
      <url>/2022/04/30/VScode-LaTex-no-bib/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本人使用的LaTeX环境为：<br>VScode 1.66.2<br>LaTeX-workshop v8.25.0<br>MikTex </p></blockquote><p>由于直接复制网络上他人的教程导致无法适用于自己的环境，bib的reference始终无法正确识别 [在正文中以？出现]，经过不断的尝试，终于找到原因：</p><span id="more"></span><p><strong>编译链设置不对</strong><br>网上的普遍配置是大多如下：</p><pre><code class="json">// 用于配置编译链&quot;latex-workshop.latex.recipes&quot;: [    &#123;        &quot;name&quot;: &quot;xelatex&quot;,        &quot;tools&quot;: [            &quot;xelatex&quot;        ]    &#125;,    &#123;        &quot;name&quot;: &quot;latexmk&quot;,        &quot;tools&quot;: [            &quot;latexmk&quot;        ]    &#125;,    &#123;        &quot;name&quot;: &quot;pdflatex-&gt;bibtex-&gt;pdflatex*2&quot;,        &quot;tools&quot;: [            &quot;pdflatex&quot;,            &quot;bibtex&quot;,            &quot;pdflatex&quot;,            &quot;pdflatex&quot;        ]    &#125;],</code></pre><p>当默认以第一种方式编译时，我的环境下是无法识别bib的，解决办法很简单就是使用<code>pdflatex-&gt;bibtex-&gt;pdflatex*2</code>为默认编译链。</p>]]></content>
      
      
      <categories>
          
          <category> VScode使用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VScode.issue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>希尔伯特空间</title>
      <link href="/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4/"/>
      <url>/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4/</url>
      
        <content type="html"><![CDATA[<h3 id="等量变换"><a href="#等量变换" class="headerlink" title="等量变换"></a>等量变换</h3><p><strong>概率场景：</strong></p><p>由于概率分布的积分对应面积为1，所以在微分角度上，两个分布的变化量是相等的，则有：<br>$$<br>p(x)dx&#x3D;p(y)dy \<br>p(y)&#x3D;p(x)\left|\frac{dx}{dy}\right|<br>$$</p><span id="more"></span><h3 id="Hilbert-Space-希尔伯特空间"><a href="#Hilbert-Space-希尔伯特空间" class="headerlink" title="Hilbert Space 希尔伯特空间"></a>Hilbert Space 希尔伯特空间</h3><p>线性空间（向量空间）关注的是向量的位置，对于一个线性空间，知道<code>基</code>（相当于三维空间中的坐标系）便可确定空间中元素的坐标（即位置）；线性空间只定义了<code>加法和数乘</code>运算。</p><ul><li><p>赋范线性空间：如果我们想知道向量的长度，定义<code>范数</code></p></li><li><p>度量空间：如果我们想知道向量间的距离，定义<code>距离</code></p></li><li><p>内积空间：如果我们想知道向量的夹角，定义<code>内积</code></p></li><li><p>欧式空间：定义了内积的有限维实线性空间</p><blockquote><p>有限维：设$A$是线性空间$E$的一个线性无关子集，我们设$A$的维度为$D_E$。当$D_E&lt; +\infty$时，称$E$为有限维的，否则称$E$为无限维的，即欧式空间中没有无限维的计算的概念  </p></blockquote></li><li><p>完备空间：如果我们想研究收敛性（极限），定义<code>完备</code></p><blockquote><p>完备性：是在极限的基础上衍生的概念。例如在有理数集上的一个序列{1，1.4，1.41，1.414，1.4142…}，可知此序列极限为2根号2，而根号2为无理数，不属于有理数集，即有理数集不具备完备性，也就是有理数集不具备极限的概念，因为有理数集上的数都是确定的</p></blockquote></li><li><p>Banach空间：完备的赋范线性空间</p></li><li><p><kbd>Hilbert空间</kbd>：完备的内积空间（极限运算中不能跑出度量的范围；是欧式空间的一种推广；由于极限计算针对的一般是函数，所以Hilbert空间一般是指函数空间，可以定义函数的內积）</p><blockquote><p>函数的內积：我们有两个函数$f(x)$与$g(x)$与区间$[a,b]$，且两函数在该区间上可积且平方可积。则积分：$\int_a^b f(x)g(x)dx$，我们称之为函数的内积，函数的内积常记作$&lt;f(x),g(x)&gt;$，如果是离散的函数则我们可以直接：$\sum{f(x)\times g(x)}$，用矩阵表示就是$F(X)G(X)$</p></blockquote></li></ul><p>希尔伯特空间是一个完备的空间，其上所有的<code>柯西列</code>等价于<code>收敛列</code>，从而微积分中的大部分概念都可以无障碍地推广到希尔伯特空间中。</p><blockquote><p><strong>柯西序列</strong><br>定义：在具有度量$d$的度量空间$S$中，一个序列为柯西序列，若其符合以下条件：<br>对于任意的实数$\epsilon &gt; 0$，存在一正整数$N$，使得每当$m,n&gt;N$时都有$d(a_m, a_n)&lt;\epsilon$</p><p>在数学中，一个柯西列是指一个这样一个序列，它的元素随着序数的增加而愈发靠近。更确切地说，在去掉有限个元素后，可以使得余下的元素中任何两点间的距离的最大值不超过任意给定的正的常数。</p><p>柯西列的定义依赖于距离的定义，所以只有在度量空间(metric space)中柯西列才有意义。在更一般的一致空间(uniform space)中，可以定义更为抽象的柯西滤子(Cauchy filter)和柯西网(Cauchy net)。</p><p>柯西序列的重要作用是定义“完备空间”。完备空间是指一种度量空间，它的所有柯西序列（如果有的话），都收敛在这个空间自己里面。</p><p>在完备空间（complete space）中，所有的柯西列都有极限，这就让人们可以在不求出这个极限（如果存在）的情况下，利用柯西列的判别法则证明该极限是存在的。柯西列在构造具有完备性的代数结构的过程中也有重要价值，如构造实数。</p><p>1.对于在某度量空间内的柯西序列，它的极限不一定在相同的度量空间内。如有理柯西序列可导出无理极限。（事实上，一种实数构造就是用这种方法）</p><p>2.任何收敛列必然是柯西列，任何柯西列必然是有界序列。</p></blockquote><p>希尔伯特空间为基于任意正交系上的多项式表示的傅立叶级数和傅立叶变换提供了一种有效的表述方式，而这也是泛函分析的核心概念之一。</p><img src="/images/v2-be26b2ba1df2edc9636647a28b22238d_1440w.jpg" alt="空间关系图" style="zoom:80%;" /><p><strong>Reproducing Kernel Hilbert Space再生核希尔伯特空间：</strong></p><p><kbd>Kernel</kbd>任何半正定的函数都可以作为核函数（Merrcer定理：充要条件）</p><blockquote><p><strong>Merrcer定理：</strong>所谓半正定函数$f(x_i, x_j)$，是指拥有训练数据集合$(x_1, x_2,\dots,x_n)$，我们定义一个矩阵的元素$a_{ij}&#x3D;f(x_i, x_j)$，这个矩阵是半正定的，那么$f(x_i, x_j)$就成为半正定的函数。</p></blockquote><p>再生核希尔伯特空间是支持监督学习（SVM）等监督学习模型的理论基础，实际上再生核希尔伯特空间就是是由核函数构成的希尔伯特空间，这里的再生指的是再生性，这里的核函数比如LibSVM中自带的几类：</p><ol><li>线性：$K(v_1,v_2)&#x3D;&lt;v_1,v_2&gt;$</li><li>多项式：$K(v_1,v_2)&#x3D;(\gamma&lt;v_1,v_2&gt;+c)^n$</li><li>高斯核：$K(v_1,v_2)&#x3D;\exp(-\gamma|v_1-v_2|^2)$</li><li>Sigmoid：$K(v_1,v_2)&#x3D;\tanh(\gamma&lt;v_1,v_2&gt;+c)$</li></ol><blockquote><p>设$\mathcal{X}$是输入空间(欧式空间$R^n$的子集或离散集合)，又设$\mathcal{H}$是特征空间(希尔伯特空间)，如果存在一个$\mathcal{X}$到$\mathcal{H}$的映射$\phi(x):\mathcal{X}\rightarrow\mathcal{H}$使得对所有$x,z\in\mathcal{X}$，函数$K(x,z)$满足条件$K(x,z)&#x3D;\phi(x)\cdot\phi(z)$则称$K(x,z)$为核函数，$\phi(x)$为映射函数，式中$\phi(x)\cdot\phi(z)$为$\phi(x)$和$\phi(z)$的內积。</p></blockquote><p>再生性指的就是<strong>原本函数之间计算内积需要算无穷维的积分（也就是这个映射函数可以映射到高维甚至无穷维（高斯核），而计算无穷维的积分是非常复杂的），但是现在只需要算核函数可以。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 专题学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>科学空间笔记</title>
      <link href="/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E7%A7%91%E5%AD%A6%E7%A9%BA%E9%97%B4%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E7%A7%91%E5%AD%A6%E7%A9%BA%E9%97%B4%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="关于VAE"><a href="#关于VAE" class="headerlink" title="关于VAE"></a>关于VAE</h2><h2 id="关于Flow"><a href="#关于Flow" class="headerlink" title="关于Flow"></a>关于Flow</h2><p><strong>缺点</strong></p><p><code>由于必须保证逆变换简单和雅可比行列式容易计算，那么每一层的非线性变换能力都很弱。所以为了保证充分的拟合能力，模型就必须堆得非常深，计算量非常大。</code></p><span id="more"></span><h2 id="余弦相似度的假设"><a href="#余弦相似度的假设" class="headerlink" title="余弦相似度的假设"></a><a href="https://kexue.fm/archives/8069">余弦相似度的假设</a></h2><p>$$<br>cos(x,y)&#x3D;\frac{\sum^d_{i&#x3D;1}{x_iy_i}}{\sqrt{\sum^d_{i&#x3D;1}{x^2_i}}\sqrt{\sum^d_{i&#x3D;1}{y^2_i}}}<br>$$</p><blockquote><p>上式等号只在<strong>标准正交基下成立</strong>。向量的“夹角余弦”本身是具有鲜明的几何意义的，但上式右端只是坐标的运算，坐标依赖于所选取的坐标基，基底不同，内积对应的坐标公式就不一样，从而余弦值的坐标公式也不一样。</p></blockquote><p><strong>如果用公式算余弦值来比较句子相似度时表现不好，那么原因可能就是此时的句向量所属的坐标系并非标准正交基。</strong></p><p>原则上我们无法确定此时向量所属坐标系，但是我们在给向量集合选择基底时，可以依据猜测：<code>会尽量地用好每一个基向量，从统计学的角度看，这就体现为每个分量的使用都是独立的、均匀的，如果这组基是标准正交基，那么对应的向量集应该表现出“各项同性”来。</code>【<strong>如果一个向量的集合满足各向同性，那么我们可以认为它源于标准正交基</strong>】</p><p><strong>标准化协方差矩阵</strong></p><p>标准正态分布的均值为0、协方差矩阵为单位阵。假设向量集合${x_i}^N_{i&#x3D;1}$执行变换$\hat{x}<em>i&#x3D;(x_i-\mu)W$使得${\hat{x}<em>i}^N</em>{i&#x3D;1}$的均值为0，协方差矩阵为单位阵，这个操作对应于数据挖掘中的白化操作(Whitening)，具体如下：<br>$$<br>均值为0则：\mu&#x3D; \frac{1}{N}\sum</em>{i&#x3D;1}^Nx_i\<br>原始协方差：\Sigma&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N(x_i-\mu)^T(x_i-\mu)&#x3D;\left(\frac{1}{N}\sum_{i&#x3D;1}^N{x^T_ix_i}\right)-\mu^T\mu \<br>变换后协方差：\hat\Sigma&#x3D;W^T\Sigma W&#x3D;I\rightarrow\Sigma&#x3D;(W^T)^{-1}W^{-1}&#x3D;(W^{-1})^TW^{-1}\<br>协方差矩阵是半正定对称矩阵，可以被SVD分解：\Sigma&#x3D;U\Lambda U^T\rightarrow W^{-1}&#x3D;\sqrt{\Lambda}U^T\rightarrow W&#x3D;U\sqrt{\Lambda^{-1}}<br>$$</p><h2 id="关于Attention"><a href="#关于Attention" class="headerlink" title="关于Attention"></a>关于Attention</h2><pre><code>RNN因其本质是马尔科夫决策过程，无法很好的学习全局信息。CNN方便并行，而且容易捕捉到一些全局的结构信息。RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好：$y_t=f(y_&#123;t-1&#125;,x_t)$CNN事实上只能获取局部信息，是通过层叠来增大感受野：$y_t=f(x_&#123;t-1&#125;,x_t,x_&#123;t+1&#125;)$ [3x的kernel]</code></pre><p>Attention的思路最为粗暴，它一步到位获取了全局信息！它的解决方案是：$y_t&#x3D;f(x_t,A,B)$ A和B是额外引入的序列，如果$A&#x3D;B&#x3D;X$就是self-attention。Attention的意思是直接将$x_t$与原来的每个词进行比较，最后算出$y_t$。<br>$$<br>Attention(Q,K,V)&#x3D;softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>$Q\in R^{n\times d_k}$，$K\in R^{m\times d_k}$，$V\in R^{m\times d_v}$。如果忽略激活函数softmax的话，那么事实上它就是三个矩阵相乘，最后的结果就是一个$n\times d_v$的矩阵。于是我们可以认为：这是一个Attention层，将序列Q编码成了一个新的的序列。事实上$Q,K,V$分别是$query,key,value$的简写，那么上式的意思就是通过$query$与各个$key$内积的并softmax的方式，来得到$query$与各个$value$的相似度，然后加权求和，得到一个向量。其中因子$d_k$起到调节作用，使得内积不至于太大（太大的话softmax后就非0即1了，不够“soft”了）。</p><pre><code>50维的词向量，将每一维打乱重新排个序（当然整体要按同样的顺序来重新排序），它还是等价于原来的词向量。既然相加的对象（词向量）都没有局部结构，我们也没必要强调被加的对象（Position_Embedding）的局部结构（也就是交叉连接）了。</code></pre><h2 id="一些观点"><a href="#一些观点" class="headerlink" title="一些观点"></a>一些观点</h2><pre><code>数据扩增是将我们的先验知识融入到模型中的一种方案。mixup相当于一个正则项，它希望模型尽可能往线性函数靠近，也就是说，既保证模型预测尽可能准确，又让模型尽可能简单。</code></pre>]]></content>
      
      
      <categories>
          
          <category> 专题学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高斯分布</title>
      <link href="/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/"/>
      <url>/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<h3 id="单变量高斯分布-univariate-Gaussian"><a href="#单变量高斯分布-univariate-Gaussian" class="headerlink" title="单变量高斯分布(univariate Gaussian )"></a>单变量高斯分布(univariate Gaussian )</h3><p>$$<br>f(x)&#x3D;\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)<br>$$</p><span id="more"></span><h3 id="单高斯分布"><a href="#单高斯分布" class="headerlink" title="单高斯分布"></a>单高斯分布</h3><p>$$<br>N(x ; u, \Sigma)&#x3D;\frac{1}{\sqrt{2 \pi}|\Sigma|} \exp \left[-\frac{1}{2}(x-u)^{T}\Sigma^{-1} (x-u)\right]<br>$$</p><p>单高斯与单变量高斯的区别在于前者维度为$d$，后者维度为$1$。对于单高斯模型，$\mu$通常代表训练样本的均值，$\Sigma$代表样本的方差。</p><h3 id="高斯混合模型（GMM）"><a href="#高斯混合模型（GMM）" class="headerlink" title="高斯混合模型（GMM）"></a>高斯混合模型（GMM）</h3><p>$$<br>\operatorname{Pr}(x)&#x3D;\Sigma_{k&#x3D;1}^{K} \pi_{k} N\left(x ; u_{k}, \Sigma_{k}\right)<br>$$</p><p>依据全概率公式，$\pi_{k}$是选中参数$u_{k}$和$\Sigma_{k}$的概率，又称权重因子。EM算法用于优化GMM。</p>]]></content>
      
      
      <categories>
          
          <category> 专题学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第5章 决策树</title>
      <link href="/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC5%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/"/>
      <url>/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC5%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<p>决策树（decision tree）是一种基本的分类和回归方法，它可以任务是if-then规则（<strong>互斥且完备</strong>）的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。<br>因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题，得到的决策树是sub-optimal的。<br>决策树的生成对应于模型的局部选择<code>局部最优</code>，决策树的剪枝对应于模型的全局选择<code>全局最优</code>。</p><span id="more"></span><h3 id="1-特征选择"><a href="#1-特征选择" class="headerlink" title="1. 特征选择"></a>1. 特征选择</h3><p>特征现在在于选取对训练数据具有分类能力的特征，通常特征选择的准则是信息增益或信息增益比。<br><strong>熵</strong><br>熵（entropy）是表示随机变量不确定性的度量：$H(X)&#x3D;-\sum_i^N{p_i\log{p_i}},\ 0\le{H(p)\le\log{n}}$<br>条件熵（conditional entropy）表示在已知随机变量X的条件下随机变量Y的不确定性：$H(Y|X)&#x3D;\sum_i^N{p_iH(Y|X&#x3D;x_i)}$</p><p><strong>信息增益</strong>（Information gain）表示得知特征X的信息而使得类Y的信息的确定性减少的程度：<br>$g(D,A)&#x3D;H(D)-H(D|A)$<br>上式中$H(D)[empirical\ entropy]$表示对数据集D进行分类的不确定性，$H(D|A)[empirical\ comditional\ entropy]$表示在特征A给定的条件下对数据集D进行分类的不确定性。一般地，熵与条件熵的差称为互信息（mutual Information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p><p><strong>信息增益比</strong>（Information gain ratio）信息增益存在偏向于选择取值较多的特征，ratio可以避免此问题。<br>$g_R(D,A)&#x3D;\frac{g(D,A)}{H_A(D)},\ H_A(D)&#x3D;-\sum_{i&#x3D;1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|},\ n是特征A的取值个数$</p><h3 id="2-决策树的生成"><a href="#2-决策树的生成" class="headerlink" title="2. 决策树的生成"></a>2. 决策树的生成</h3><p><strong>ID3：</strong>核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。ID3相当于用极大似然法进行概率模型的选择。<strong>C4.5：</strong>改进ID3，并使用信息增益比。</p><h3 id="3-决策树的剪枝（pruning）"><a href="#3-决策树的剪枝（pruning）" class="headerlink" title="3. 决策树的剪枝（pruning）"></a>3. 决策树的剪枝（pruning）</h3><p>决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现。<br>设树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该结点有$N_t$个样本点，其中$k$类样本点有$N_{tk}$个，则决策树学习的损失函数可以定义为：<br>$C_\alpha(T)&#x3D;\sum_t^{|T|}N_tH_t(T)+\alpha|T|&#x3D;C(T)+\alpha|T|\ H_t(T)&#x3D;-\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}\C(T)&#x3D;\sum_t^{|T|}{N_tH_t(T)&#x3D;-\sum_t^{|T|}\sum_k^K{N_{tk}}\log\frac{N_{tk}}{N_t}}$<br>$C(T)$表示模型对训练数据的预测误差（模型与训练数据的拟合程度），$|T|$表示模型复杂度，$\alpha\ge0$控制两者间的影响。较大的$\alpha$促使选择较简单的模型，较小的$\alpha$促使选择较复杂的模型，$\alpha&#x3D;0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。剪枝就是当$\alpha$确定时，选择损失函数最小的模型。损失函数的极小化等价于正则化的极大似然估计。</p><blockquote><p><strong>输入：</strong>生成算法产生的整个树$T$，参数$\alpha$<br><strong>输出：</strong>修剪后的子树$T_{\alpha}$<br>1-计算每个结点的经验熵<br>2-递归地从树的叶结点向上回缩<br>3-根据回缩前B后A损失函数值$C_{\alpha}(T_A)\le C_{\alpha}(T_B)$，剪掉后损失减少则剪枝（重复）</p></blockquote><h3 id="4-CART算法"><a href="#4-CART算法" class="headerlink" title="4. CART算法"></a>4. CART算法</h3><p>分类与回归树（CART）是在给定输入随机变量X条件下输出随机变量Y的条件概率分布。（左是右否的二叉树）</p><blockquote><p><strong>生成：</strong>基于训练数据集生成决策树，生成的树要尽量大。<br><strong>剪枝：</strong>用验证集对已生成的树进行剪枝并选择最优子树，用损失函数最小作为剪枝的标准。</p></blockquote><p><strong>CART生成</strong><br><code>回归树：</code>平方误差最小化准则<br><code>分类树：</code>基尼指数最小化准则</p><blockquote><p><strong>最小二乘回归树生成算法</strong><br><strong>输入：</strong> 训练数据集$D$<br><strong>输出：</strong> 回归树$f(x)$<br>在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉树：1-选择最优切分变量$j$与切分点$s$，求解$\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$遍历变量$j$对固定的切分变量$j$扫描切分点$s$选择使式达到最小值的对$(j,s)$。2-用选定的$(j,s)$划分区域并决定相应的输出值$R_1(j,s)&#x3D;{x|x^{(j)}\le s},\ \ R_2(j,s)&#x3D;{x|x^{(j)}\gt s},\ \ \hat{c}<em>m&#x3D;\frac{1}{N_m}\sum</em>{x_i\in R_m(j,s)}y_i,x\in R_m$。3-重复1、2。4-将输入空间划分为$M$个区域$R_1,R_2,\cdots,R_M$生成决策树：$f(x)&#x3D;\sum_m^M\hat{c}_mI(x\in R_m)$</p><p><strong>基尼指数分类树生成算法</strong><br><strong>输入：</strong> 训练数据集$D$，停止计算的条件<br><strong>输出：</strong> CART决策树<br>从根结点开始构建二叉树：1-设结点的训练数据集为$D$，计算现有特征对数据集的基尼指数$Gini(D)$。 根据特征$A&#x3D;a$将数据集分成$D_1$和$D_2$计算基尼指数$Gini(D,A)$。2-在所有特征与切分点对$&lt;A,a&gt;$中选择基尼指数最小的作为最优特征与最优切分点，由此生成两个子结点。3-重复1，2，直到满足停止条件。4-生成CART树。</p><p><code>基尼指数：</code>$K$类中样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数为：$Gini(p)&#x3D;\sum_k^K{p_k(1-p_k)}&#x3D;1-\sum_k^K{p_k^2}$。对于给定样本集合$D$，基尼指数为$Gini(D)&#x3D;1-\sum_k^K(\frac{|C_k|}{|D|})^2$，对于在特征$A$条件下的集合$D$的基尼指数为$Gini(D,A)&#x3D;\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$。基尼指数越大，样本集合的不确定性也越大（与熵类似）。<br><code>排列组合：</code>排列$A_n^m&#x3D;\frac{n!}{(n-m)!}$， 组合$C_n^m&#x3D;\pmatrix{\begin{matrix}n\m\end{matrix}}&#x3D;\frac{A_n^m}{m!}&#x3D;\frac{n!}{m!(n-m)!}&#x3D;C(n,m)&#x3D;C(n,n-m)$<br><code>二项式定理：</code>$(a+b)^n&#x3D;\sum_i^nC_n^i a^{n-i}b^i$，$C_n^i$杨辉三角<br><code>伯努利分布：</code>关于布尔变量$x\in{0,1}$的概率分布，其连续参数$p\in[0,1]$表示变量$x&#x3D;1$的概率：$p(x)&#x3D;p^x(1-p)^{1-x},E(x)&#x3D;p,\sigma&#x3D;p(1-p)$<br><code>二项分布：</code>1-实验次数固定为$n$；2-每次事件只有两种可能；3-每次成功的概率固定为$p$；4-表示成功$x$次的概率：$p(x)&#x3D;C_n^x p^x(1-p)^{n-x},E(x)&#x3D;np,\sigma&#x3D;\sqrt{np(1-p)}$<br><code>几何分布：</code>0-与二项分类相似，唯一不同在于4-表示进行$x$次尝试，取得第一次成功的概率：$p(x)&#x3D;(1-p)^{x-1}p,E(x)&#x3D;1&#x2F;p,\sigma&#x3D;(1-p)&#x2F;p^2$<br>&#96;&#96;泊松分布：&#96;1-事件独立；2-在任意相同的时间范围内，事件发生的概率相同；3-在某个时间范围内，发生某件事情$x$的概率：$p(x)&#x3D;\frac{u^xe^{-u}}{x!},E(x)&#x3D;\sigma&#x3D;u\ [p&#x3D;\frac{u}{n},p(x)&#x3D;\lim_{n\rightarrow\infty}\pmatrix{\begin{matrix}n\m\end{matrix}}p^x(1-p)^{n-x}]即在p上的极限$</p><p><code>伯努利扔一次硬币；二项分布是多次伯努利；泊松分布是p很小的二项，即无数次扔硬币且正面概率极小；正态分布是n很大的二项，即无数次扔硬币且硬币完全相同。</code></p></blockquote><p><strong>CART剪枝</strong></p><blockquote><p><strong>输入：</strong> CART算法完全生成的决策树$T_0$<br><strong>输出：</strong> 最优决策树$T_\alpha$<br>1-设$k&#x3D;0,T&#x3D;T_0,\alpha&#x3D;+\infty$<br>2-自下而上地对各内部结点$t$计算$C(T_t)$，$|T_t|$以及$g(t)&#x3D;\frac{C(t)-C(T_t)}{|T_t|-1},\alpha&#x3D;\min(\alpha,g(t))$。以$|T_t|$为叶结点个数$t$为根节点的子树$T_t$，对训练数据的预测误差$C(T_t)$。<br>3-对$g(t)&#x3D;\alpha$的内部结点$t$进行剪枝，并对叶结点以多数表决法决定其类，得到树$T$。<br>4-设$k&#x3D;k+1,T_k&#x3D;T,\alpha_k&#x3D;\alpha$<br>5-如果$T_k$不是由根节点及两个叶结点构成的树，则回到1-$\alpha&#x3D;+\infty$；否则令$T_k&#x3D;T_n$<br>6-采用交叉验证法在子树序列$T_0,T_1,\cdots,T_n$中选取最优子树$T_{\alpha}$。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 《统计学习方法》（第二版） </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第4章 朴素贝叶斯法</title>
      <link href="/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC4%E7%AB%A0%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"/>
      <url>/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC4%E7%AB%A0%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="1-基本方法"><a href="#1-基本方法" class="headerlink" title="1.基本方法"></a>1.基本方法</h3><p>朴素贝叶斯法（naive Bayes）是基于贝叶斯定理与特征条件独立假设的分类法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$.</p><span id="more"></span><p>先学习先验概率分布$P(Y&#x3D;c_k)$及条件概率分布$P(X&#x3D;x|Y&#x3D;c_k)&#x3D;P(X^{(1)}&#x3D;x^{(1)},X^{(2)}&#x3D;x^{(2)},\cdots,X^{(N)}&#x3D;x^{(n)}|Y&#x3D;c_k)$从而得到联合概率分布$P(X,Y)$<br>由于条件概率分布有指数级数量的参数，所以对条件概率分布做了<strong>条件独立性假设</strong><code>用于分类的特征在类确定的条件下都是条件独立的</code>：$P(X&#x3D;x|Y&#x3D;c_k)&#x3D;P(X^{(1)}&#x3D;x^{(1)},X^{(2)}&#x3D;x^{(2)},\cdots,X^{(N)}&#x3D;x^{(n)}|Y&#x3D;c_k)&#x3D;\prod_{j&#x3D;1}^n{P(X^j&#x3D;x^j|Y&#x3D;c_k)}$<br>分类器：$y&#x3D;f(x)&#x3D;\arg\max_{c_k}\frac{P(Y&#x3D;c_k)\Pi_{j}P(X^{j}&#x3D;x^j|Y&#x3D;c_k)}{\sum_k{P(Y&#x3D;c_k)\Pi_{j}P(X^{j}&#x3D;x^j|Y&#x3D;c_k)}}$</p><p><strong>后验概率最大化的含义</strong></p><p>期望风险最小化准则变为了后验概率最大化准则<br>$$<br>\begin{align}<br>f(x)&amp;&#x3D;\arg\min_{y\in\mathcal{Y}}\sum_{k&#x3D;1}^K{L(c_k,y)P(c_k|X&#x3D;x)}\<br>&amp;&#x3D;\arg\min_{y\in\mathcal{Y}}\sum_{k&#x3D;1}^K{P(y\neq c_k|X&#x3D;x)}\<br>&amp;&#x3D;\arg\min_{y\in\mathcal{Y}}{(1-P(y&#x3D;c_k|X&#x3D;x))}\<br>&amp;&#x3D;\arg\max_{y\in\mathcal{Y}}{P(y&#x3D;c_k|X&#x3D;x)}\<br>\end{align}<br>$$</p><h3 id="2-朴素贝叶斯法的参数估计"><a href="#2-朴素贝叶斯法的参数估计" class="headerlink" title="2.朴素贝叶斯法的参数估计"></a>2.朴素贝叶斯法的参数估计</h3><p><strong>极大似然估计</strong></p><p>估计先验概率：$P(Y&#x3D;c_k)&#x3D;\frac{\sum_{i&#x3D;1}^N{I(y_i&#x3D;c_k)}}{N},I为指示函数$<br>估计条件概率：$P(X^j&#x3D;a_{jl}|Y&#x3D;c_k)&#x3D;\frac{\sum_{i&#x3D;1}^{N}{I(x_i^j&#x3D;a_{jl},y_i&#x3D;c_k)}}{\sum_{i&#x3D;1}^{N}{I(y_i&#x3D;c_k)}}$<br>会出现所有估计概率值为0的情况</p><p><strong>贝叶斯估计</strong></p><p>估计先验概率：$P_\lambda(Y&#x3D;c_k)&#x3D;\frac{\sum_{i&#x3D;1}^N{I(y_i&#x3D;c_k)}+\lambda}{N+K\lambda},I为指示函数$<br>估计条件概率：$P_\lambda(X^j&#x3D;a_{jl}|Y&#x3D;c_k)&#x3D;\frac{\sum_{i&#x3D;1}^N{I(x_i^j&#x3D;a_{jl},y_i&#x3D;c_k)+\lambda}}{\sum_{i&#x3D;1}^N{I(y_i&#x3D;c_k)+S_j\lambda}}$<br>$\lambda&#x3D;0$就是<code>极大似然估计</code>，$\lambda&#x3D;1$就是<code>拉普拉斯平滑</code></p>]]></content>
      
      
      <categories>
          
          <category> 《统计学习方法》（第二版） </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第3章 K近邻法</title>
      <link href="/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC3%E7%AB%A0%20K%E8%BF%91%E9%82%BB%E6%B3%95/"/>
      <url>/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC3%E7%AB%A0%20K%E8%BF%91%E9%82%BB%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="3-1-k-近邻算法-（-k-nearest-neighbor-KNN）"><a href="#3-1-k-近邻算法-（-k-nearest-neighbor-KNN）" class="headerlink" title="3.1 $k$近邻算法 （$k$-nearest neighbor, KNN）"></a>3.1 $k$近邻算法 （$k$-nearest neighbor, KNN）</h3><blockquote><p><strong>输入：</strong> 训练集$T&#x3D;{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$<br><strong>输出：</strong> 实例$x$所属类别$y$<br>1-根据给定的距离度量，在训练集$T$中找出与$x$最近邻的$k$个点，涵盖这$k$个点的$x$的领域记作$N_k(x)$;<br>2-在$N_k(x)$中根据分类决策规则决定$x$的类别$y$：$y&#x3D;\arg\max_{c_j}\sum_{x_i\in N_k(x)}{I(y_i&#x3D;c_j)}$，$I$为指示函数。</p></blockquote><p>是一种基本分类与回归方法，没有显式的学习过程</p><span id="more"></span><h3 id="3-2-k-近邻模型"><a href="#3-2-k-近邻模型" class="headerlink" title="3.2 $k$近邻模型"></a>3.2 $k$近邻模型</h3><p>$k$近邻算法使用的模型实际上对应于对特征空间的划分，模型有三个基本要素：距离度量、$k$值得选择和分类决策规则。</p><p><strong>距离度量</strong></p><blockquote><p>欧式距离、$L_p$距离（Minkowski距离）</p><p>$L_p(x_i,y_i)&#x3D;(\sum_{l&#x3D;1}^n|x_i^{(l)}-x_j^{(l)}|^p)^\frac{1}{p},p\ge1$<br>$p&#x3D;2$，欧式距离；$p&#x3D;1$，Manhattan距离；$p&#x3D;\infty$，各个坐标距离的最大值</p></blockquote><p><strong>$k$值的选择</strong></p><blockquote><p><strong>较小值：</strong> 相当于在较小邻域中训练实例进行预测，近似误差会减小，估计误差会增大，对近邻点非常敏感。<u><em>k值减小意味着整体模型变得复杂，容易过拟合。</em></u></p><p><strong>较大值：</strong> 相当于在较大邻域中训练实例进行预测，估计误差会减小，近似误差会增大，对远邻点依然有效，容易误检。<u><em>k值增大意味着整体模型变得简单，容易欠拟合。</em></u></p><p>$\mathbf{k&#x3D;N}$：无论什么输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略了实例中大量有用信息。</p></blockquote><h3 id="3-3-k-近邻法的实现：-kd-树"><a href="#3-3-k-近邻法的实现：-kd-树" class="headerlink" title="3.3 $k$近邻法的实现：$kd$树"></a>3.3 $k$近邻法的实现：$kd$树</h3><p>以特征$x$的第一维为坐标轴，以中位数为切分点，然后是第二维，第三维，划分得到最后的二叉树。</p>]]></content>
      
      
      <categories>
          
          <category> 《统计学习方法》（第二版） </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第2章 感知机</title>
      <link href="/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC2%E7%AB%A0%20%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC2%E7%AB%A0%20%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h3 id="2-1-感知机模型"><a href="#2-1-感知机模型" class="headerlink" title="2.1 感知机模型"></a>2.1 感知机模型</h3><p>$$<br>f(x)&#x3D;sign(w\cdot x+b),\ sign(x)&#x3D;\begin{cases}+1,x\ge0\-1,x\lt0\end{cases}<br>$$</p><p>对于特征空间$\mathbf{R}^n$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距。</p><span id="more"></span><h3 id="2-2-感知机学习策略"><a href="#2-2-感知机学习策略" class="headerlink" title="2.2 感知机学习策略"></a>2.2 感知机学习策略</h3><ul><li>数据集的线性可分性</li><li>损失函数是参数$w$和$b$连续可导函数：点到超平面的距离 $\frac{1}{|w|}|w\cdot x_i+b|$</li><li>误分类数据满足：$-y_i(w\cdot x_i+b)\gt0$，$y(w\cdot x+b)$称为样本点的函数间隔</li></ul><h3 id="2-3-感知机学习算法"><a href="#2-3-感知机学习算法" class="headerlink" title="2.3 感知机学习算法"></a>2.3 感知机学习算法</h3><p><strong>原始形式</strong></p><blockquote><p><strong>输入：</strong>数据集、学习率$\eta\ (0\lt\eta\le1)$<br><strong>输出：</strong>$w,b,f(x)&#x3D;sign(w\cdot x+b)$</p><ol><li><p>选取初始值$w_0,b_0$</p></li><li><p>在训练集中选取数据$(x_i,y_i)$</p></li><li><p>如果$y_i(w\cdot x_i+b)\le0$</p><p>$$w\leftarrow w+\eta y_i x_i\<br>b\leftarrow b+\eta y_i$$</p></li><li><p>转至2，直到无误分类点</p></li></ol></blockquote><p>首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数，在一个过程中一次随机选取一个误分类点使其梯度下降。</p><p><strong>Novikoff定理</strong></p><ol><li><p>$\exists:y_i(\hat{W}<em>{opt}\cdot\hat{x}<em>i)&#x3D;y_i(W</em>{opt}\cdot x_i+b</em>{opt})\ge\gamma,|\hat{W}_{opt}|&#x3D;1$</p></li><li><p>在训练集上的误分类次数$k$满足不等式：$k\le(\frac{R}{\gamma})^2,R&#x3D;\max_{1\le i\le N}|\hat{x}_i|^2$</p></li></ol><p>说明误分类有上界；训练集可分时，原始形式收敛，不可分时，不收敛；2可以近似看成线段划分，即特征空间最多可以划分成K个。</p><p><code>当训练集可分时，感知机学习算法存在无穷多解，由于不同的初始值或不同的迭代顺序而有所不同。</code></p><p><strong>对偶形式</strong></p><blockquote><p><strong>输入：</strong>数据集、学习率$\eta\ (0\lt\eta\le1)$<br><strong>输出：</strong>$\alpha,b,f(x)&#x3D;sign(\sum_{j&#x3D;1}^N\alpha_j y_j x_j\cdot x+b)$</p><ol><li><p>选取初始值$a\leftarrow0,b\leftarrow0$</p></li><li><p>在训练集中选取数据$(x_i,y_i)$</p></li><li><p>如果$y_i(\sum_{j&#x3D;1}^N\alpha_j y_j x_j\cdot x_i+b)\le0$</p></li></ol><p>  $$\alpha_i\leftarrow\alpha_i+\eta\<br>  b\leftarrow b+\eta y_i$$</p><ol start="4"><li>转至2，直到无误分类点</li></ol></blockquote><p><strong>基本思想：</strong>将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合，通过求解其系数而得到$w$和$b$。实例点更新次数越多，意味着它距离分类超平面越近，也就越难正确分类。对偶形式中训练实例仅以內积$x_j\cdot x_i$的形式出现（Gram矩阵）。</p><h3 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h3><p><strong>1. 为什么感知机不能表示异或</strong></p><p><strong>2. 样本集线性可分的充要条件是正实例点击所构成的凸壳$^1$与负实例所构成的凸壳互不相交</strong></p><p>$^1$设集合$S\subset\mathbf{R}^n$是由$\mathbf{R}^n$中的$k$个点组成的集合，即$S&#x3D;{x_1,x_2,\cdots,x_k}$ 定义$S$的凸壳$conv(S)$为：<br>$$<br>conv(S)&#x3D;\left{x&#x3D;\sum_{i&#x3D;1}^k{\lambda_i x_i}\mid\sum_{i&#x3D;1}^k{\lambda_i&#x3D;1},\lambda_i\ge0,i&#x3D;1,2,\cdots,k\right}<br>$$<br><code>最小凸多边形、点集合的边界所围成的区域、点集的线性组合</code></p>]]></content>
      
      
      <categories>
          
          <category> 《统计学习方法》（第二版） </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo搭建GitHub个人博客</title>
      <link href="/2022/04/10/Hexo%E6%90%AD%E5%BB%BAGitHub%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2022/04/10/Hexo%E6%90%AD%E5%BB%BAGitHub%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><blockquote><ul><li>安装Node.js</li><li>安装Git</li><li>远程连接GitHub</li><li>安装Hexo</li><li>Hexo的本地配置</li><li>博客相关指令</li></ul></blockquote><span id="more"></span><h3 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h3><blockquote><p>Node.js就是一个用于创建服务器端应用程序的运行系统，它可以轻松构建网络或其他事件驱动的应用程序服务器。</p></blockquote><p>进入<a href="https://nodejs.org/en/download/">Node.js官网</a>下载对应版本的安装包，默认设置安装。<br>安装完成后进入终端检查是否安装成功，按<code>Win+R</code>进入<code>cmd</code>输入<code>node -v</code>和<code>npm -v</code>，如果出现版本号，则证明安装成功。</p><blockquote><p>npm (Node Package Manager) NodeJS包管理和分发工具<br>(<em>可选</em>) - 添加镜像（进入cmd，以阿里为例）<br>npm config set registry <a href="https://registry.npm.taobao.org/">https://registry.npm.taobao.org</a></p></blockquote><h3 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h3><blockquote><p>Git是一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理，帮助我们把本地网页上传到Github。</p></blockquote><p>进入<a href="https://git-scm.com/download/win">Git官网</a>下载，默认设置安装。<br>安装完成之后在<code>cmd</code>中使用<code>git --version</code>验证是否安装成功。</p><h3 id="远程连接GitHub"><a href="#远程连接GitHub" class="headerlink" title="远程连接GitHub"></a>远程连接GitHub</h3><p><strong>生成密钥</strong></p><pre><code class="bash"># git bash中设置user.name和user.emailgit config --global user.name &quot;GitHub用户名&quot;git config --global user.email &quot;GitHub注册邮箱&quot;# 生成密钥ssh-keygen -t rsa -C &quot;注册邮箱&quot;# 将公钥id_rsa.pub添加到GitHubgithub -&gt; settings -&gt; ssh and gpg keys# 测试本地连接ssh -T git@github.com</code></pre><h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><blockquote><p>Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Heroku，是搭建博客的首选框架。</p></blockquote><p>进入<code>cmd</code>输入<code>npm</code>安装命令</p><pre><code class="bash"># 安装指令 -g 表示全局安装npm install hexo-cli -g# 验证指令hexo -v</code></pre><h3 id="Hexo的本地配置"><a href="#Hexo的本地配置" class="headerlink" title="Hexo的本地配置"></a>Hexo的本地配置</h3><p><strong>初始化</strong><br>在<code>Git Bash</code>中执行</p><pre><code class="bash"># 创建存放博客文件夹mkdir myBlog# 初始化hexo init myBlog</code></pre><p>看到<code>Start blogging with Hexo</code>则表示初始化成功。<br><strong>部署到GitHub</strong><br>安装必要插件：</p><pre><code class="bash">npm install hexo-deployer-git --save  #安装自动部署Github插件</code></pre><p>修改配置文件<code>_config.yml</code></p><pre><code class="yml"># Deployment## Docs: https://hexo.io/docs/one-command-deploymentdeploy:  type: git  repo: git@github.com:seeyourmind/seeyourmind.github.io  branch: master</code></pre><p><strong>更换主题</strong><br>以aomori为例</p><pre><code class="bash">git clone https://github.com/lh1me/hexo-theme-aomori.git themes/aomori</code></pre><h3 id="博客相关指令"><a href="#博客相关指令" class="headerlink" title="博客相关指令"></a>博客相关指令</h3><blockquote><p><strong>常用指令</strong><br>hexo new post “article title”<br>hexo s<br>hexo clean &amp;&amp; hexo g -s<br>hexo clean &amp;&amp; hexo g -d</p></blockquote><p><strong>文章书写</strong></p><pre><code class="bash">hexo new [layout] &lt;title&gt; 或 hexo n [layout] &lt;title&gt;# layout: #  post(默认，存于source/_posts)#  draft(草稿，存于source/_drafts，可以使用publish指令将其推送到_posts)#  page(页面)-p --path 自定义新文章-r --replece 替换同名文章-s --slug 文章的slug，作为新文章的文件名和发布后的URL# 示例hexo new page --path about/me &quot;About me&quot;</code></pre><p><strong>生成静态文件</strong></p><pre><code class="bash">hexo generate 或 hexo g-d --deploy 文件生成后部署网站-w --watch 监视文件变动-b --bail 生成过程中出现异常时抛出-f --force 强制重新生成文件-c --concurrency 最大同时生成文件数量，默认无限制</code></pre><p><strong>发布草稿</strong></p><pre><code class="bash">hexo publish [layout] &lt;filename&gt;</code></pre><p><strong>启动服务器</strong><br><code>hexo server 或 hexo s</code> 启动服务器，<code>ctrl+c</code> 结束，默认地址为：<a href="http://localhost:4000/">http://localhost:4000/</a><br><strong>部署网站</strong></p><pre><code class="bash">hexo deploy 或 hexo d-g 或--generate 部署之前写成静态文件</code></pre><p><strong>渲染文件</strong></p><pre><code class="bash">hexo render &lt;file1&gt; [file2]-o或--output 设置输出路径</code></pre><p><strong>清除缓存文件</strong></p><pre><code class="bash">hexo clean</code></pre><p><strong>列出网站资料</strong></p><pre><code class="bash">hexo list &lt;type&gt;</code></pre><p><strong>显示草稿</strong></p><pre><code class="bash">hexo --deaft</code></pre><p><strong>自定义当前工作目录</strong></p><pre><code class="bash">hexo --cwd /path/to/cwd</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第1章 统计学习及监督学习概论</title>
      <link href="/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC1%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/"/>
      <url>/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC1%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<h3 id="1-统计学习基本分类"><a href="#1-统计学习基本分类" class="headerlink" title="1. 统计学习基本分类"></a>1. 统计学习基本分类</h3><p><strong>监督学习</strong>    <code>学习输入到输出的映射的统计规律</code></p><blockquote><p>1-输入空间、特征空间和输出空间：特征连续预测是回归，离散预测是分类<br>2-联合概率分布：假设输入与输出服从联合分布（关于数据的基本假设）<br>3-假设空间：模型属于由输入空间到输出空间的映射的集合，意味着学习范围的确定。模型可以是概率模型（$P(Y|X)$）或非概率模型$Y&#x3D;f(X)$</p></blockquote><span id="more"></span><p><strong>无监督学习</strong>    <code>学习数据中的统计规律或潜在结构</code></p><blockquote><p>旨在从假设空间中选出在给定评价标准下的最优模型<br>模型可以实现对数据的聚类、降维或概率估计</p></blockquote><p><strong>强化学习</strong>    <code>学习最优的序贯决策</code></p><blockquote><p>强化学习的马尔可夫决策过程是状态、奖励、动作序列上的随机过程，$&lt;S,A,P,r,\gamma&gt;$<br><strong>S</strong>tate、<strong>A</strong>ction、transition <strong>P</strong>robability ($P(s^\prime|s,a)&#x3D;P(s_{t+1}&#x3D;s^\prime|s_t&#x3D;s,a_t&#x3D;a)$)、<strong>r</strong>eward function ($r(s,a)&#x3D;E(r_{t+1}|s_t&#x3D;s,a_t&#x3D;a)$)、discount factor ($\gamma\in[0,1]$)<br>策略$\pi$定义为给定状态下动作的函数$a&#x3D;f(s)$或$P(a|s)$<br>状态价值函数定义为策略$\pi$从某一个状态$s$开始的长期累积奖励的数学期望<br>动作价值函数定义为策略$\pi$从某一个状态$s$和动作$a$开始的长期累积奖励的数学期望</p><p>强化学习方法有基于策略的(policy-based)、基于价值的(value-based)无模型(model-free)方法和有模型(model-based)方法。<br>model-based直接学习马尔科夫决策过程<br>policy-based model-free求解最优策略$\pi^*$<br>value-based model-free求解最优价值函数$q^*(s,a)$</p></blockquote><p><strong>半监督学习</strong>    <code>利用未标注数据中的信息辅助标注数据进行监督学习</code></p><p><strong>主动学习</strong>     <code>找出对学习最有帮助的实例让teacher标注</code></p><h3 id="2-按模型分类"><a href="#2-按模型分类" class="headerlink" title="2. 按模型分类"></a>2. 按模型分类</h3><p><strong>概率模型与非概率模型</strong></p><blockquote><p><strong>基本概率公式</strong><br>加法规则：$P(x)&#x3D;\sum_yP(x,y)$<br>乘法规则：$P(x,y)&#x3D;P(x)P(y|x)$</p></blockquote><p><strong>线性模型与非线性模型</strong></p><p><strong>参数化模型与非参数化模型</strong></p><p>参数化模型假设模型参数的维度固定，模型可以由有限维参数完全刻画；非参数化模型假设模型参数的维度不固定或者说无穷大，随着训练数据量的增加而不断增大。</p><h3 id="3-按算法分类"><a href="#3-按算法分类" class="headerlink" title="3. 按算法分类"></a>3. 按算法分类</h3><p><strong>在线学习与批量学习</strong></p><p>在线学习每次接受一个样本，进行预测，之后学习模型，不断重复；批量学习一次接受所有数据，学习模型，之后进行预测。<br>在线学习可以是监督学习，也可以是无监督学习，强化学习本身就拥有在线学习的特点。</p><h3 id="4-按技巧分类"><a href="#4-按技巧分类" class="headerlink" title="4. 按技巧分类"></a>4. 按技巧分类</h3><p><strong>贝叶斯学习</strong></p><p>在概率模型学习和推理中，利用贝叶斯定理计算在给定数据条件下模型的条件概率，即后验概率，并应用这个原理进行模型的估计，以及对数据的预测。</p><p>贝叶斯估计和极大似然估计代表着统计学中贝叶斯学派和频率学派对统计的不同认识。<br>假设先验分布是均匀分布，取后验概率最大，就能从<code>贝叶斯估计</code>得到<code>极大似然估计</code>。<br>$$<br>D\rightarrow{极大似然估计}\rightarrow\hat{\theta&#x3D;\arg\max_{\theta}P(D|\theta)}\ [图像是只在\hat{\theta}有值]\<br>D\rightarrow{贝叶斯估计}\rightarrow\hat{P}(\theta|D)&#x3D;\frac{P(\theta)P(D|\theta)}{P(D)}\ [图像是正态分布曲线]<br>$$<br><strong>核方法</strong></p><p>核方法是使用核函数表示和学习费线性模型的一种机器学习方法。有一些线性模型的学习方法基于相似度计算&lt;向量內积&gt;，核方法可以把他们扩展到非线性模型的学习中，使其应用更广泛。<br>线性转非线性直接做是显式的定义输入空间到特征空间的映射，在特征空间做內积；核方法不显式的定义这个映射，而是定义核函数&lt;映射之后在特征空间的內积&gt;。<br>$$<br>\frac{输入空间}{x_1,x_2}\stackrel{映射函数\psi}{\longrightarrow}\frac{特征空间}{\psi(x_1),\psi(x_2)}\<br>核函数定义在输入空间\frac{输入空间}{K(x_1,x_2)} \ K(x_1,x_2)&#x3D;&lt;\psi(x_1),\psi(x_2)&gt;<br>$$</p><h3 id="5-统计学习方法三要素"><a href="#5-统计学习方法三要素" class="headerlink" title="5. 统计学习方法三要素"></a>5. 统计学习方法三要素</h3><p><strong>模型</strong></p><p>在监督学习过程中，模型就是所要学习的条件概率分布或决策函数，模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数。</p><p><strong>策略</strong></p><p>损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。<br>损失函数是$f(X)$和$Y$的<strong>非负</strong>实值函数，记作$L(Y,f(X))$；损失函数的期望是模型$f(X)$关于联合分布$P(x,Y)$的平均意义下的损失，称为风险函数或期望损失$R_{exp}(f)$。<br>$$<br>R_{exp}(f)&#x3D;E_P[L(Y,f(X))]&#x3D;\int_{\mathcal{X}\times\mathcal{Y}}{L(y,f(x))P(x,y)dxdy}<br>$$<br>由于联合分布未知，监督学习就是为了学习联合分布，所以监督学习是一个ill-formed problem。根据大数定律，当样本容量$N$趋于无穷时，经验风险$R_{emp}$趋近于期望风险$R_{exp}$<br>$$<br>R_{emp}(f)&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N{L(y_i,f(x_i))}<br>$$<br><code>大数定律：说如果统计数据足够大,那么事物出现的频率就能无限接近它的期望值。</code></p><p>由于现实中训练样本数目有限，甚至很小，所以需要矫正。</p><blockquote><p><strong>经验风险最小化</strong><br>$\min_{f\in{F}}\frac{1}{N}\sum_{i&#x3D;1}^N{L(y_i,f(x_i))}$<br>当模型是条件概率分布、损失函数是对数损失函数时，经验风险最小化等价于<code>极大似然估计</code><br><strong>结构风险最小化</strong><br>$R_{srm}(f)&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N{L(y_i,f(xi))+\lambda J(f)},\ \lambda\ge0$<br>$J(f)$为模型的复杂度，是定义在假设空间的泛函；$\lambda$用以权衡经验风险和模型复杂度<br>当模型是条件概率分布、损失函数是对数损失函数时，模型复杂度由模型的先验概率表示时，结构风险最小化等价于<code>最大后验概率估计</code></p></blockquote><p><strong>算法</strong></p><h3 id="6-模型评估与模型选择"><a href="#6-模型评估与模型选择" class="headerlink" title="6.模型评估与模型选择"></a>6.模型评估与模型选择</h3><p>通常将学习方法对未知数据的预测能力称为泛化能力</p><p><strong>模型选择方法：</strong>正则化和交叉验证</p><p>正则化符合奥卡姆剃刀（Occam’s razor）原理<br><code>如无必要，勿增实体，即简单有效原理</code></p><p>交叉验证：简单交叉（分成两部分）、S折交叉（分成S分）、留一交叉（S&#x3D;N）</p><h3 id="7-泛化误差上界"><a href="#7-泛化误差上界" class="headerlink" title="7.泛化误差上界"></a>7.泛化误差上界</h3><p>$$<br>对二分类问题，泛化误差R(f)：R(f)\le\hat{R}(f)+\epsilon(d,N,\delta),\ \epsilon(d,N,\delta)&#x3D;\sqrt{\frac{1}{2N}(\log{d}+\log\frac{1}{\delta})}\<br>Hoeffding不等式：\forall{t\gt0},P(|\overline{X}-E[\overline{X}]|\ge t)\le2exp(-\frac{2n^2t^2}{\sum_{i&#x3D;1}^{n}(b_i-a_i)^2})<br>$$</p><h3 id="8-监督学习应用"><a href="#8-监督学习应用" class="headerlink" title="8.监督学习应用"></a>8.监督学习应用</h3><p>TP（正确（T）预测为正样例（P））——正预测为正<br>FN（错误（F）预测为负样本（N））——正预测为负<br>FP（错误（F）预测为正样例（P））——负预测为正<br>TN（正确（T）预测为负样本（N））——负预测为负<br>准确率：预测为正的里面真实为正的<br>召回率：真实为正的里面预测为正的</p>]]></content>
      
      
      <categories>
          
          <category> 《统计学习方法》（第二版） </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/04/10/hello-world/"/>
      <url>/2022/04/10/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><p><strong>To Do</strong></p><blockquote><p>天生带来允许，文化造成封闭 —— 《人类简史》</p></blockquote>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
