<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

    
    <title>Seeyourmind</title>

    <meta name="description" content="Seeyourmind">
    <meta name="keywords" content="">

    



    <meta property="og:type" content="website"/>
    <meta property="og:title" content=""/>
    <meta property="og:description" content=""/>
    <meta property="og:locale" content="cn,en,ja,default" />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="http://example.com/index.html" />
    <meta property="og:site_name" content="FZY" />
    <meta property="article:publisher" content="" />
    <script type="application/ld+json">
        {
            "@context": "https://schema.org",
            "@type": "WebPage",
            "name": "",
            "description": "",
            "publisher": {
                "@type": "Organization",
                "name": "FZY"
            },
        }
    </script>


    

    

    

    

    
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>😉</text></svg>">
    

    
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    

    

    
<link rel="stylesheet" href="/dist/build.css?v=1646451311888.css">


    
<link rel="stylesheet" href="/dist/custom.css?v=1646451311888.css">


    <script>
        window.isPost = false
        window.aomori = {
            
            gitalk: {
                enable: true,
                clientID: "7f65031d09ca001ce168",
                clientSecret: "886e5d3f9b29dddbae3fa445680cb33d8f8b55eb",
                repo: "seeyourmind.github.io",
                owner: "seeyourmind",
                admin: ["seeyourmind",],
                distractionFreeMode: true // Facebook-like distraction free mode  // Facebook-like distraction free mode
            },
            
            
            
        }
        window.aomori_logo_typed_animated = true
        window.aomori_search_algolia = false

    </script>

    <!--将该代码放入博客模板的head中即可-->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
        }
        });
    </script>
    <!--latex数学显示公式-->
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<meta name="generator" content="Hexo 6.1.0"></head>

<body>

    <div class="container">
    <header class="header">
        <div class="header-type">
            
            <div class="header-type-inner">
                
                    <div id="typed-strings" style="display:none">
                        <p>Seeyourmind</p>
                    </div>
                    <a class="header-type-title" id="typed" href="/"></a>
                
    
                
            </div>
        </div>
        <div class="header-menu">
            <div class="header-menu-inner">
                
                <a href="/">首页</a>
                
                <a href="/archives">归档</a>
                
            </div>
            <div class="header-menu-social">
                
    <a class="social" target="_blank" href="https://github.com/seeyourmind">
        <box-icon type='logo' name='github'></box-icon>
    </a>

    <a class="social" target="_blank" href="live:.cid.1f6ac2746d42e0ee">
        <box-icon type='logo' name='skype'></box-icon>
    </a>

            </div>
        </div>

        <div class="header-menu-mobile">
            <div class="header-menu-mobile-inner" id="mobile-menu-open">
                <i class="icon icon-menu"></i>
            </div>
        </div>
    </header>

    <div class="header-menu-mobile-menu">
        <div class="header-menu-mobile-menu-bg"></div>
        <div class="header-menu-mobile-menu-wrap">
            <div class="header-menu-mobile-menu-inner">
                <div class="header-menu-mobile-menu-close" id="mobile-menu-close">
                    <i class="icon icon-cross"></i>
                </div>
                <div class="header-menu-mobile-menu-list">
                    
                    <a href="/">首页</a>
                    
                    <a href="/archives">归档</a>
                    
                </div>
            </div>
        </div>
    </div>

</div>

    <div class="container">
        <div class="main">
            <section class="inner">
                <section class="inner-main">
                    <div class="index">
  
    
      <article
id="post-Hexo搭建GitHub个人博客"
class="article article-type-post"
>



<div class="article-inner">
    
    <div class="article-feature">
        <a href="/2022/04/10/Hexo%E6%90%AD%E5%BB%BAGitHub%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/">
            <img
                src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 3 2'%3E%3C/svg%3E"
                data-src="https://dfzximg01.dftoutiao.com/news/20220411/20220411090406_a121ce0b5d133db4a93473fe586c5c2f_6.jpeg"
                alt="item.title"
                class="lazy"
            />
        </a>
    </div>
    

    <div class="article-body">
    <header class="article-title">
        <a href="/2022/04/10/Hexo%E6%90%AD%E5%BB%BAGitHub%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/">Hexo搭建GitHub个人博客</a>
    </header>
    <div class="article-entry post-inner-html">
        
        <p>this is test file.</p>

        
    </div>
    </div>

    <div class="article-badge">
        
        <div class="article-top">
            <box-icon type='solid' name='to-top' color='#ffffff' size='xs'></box-icon>
        </div>
        
        
            <div class="article-repost">
                <box-icon name='repost' color='#ffffff' size='xs'></box-icon>
            </div>
        
    </div>

</div>

<footer class="article-footer">
    <div class="article-more-info">
    <div class="article-date">
  <time datetime="2022-04-10T13:15:27.000Z" itemprop="datePublished">2022-04-10</time>
</div>
    
    
        <div class="article-tag">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B5%8B%E8%AF%95/" rel="tag">测试</a></li></ul>
        </div>
    
    </div>
</footer>

</article>

    
  
    
      <article
  id="tweet-学习笔记/专题学习/凸优化"
  class="article article-type-tweet"
>

    <div class="article-tweet article-type-tweet-inner">

        <box-icon class="article-tweet-quote" type='solid' name='quote-left' color="#CCC"></box-icon>

        <div class="article-tweet-content post-inner-html">
            
                <p><strong>To Do</strong></p>

            
        </div>

        <footer class="article-footer">
            <a class="article-footer-common" href="/2022/04/11/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E5%87%B8%E4%BC%98%E5%8C%96/">评论</a>
            <div class="article-more-info">
            <div class="article-date">
  <time datetime="2022-04-11T13:15:27.000Z" itemprop="datePublished">2022-04-11</time>
</div>
            
                <div class="article-category">
                <a class="article-category-link" href="/categories/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/">专题学习</a>
                </div>
            
            
                <div class="article-tag">
                <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li></ul>
                </div>
            
            </div>
        </footer>

        <div class="article-badge">
            
            
        </div>

    </div>

</article>

    
  
    
      <article
id="post-学习笔记/专题学习/希尔伯特空间"
class="article article-type-post"
>



<div class="article-inner">
    

    <div class="article-body">
    <header class="article-title">
        <a href="/2022/04/11/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4/">希尔伯特空间</a>
    </header>
    <div class="article-entry post-inner-html">
        
        <h3 id="等量变换"><a href="#等量变换" class="headerlink" title="等量变换"></a>等量变换</h3><p><strong>概率场景：</strong></p>
<p>由于概率分布的积分对应面积为1，所以在微分角度上，两个分布的变化量是相等的，则有：<br>$$<br>p(x)dx&#x3D;p(y)dy \<br>p(y)&#x3D;p(x)\left|\frac{dx}{dy}\right|<br>$$</p>
<h3 id="Hilbert-Space-希尔伯特空间"><a href="#Hilbert-Space-希尔伯特空间" class="headerlink" title="Hilbert Space 希尔伯特空间"></a>Hilbert Space 希尔伯特空间</h3><p>线性空间（向量空间）关注的是向量的位置，对于一个线性空间，知道<code>基</code>（相当于三维空间中的坐标系）便可确定空间中元素的坐标（即位置）；线性空间只定义了<code>加法和数乘</code>运算。</p>
<ul>
<li><p>赋范线性空间：如果我们想知道向量的长度，定义<code>范数</code></p>
</li>
<li><p>度量空间：如果我们想知道向量间的距离，定义<code>距离</code></p>
</li>
<li><p>内积空间：如果我们想知道向量的夹角，定义<code>内积</code></p>
</li>
<li><p>欧式空间：定义了内积的有限维实线性空间</p>
<blockquote>
<p>有限维：设$A$是线性空间$E$的一个线性无关子集，我们设$A$的维度为$D_E$。当$D_E&lt; +\infty$时，称$E$为有限维的，否则称$E$为无限维的，即欧式空间中没有无限维的计算的概念  </p>
</blockquote>
</li>
<li><p>完备空间：如果我们想研究收敛性（极限），定义<code>完备</code></p>
<blockquote>
<p>完备性：是在极限的基础上衍生的概念。例如在有理数集上的一个序列{1，1.4，1.41，1.414，1.4142…}，可知此序列极限为2根号2，而根号2为无理数，不属于有理数集，即有理数集不具备完备性，也就是有理数集不具备极限的概念，因为有理数集上的数都是确定的</p>
</blockquote>
</li>
<li><p>Banach空间：完备的赋范线性空间</p>
</li>
<li><p><kbd>Hilbert空间</kbd>：完备的内积空间（极限运算中不能跑出度量的范围；是欧式空间的一种推广；由于极限计算针对的一般是函数，所以Hilbert空间一般是指函数空间，可以定义函数的內积）</p>
<blockquote>
<p>函数的內积：我们有两个函数$f(x)$与$g(x)$与区间$[a,b]$，且两函数在该区间上可积且平方可积。则积分：$\int_a^b f(x)g(x)dx$，我们称之为函数的内积，函数的内积常记作$&lt;f(x),g(x)&gt;$，如果是离散的函数则我们可以直接：$\sum{f(x)\times g(x)}$，用矩阵表示就是$F(X)G(X)$</p>
</blockquote>
</li>
</ul>
<p>希尔伯特空间是一个完备的空间，其上所有的<code>柯西列</code>等价于<code>收敛列</code>，从而微积分中的大部分概念都可以无障碍地推广到希尔伯特空间中。</p>
<blockquote>
<p><strong>柯西序列</strong><br>定义：在具有度量$d$的度量空间$S$中，一个序列为柯西序列，若其符合以下条件：<br>对于任意的实数$\epsilon &gt; 0$，存在一正整数$N$，使得每当$m,n&gt;N$时都有$d(a_m, a_n)&lt;\epsilon$</p>
<p>在数学中，一个柯西列是指一个这样一个序列，它的元素随着序数的增加而愈发靠近。更确切地说，在去掉有限个元素后，可以使得余下的元素中任何两点间的距离的最大值不超过任意给定的正的常数。</p>
<p>柯西列的定义依赖于距离的定义，所以只有在度量空间(metric space)中柯西列才有意义。在更一般的一致空间(uniform space)中，可以定义更为抽象的柯西滤子(Cauchy filter)和柯西网(Cauchy net)。</p>
<p>柯西序列的重要作用是定义“完备空间”。完备空间是指一种度量空间，它的所有柯西序列（如果有的话），都收敛在这个空间自己里面。</p>
<p>在完备空间（complete space）中，所有的柯西列都有极限，这就让人们可以在不求出这个极限（如果存在）的情况下，利用柯西列的判别法则证明该极限是存在的。柯西列在构造具有完备性的代数结构的过程中也有重要价值，如构造实数。</p>
<p>1.对于在某度量空间内的柯西序列，它的极限不一定在相同的度量空间内。如有理柯西序列可导出无理极限。（事实上，一种实数构造就是用这种方法）</p>
<p>2.任何收敛列必然是柯西列，任何柯西列必然是有界序列。</p>
</blockquote>
<p>希尔伯特空间为基于任意正交系上的多项式表示的傅立叶级数和傅立叶变换提供了一种有效的表述方式，而这也是泛函分析的核心概念之一。</p>
<img src="/images/v2-be26b2ba1df2edc9636647a28b22238d_1440w.jpg" alt="空间关系图" style="zoom:80%;" />

<p><strong>Reproducing Kernel Hilbert Space再生核希尔伯特空间：</strong></p>
<p><kbd>Kernel</kbd>任何半正定的函数都可以作为核函数（Merrcer定理：充要条件）</p>
<blockquote>
<p><strong>Merrcer定理：</strong>所谓半正定函数$f(x_i, x_j)$，是指拥有训练数据集合$(x_1, x_2,\dots,x_n)$，我们定义一个矩阵的元素$a_{ij}&#x3D;f(x_i, x_j)$，这个矩阵是半正定的，那么$f(x_i, x_j)$就成为半正定的函数。</p>
</blockquote>
<p>再生核希尔伯特空间是支持监督学习（SVM）等监督学习模型的理论基础，实际上再生核希尔伯特空间就是是由核函数构成的希尔伯特空间，这里的再生指的是再生性，这里的核函数比如LibSVM中自带的几类：</p>
<ol>
<li>线性：$K(v_1,v_2)&#x3D;&lt;v_1,v_2&gt;$</li>
<li>多项式：$K(v_1,v_2)&#x3D;(\gamma&lt;v_1,v_2&gt;+c)^n$</li>
<li>高斯核：$K(v_1,v_2)&#x3D;\exp(-\gamma|v_1-v_2|^2)$</li>
<li>Sigmoid：$K(v_1,v_2)&#x3D;\tanh(\gamma&lt;v_1,v_2&gt;+c)$</li>
</ol>
<blockquote>
<p>设$\mathcal{X}$是输入空间(欧式空间$R^n$的子集或离散集合)，又设$\mathcal{H}$是特征空间(希尔伯特空间)，如果存在一个$\mathcal{X}$到$\mathcal{H}$的映射$\phi(x):\mathcal{X}\rightarrow\mathcal{H}$使得对所有$x,z\in\mathcal{X}$，函数$K(x,z)$满足条件$K(x,z)&#x3D;\phi(x)\cdot\phi(z)$则称$K(x,z)$为核函数，$\phi(x)$为映射函数，式中$\phi(x)\cdot\phi(z)$为$\phi(x)$和$\phi(z)$的內积。</p>
</blockquote>
<p>再生性指的就是<strong>原本函数之间计算内积需要算无穷维的积分（也就是这个映射函数可以映射到高维甚至无穷维（高斯核），而计算无穷维的积分是非常复杂的），但是现在只需要算核函数可以。</strong></p>

        
    </div>
    </div>

    <div class="article-badge">
        
        
    </div>

</div>

<footer class="article-footer">
    <div class="article-more-info">
    <div class="article-date">
  <time datetime="2022-04-11T12:15:27.000Z" itemprop="datePublished">2022-04-11</time>
</div>
    
        <div class="article-category">
        <a class="article-category-link" href="/categories/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/">专题学习</a>
        </div>
    
    
        <div class="article-tag">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li></ul>
        </div>
    
    </div>
</footer>

</article>

    
  
    
      <article
id="post-学习笔记/专题学习/流形学习"
class="article article-type-post"
>



<div class="article-inner">
    

    <div class="article-body">
    <header class="article-title">
        <a href="/2022/04/11/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0/">流形学习</a>
    </header>
    <div class="article-entry post-inner-html">
        
        <p><strong>To Do</strong></p>

        
    </div>
    </div>

    <div class="article-badge">
        
        
    </div>

</div>

<footer class="article-footer">
    <div class="article-more-info">
    <div class="article-date">
  <time datetime="2022-04-11T11:15:27.000Z" itemprop="datePublished">2022-04-11</time>
</div>
    
        <div class="article-category">
        <a class="article-category-link" href="/categories/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/">专题学习</a>
        </div>
    
    
        <div class="article-tag">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li></ul>
        </div>
    
    </div>
</footer>

</article>

    
  
    
      <article
id="post-学习笔记/专题学习/科学空间笔记"
class="article article-type-post"
>



<div class="article-inner">
    

    <div class="article-body">
    <header class="article-title">
        <a href="/2022/04/11/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E7%A7%91%E5%AD%A6%E7%A9%BA%E9%97%B4%E7%AC%94%E8%AE%B0/">科学空间笔记</a>
    </header>
    <div class="article-entry post-inner-html">
        
        <h2 id="关于VAE"><a href="#关于VAE" class="headerlink" title="关于VAE"></a>关于VAE</h2><h2 id="关于Flow"><a href="#关于Flow" class="headerlink" title="关于Flow"></a>关于Flow</h2><p><strong>缺点</strong></p>
<p><code>由于必须保证逆变换简单和雅可比行列式容易计算，那么每一层的非线性变换能力都很弱。所以为了保证充分的拟合能力，模型就必须堆得非常深，计算量非常大。</code></p>
<h2 id="余弦相似度的假设"><a href="#余弦相似度的假设" class="headerlink" title="余弦相似度的假设"></a><a target="_blank" rel="noopener" href="https://kexue.fm/archives/8069">余弦相似度的假设</a></h2><p>$$<br>cos(x,y)&#x3D;\frac{\sum^d_{i&#x3D;1}{x_iy_i}}{\sqrt{\sum^d_{i&#x3D;1}{x^2_i}}\sqrt{\sum^d_{i&#x3D;1}{y^2_i}}}<br>$$</p>
<blockquote>
<p>上式等号只在<strong>标准正交基下成立</strong>。向量的“夹角余弦”本身是具有鲜明的几何意义的，但上式右端只是坐标的运算，坐标依赖于所选取的坐标基，基底不同，内积对应的坐标公式就不一样，从而余弦值的坐标公式也不一样。</p>
</blockquote>
<p><strong>如果用公式算余弦值来比较句子相似度时表现不好，那么原因可能就是此时的句向量所属的坐标系并非标准正交基。</strong></p>
<p>原则上我们无法确定此时向量所属坐标系，但是我们在给向量集合选择基底时，可以依据猜测：<code>会尽量地用好每一个基向量，从统计学的角度看，这就体现为每个分量的使用都是独立的、均匀的，如果这组基是标准正交基，那么对应的向量集应该表现出“各项同性”来。</code>【<strong>如果一个向量的集合满足各向同性，那么我们可以认为它源于标准正交基</strong>】</p>
<p><strong>标准化协方差矩阵</strong></p>
<p>标准正态分布的均值为0、协方差矩阵为单位阵。假设向量集合${x_i}^N_{i&#x3D;1}$执行变换$\hat{x}<em>i&#x3D;(x_i-\mu)W$使得${\hat{x}<em>i}^N</em>{i&#x3D;1}$的均值为0，协方差矩阵为单位阵，这个操作对应于数据挖掘中的白化操作(Whitening)，具体如下：<br>$$<br>均值为0则：\mu&#x3D; \frac{1}{N}\sum</em>{i&#x3D;1}^Nx_i\<br>原始协方差：\Sigma&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N(x_i-\mu)^T(x_i-\mu)&#x3D;\left(\frac{1}{N}\sum_{i&#x3D;1}^N{x^T_ix_i}\right)-\mu^T\mu \<br>变换后协方差：\hat\Sigma&#x3D;W^T\Sigma W&#x3D;I\rightarrow\Sigma&#x3D;(W^T)^{-1}W^{-1}&#x3D;(W^{-1})^TW^{-1}\<br>协方差矩阵是半正定对称矩阵，可以被SVD分解：\Sigma&#x3D;U\Lambda U^T\rightarrow W^{-1}&#x3D;\sqrt{\Lambda}U^T\rightarrow W&#x3D;U\sqrt{\Lambda^{-1}}<br>$$</p>
<h2 id="关于Attention"><a href="#关于Attention" class="headerlink" title="关于Attention"></a>关于Attention</h2><pre><code>RNN因其本质是马尔科夫决策过程，无法很好的学习全局信息。
CNN方便并行，而且容易捕捉到一些全局的结构信息。
RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好：$y_t=f(y_&#123;t-1&#125;,x_t)$
CNN事实上只能获取局部信息，是通过层叠来增大感受野：$y_t=f(x_&#123;t-1&#125;,x_t,x_&#123;t+1&#125;)$ [3x的kernel]
</code></pre>
<p>Attention的思路最为粗暴，它一步到位获取了全局信息！它的解决方案是：$y_t&#x3D;f(x_t,A,B)$ A和B是额外引入的序列，如果$A&#x3D;B&#x3D;X$就是self-attention。Attention的意思是直接将$x_t$与原来的每个词进行比较，最后算出$y_t$。<br>$$<br>Attention(Q,K,V)&#x3D;softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>$Q\in R^{n\times d_k}$，$K\in R^{m\times d_k}$，$V\in R^{m\times d_v}$。如果忽略激活函数softmax的话，那么事实上它就是三个矩阵相乘，最后的结果就是一个$n\times d_v$的矩阵。于是我们可以认为：这是一个Attention层，将序列Q编码成了一个新的的序列。事实上$Q,K,V$分别是$query,key,value$的简写，那么上式的意思就是通过$query$与各个$key$内积的并softmax的方式，来得到$query$与各个$value$的相似度，然后加权求和，得到一个向量。其中因子$d_k$起到调节作用，使得内积不至于太大（太大的话softmax后就非0即1了，不够“soft”了）。</p>
<pre><code>50维的词向量，将每一维打乱重新排个序（当然整体要按同样的顺序来重新排序），它还是等价于原来的词向量。既然相加的对象（词向量）都没有局部结构，我们也没必要强调被加的对象（Position_Embedding）的局部结构（也就是交叉连接）了。
</code></pre>
<h2 id="一些观点"><a href="#一些观点" class="headerlink" title="一些观点"></a>一些观点</h2><pre><code>数据扩增是将我们的先验知识融入到模型中的一种方案。
mixup相当于一个正则项，它希望模型尽可能往线性函数靠近，也就是说，既保证模型预测尽可能准确，又让模型尽可能简单。
</code></pre>

        
    </div>
    </div>

    <div class="article-badge">
        
        
    </div>

</div>

<footer class="article-footer">
    <div class="article-more-info">
    <div class="article-date">
  <time datetime="2022-04-11T10:15:27.000Z" itemprop="datePublished">2022-04-11</time>
</div>
    
        <div class="article-category">
        <a class="article-category-link" href="/categories/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/">专题学习</a>
        </div>
    
    
        <div class="article-tag">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li></ul>
        </div>
    
    </div>
</footer>

</article>

    
  
    
      <article
id="post-学习笔记/专题学习/高斯分布"
class="article article-type-post"
>



<div class="article-inner">
    

    <div class="article-body">
    <header class="article-title">
        <a href="/2022/04/11/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/">高斯分布</a>
    </header>
    <div class="article-entry post-inner-html">
        
        <h3 id="单变量高斯分布-univariate-Gaussian"><a href="#单变量高斯分布-univariate-Gaussian" class="headerlink" title="单变量高斯分布(univariate Gaussian )"></a>单变量高斯分布(univariate Gaussian )</h3><p>$$<br>f(x)&#x3D;\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)<br>$$</p>
<h3 id="单高斯分布"><a href="#单高斯分布" class="headerlink" title="单高斯分布"></a>单高斯分布</h3><p>$$<br>N(x ; u, \Sigma)&#x3D;\frac{1}{\sqrt{2 \pi}|\Sigma|} \exp \left[-\frac{1}{2}(x-u)^{T}\Sigma^{-1} (x-u)\right]<br>$$</p>
<p>单高斯与单变量高斯的区别在于前者维度为$d$，后者维度为$1$。对于单高斯模型，$\mu$通常代表训练样本的均值，$\Sigma$代表样本的方差。</p>
<h3 id="高斯混合模型（GMM）"><a href="#高斯混合模型（GMM）" class="headerlink" title="高斯混合模型（GMM）"></a>高斯混合模型（GMM）</h3><p>$$<br>\operatorname{Pr}(x)&#x3D;\Sigma_{k&#x3D;1}^{K} \pi_{k} N\left(x ; u_{k}, \Sigma_{k}\right)<br>$$</p>
<p>依据全概率公式，$\pi_{k}$是选中参数$u_{k}$和$\Sigma_{k}$的概率，又称权重因子。EM算法用于优化GMM。</p>

        
    </div>
    </div>

    <div class="article-badge">
        
        
    </div>

</div>

<footer class="article-footer">
    <div class="article-more-info">
    <div class="article-date">
  <time datetime="2022-04-11T08:15:27.000Z" itemprop="datePublished">2022-04-11</time>
</div>
    
        <div class="article-category">
        <a class="article-category-link" href="/categories/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/">专题学习</a>
        </div>
    
    
        <div class="article-tag">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li></ul>
        </div>
    
    </div>
</footer>

</article>

    
  
    
      <article
id="post-学习笔记/《统计学习方法》（第二版）/第5章 决策树"
class="article article-type-post"
>



<div class="article-inner">
    

    <div class="article-body">
    <header class="article-title">
        <a href="/2022/04/10/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC5%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/">第5章 决策树</a>
    </header>
    <div class="article-entry post-inner-html">
        
        <p>决策树（decision tree）是一种基本的分类和回归方法，它可以任务是if-then规则（<strong>互斥且完备</strong>）的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。<br>因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题，得到的决策树是sub-optimal的。<br>决策树的生成对应于模型的局部选择<code>局部最优</code>，决策树的剪枝对应于模型的全局选择<code>全局最优</code>。</p>
<h3 id="1-特征选择"><a href="#1-特征选择" class="headerlink" title="1. 特征选择"></a>1. 特征选择</h3><p>特征现在在于选取对训练数据具有分类能力的特征，通常特征选择的准则是信息增益或信息增益比。<br><strong>熵</strong><br>熵（entropy）是表示随机变量不确定性的度量：$H(X)&#x3D;-\sum_i^N{p_i\log{p_i}},\ 0\le{H(p)\le\log{n}}$<br>条件熵（conditional entropy）表示在已知随机变量X的条件下随机变量Y的不确定性：$H(Y|X)&#x3D;\sum_i^N{p_iH(Y|X&#x3D;x_i)}$</p>
<p><strong>信息增益</strong>（Information gain）表示得知特征X的信息而使得类Y的信息的确定性减少的程度：<br>$g(D,A)&#x3D;H(D)-H(D|A)$<br>上式中$H(D)[empirical\ entropy]$表示对数据集D进行分类的不确定性，$H(D|A)[empirical\ comditional\ entropy]$表示在特征A给定的条件下对数据集D进行分类的不确定性。一般地，熵与条件熵的差称为互信息（mutual Information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p>
<p><strong>信息增益比</strong>（Information gain ratio）信息增益存在偏向于选择取值较多的特征，ratio可以避免此问题。<br>$g_R(D,A)&#x3D;\frac{g(D,A)}{H_A(D)},\ H_A(D)&#x3D;-\sum_{i&#x3D;1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|},\ n是特征A的取值个数$</p>
<h3 id="2-决策树的生成"><a href="#2-决策树的生成" class="headerlink" title="2. 决策树的生成"></a>2. 决策树的生成</h3><p><strong>ID3：</strong>核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。ID3相当于用极大似然法进行概率模型的选择。<strong>C4.5：</strong>改进ID3，并使用信息增益比。</p>
<h3 id="3-决策树的剪枝（pruning）"><a href="#3-决策树的剪枝（pruning）" class="headerlink" title="3. 决策树的剪枝（pruning）"></a>3. 决策树的剪枝（pruning）</h3><p>决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现。<br>设树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该结点有$N_t$个样本点，其中$k$类样本点有$N_{tk}$个，则决策树学习的损失函数可以定义为：<br>$C_\alpha(T)&#x3D;\sum_t^{|T|}N_tH_t(T)+\alpha|T|&#x3D;C(T)+\alpha|T|\ H_t(T)&#x3D;-\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}\C(T)&#x3D;\sum_t^{|T|}{N_tH_t(T)&#x3D;-\sum_t^{|T|}\sum_k^K{N_{tk}}\log\frac{N_{tk}}{N_t}}$<br>$C(T)$表示模型对训练数据的预测误差（模型与训练数据的拟合程度），$|T|$表示模型复杂度，$\alpha\ge0$控制两者间的影响。较大的$\alpha$促使选择较简单的模型，较小的$\alpha$促使选择较复杂的模型，$\alpha&#x3D;0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。剪枝就是当$\alpha$确定时，选择损失函数最小的模型。损失函数的极小化等价于正则化的极大似然估计。</p>
<blockquote>
<p><strong>输入：</strong>生成算法产生的整个树$T$，参数$\alpha$<br><strong>输出：</strong>修剪后的子树$T_{\alpha}$<br>1-计算每个结点的经验熵<br>2-递归地从树的叶结点向上回缩<br>3-根据回缩前B后A损失函数值$C_{\alpha}(T_A)\le C_{\alpha}(T_B)$，剪掉后损失减少则剪枝（重复）</p>
</blockquote>
<h3 id="4-CART算法"><a href="#4-CART算法" class="headerlink" title="4. CART算法"></a>4. CART算法</h3><p>分类与回归树（CART）是在给定输入随机变量X条件下输出随机变量Y的条件概率分布。（左是右否的二叉树）</p>
<blockquote>
<p><strong>生成：</strong>基于训练数据集生成决策树，生成的树要尽量大。<br><strong>剪枝：</strong>用验证集对已生成的树进行剪枝并选择最优子树，用损失函数最小作为剪枝的标准。</p>
</blockquote>
<p><strong>CART生成</strong><br><code>回归树：</code>平方误差最小化准则<br><code>分类树：</code>基尼指数最小化准则</p>
<blockquote>
<p><strong>最小二乘回归树生成算法</strong><br><strong>输入：</strong>训练数据集$D$<br><strong>输出：</strong>回归树$f(x)$<br>在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉树：1-选择最优切分变量$j$与切分点$s$，求解$\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$遍历变量$j$对固定的切分变量$j$扫描切分点$s$选择使式达到最小值的对$(j,s)$。2-用选定的$(j,s)$划分区域并决定相应的输出值$R_1(j,s)&#x3D;{x|x^{(j)}\le s},\ \ R_2(j,s)&#x3D;{x|x^{(j)}\gt s},\ \ \hat{c}<em>m&#x3D;\frac{1}{N_m}\sum</em>{x_i\in R_m(j,s)}y_i,x\in R_m$。3-重复1、2。4-将输入空间划分为$M$个区域$R_1,R_2,\cdots,R_M$生成决策树：$f(x)&#x3D;\sum_m^M\hat{c}_mI(x\in R_m)$</p>
<p><strong>基尼指数分类树生成算法</strong><br><strong>输入：</strong>训练数据集$D$，停止计算的条件<br><strong>输出：</strong>CART决策树<br>从根结点开始构建二叉树：1-设结点的训练数据集为$D$，计算现有特征对数据集的基尼指数$Gini(D)$。 根据特征$A&#x3D;a$将数据集分成$D_1$和$D_2$计算基尼指数$Gini(D,A)$。2-在所有特征与切分点对$&lt;A,a&gt;$中选择基尼指数最小的作为最优特征与最优切分点，由此生成两个子结点。3-重复1，2，直到满足停止条件。4-生成CART树。</p>
<p><code>基尼指数：</code>$K$类中样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数为：$Gini(p)&#x3D;\sum_k^K{p_k(1-p_k)}&#x3D;1-\sum_k^K{p_k^2}$。对于给定样本集合$D$，基尼指数为$Gini(D)&#x3D;1-\sum_k^K(\frac{|C_k|}{|D|})^2$，对于在特征$A$条件下的集合$D$的基尼指数为$Gini(D,A)&#x3D;\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$。基尼指数越大，样本集合的不确定性也越大（与熵类似）。<br><code>排列组合：</code>排列$A_n^m&#x3D;\frac{n!}{(n-m)!}$， 组合$C_n^m&#x3D;\pmatrix{\begin{matrix}n\m\end{matrix}}&#x3D;\frac{A_n^m}{m!}&#x3D;\frac{n!}{m!(n-m)!}&#x3D;C(n,m)&#x3D;C(n,n-m)$<br><code>二项式定理：</code>$(a+b)^n&#x3D;\sum_i^nC_n^i a^{n-i}b^i$，$C_n^i$杨辉三角<br><code>伯努利分布：</code>关于布尔变量$x\in{0,1}$的概率分布，其连续参数$p\in[0,1]$表示变量$x&#x3D;1$的概率：$p(x)&#x3D;p^x(1-p)^{1-x},E(x)&#x3D;p,\sigma&#x3D;p(1-p)$<br><code>二项分布：</code>1-实验次数固定为$n$；2-每次事件只有两种可能；3-每次成功的概率固定为$p$；4-表示成功$x$次的概率：$p(x)&#x3D;C_n^x p^x(1-p)^{n-x},E(x)&#x3D;np,\sigma&#x3D;\sqrt{np(1-p)}$<br><code>几何分布：</code>0-与二项分类相似，唯一不同在于4-表示进行$x$次尝试，取得第一次成功的概率：$p(x)&#x3D;(1-p)^{x-1}p,E(x)&#x3D;1&#x2F;p,\sigma&#x3D;(1-p)&#x2F;p^2$<br>&#96;&#96;泊松分布：&#96;1-事件独立；2-在任意相同的时间范围内，事件发生的概率相同；3-在某个时间范围内，发生某件事情$x$的概率：$p(x)&#x3D;\frac{u^xe^{-u}}{x!},E(x)&#x3D;\sigma&#x3D;u\ [p&#x3D;\frac{u}{n},p(x)&#x3D;\lim_{n\rightarrow\infty}\pmatrix{\begin{matrix}n\m\end{matrix}}p^x(1-p)^{n-x}]即在p上的极限$</p>
<p><code>伯努利扔一次硬币；二项分布是多次伯努利；泊松分布是p很小的二项，即无数次扔硬币且正面概率极小；正态分布是n很大的二项，即无数次扔硬币且硬币完全相同。</code></p>
</blockquote>
<p><strong>CART剪枝</strong></p>
<blockquote>
<p><strong>输入：</strong>CART算法完全生成的决策树$T_0$<br><strong>输出：</strong>最优决策树$T_\alpha$<br>1-设$k&#x3D;0,T&#x3D;T_0,\alpha&#x3D;+\infty$<br>2-自下而上地对各内部结点$t$计算$C(T_t)$，$|T_t|$以及$g(t)&#x3D;\frac{C(t)-C(T_t)}{|T_t|-1},\alpha&#x3D;\min(\alpha,g(t))$。以$|T_t|$为叶结点个数$t$为根节点的子树$T_t$，对训练数据的预测误差$C(T_t)$。<br>3-对$g(t)&#x3D;\alpha$的内部结点$t$进行剪枝，并对叶结点以多数表决法决定其类，得到树$T$。<br>4-设$k&#x3D;k+1,T_k&#x3D;T,\alpha_k&#x3D;\alpha$<br>5-如果$T_k$不是由根节点及两个叶结点构成的树，则回到1-$\alpha&#x3D;+\infty$；否则令$T_k&#x3D;T_n$<br>6-采用交叉验证法在子树序列$T_0,T_1,\cdots,T_n$中选取最优子树$T_{\alpha}$。</p>
</blockquote>

        
    </div>
    </div>

    <div class="article-badge">
        
        
    </div>

</div>

<footer class="article-footer">
    <div class="article-more-info">
    <div class="article-date">
  <time datetime="2022-04-10T13:18:27.000Z" itemprop="datePublished">2022-04-10</time>
</div>
    
        <div class="article-category">
        <a class="article-category-link" href="/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/">《统计学习方法》（第二版）</a>
        </div>
    
    
        <div class="article-tag">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li></ul>
        </div>
    
    </div>
</footer>

</article>

    
  
    
      <article
id="post-学习笔记/《统计学习方法》（第二版）/第3章 K近邻法"
class="article article-type-post"
>



<div class="article-inner">
    

    <div class="article-body">
    <header class="article-title">
        <a href="/2022/04/10/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC3%E7%AB%A0%20K%E8%BF%91%E9%82%BB%E6%B3%95/">第3章 K近邻法</a>
    </header>
    <div class="article-entry post-inner-html">
        
        <h3 id="3-1-k-近邻算法-（-k-nearest-neighbor-KNN）"><a href="#3-1-k-近邻算法-（-k-nearest-neighbor-KNN）" class="headerlink" title="3.1 $k$近邻算法 （$k$-nearest neighbor, KNN）"></a>3.1 $k$近邻算法 （$k$-nearest neighbor, KNN）</h3><blockquote>
<p><strong>输入：</strong>训练集$T&#x3D;{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$<br><strong>输出：</strong>实例$x$所属类别$y$<br>1-根据给定的距离度量，在训练集$T$中找出与$x$最近邻的$k$个点，涵盖这$k$个点的$x$的领域记作$N_k(x)$;<br>2-在$N_k(x)$中根据分类决策规则决定$x$的类别$y$：$y&#x3D;\arg\max_{c_j}\sum_{x_i\in N_k(x)}{I(y_i&#x3D;c_j)}$，$I$为指示函数。</p>
</blockquote>
<p>是一种基本分类与回归方法，没有显式的学习过程</p>
<h3 id="3-2-k-近邻模型"><a href="#3-2-k-近邻模型" class="headerlink" title="3.2 $k$近邻模型"></a>3.2 $k$近邻模型</h3><p>$k$近邻算法使用的模型实际上对应于对特征空间的划分，模型有三个基本要素：距离度量、$k$值得选择和分类决策规则。</p>
<p><strong>距离度量</strong></p>
<blockquote>
<p>欧式距离、$L_p$距离（Minkowski距离）</p>
<p>$L_p(x_i,y_i)&#x3D;(\sum_{l&#x3D;1}^n|x_i^{(l)}-x_j^{(l)}|^p)^\frac{1}{p},p\ge1$<br>$p&#x3D;2$，欧式距离；$p&#x3D;1$，Manhattan距离；$p&#x3D;\infty$，各个坐标距离的最大值</p>
</blockquote>
<p><strong>$k$值的选择</strong></p>
<blockquote>
<p><strong>较小值：</strong>相当于在较小邻域中训练实例进行预测，近似误差会减小，估计误差会增大，对近邻点非常敏感。<em>k值减小意味着整体模型变得复杂，容易过拟合。</em></p>
<p><strong>较大值：</strong>相当于在较大邻域中训练实例进行预测，估计误差会减小，近似误差会增大，对远邻点依然有效，容易误检。<em>k值增大意味着整体模型变得简单，容易欠拟合。</em></p>
<p>$\mathbf{k&#x3D;N}$：无论什么输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略了实例中大量有用信息。</p>
</blockquote>
<h3 id="3-3-k-近邻法的实现：-kd-树"><a href="#3-3-k-近邻法的实现：-kd-树" class="headerlink" title="3.3 $k$近邻法的实现：$kd$树"></a>3.3 $k$近邻法的实现：$kd$树</h3><p>以特征$x$的第一维为坐标轴，以中位数为切分点，然后是第二维，第三维，划分得到最后的二叉树。</p>

        
    </div>
    </div>

    <div class="article-badge">
        
        
    </div>

</div>

<footer class="article-footer">
    <div class="article-more-info">
    <div class="article-date">
  <time datetime="2022-04-10T13:17:27.000Z" itemprop="datePublished">2022-04-10</time>
</div>
    
        <div class="article-category">
        <a class="article-category-link" href="/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/">《统计学习方法》（第二版）</a>
        </div>
    
    
        <div class="article-tag">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li></ul>
        </div>
    
    </div>
</footer>

</article>

    
  
    
      <article
id="post-学习笔记/《统计学习方法》（第二版）/第4章 朴素贝叶斯法"
class="article article-type-post"
>



<div class="article-inner">
    

    <div class="article-body">
    <header class="article-title">
        <a href="/2022/04/10/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC4%E7%AB%A0%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/">第4章 朴素贝叶斯法</a>
    </header>
    <div class="article-entry post-inner-html">
        
        <h3 id="1-基本方法"><a href="#1-基本方法" class="headerlink" title="1.基本方法"></a>1.基本方法</h3><p>朴素贝叶斯法（naive Bayes）是基于贝叶斯定理与特征条件独立假设的分类法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$.</p>
<p>先学习先验概率分布$P(Y&#x3D;c_k)$及条件概率分布$P(X&#x3D;x|Y&#x3D;c_k)&#x3D;P(X^{(1)}&#x3D;x^{(1)},X^{(2)}&#x3D;x^{(2)},\cdots,X^{(N)}&#x3D;x^{(n)}|Y&#x3D;c_k)$从而得到联合概率分布$P(X,Y)$<br>由于条件概率分布有指数级数量的参数，所以对条件概率分布做了<strong>条件独立性假设</strong><code>用于分类的特征在类确定的条件下都是条件独立的</code>：$P(X&#x3D;x|Y&#x3D;c_k)&#x3D;P(X^{(1)}&#x3D;x^{(1)},X^{(2)}&#x3D;x^{(2)},\cdots,X^{(N)}&#x3D;x^{(n)}|Y&#x3D;c_k)&#x3D;\prod_{j&#x3D;1}^n{P(X^j&#x3D;x^j|Y&#x3D;c_k)}$<br>分类器：$y&#x3D;f(x)&#x3D;\arg\max_{c_k}\frac{P(Y&#x3D;c_k)\Pi_{j}P(X^{j}&#x3D;x^j|Y&#x3D;c_k)}{\sum_k{P(Y&#x3D;c_k)\Pi_{j}P(X^{j}&#x3D;x^j|Y&#x3D;c_k)}}$</p>
<p><strong>后验概率最大化的含义</strong></p>
<p>期望风险最小化准则变为了后验概率最大化准则<br>$$<br>\begin{align}<br>f(x)&amp;&#x3D;\arg\min_{y\in\mathcal{Y}}\sum_{k&#x3D;1}^K{L(c_k,y)P(c_k|X&#x3D;x)}\<br>&amp;&#x3D;\arg\min_{y\in\mathcal{Y}}\sum_{k&#x3D;1}^K{P(y\neq c_k|X&#x3D;x)}\<br>&amp;&#x3D;\arg\min_{y\in\mathcal{Y}}{(1-P(y&#x3D;c_k|X&#x3D;x))}\<br>&amp;&#x3D;\arg\max_{y\in\mathcal{Y}}{P(y&#x3D;c_k|X&#x3D;x)}\<br>\end{align}<br>$$</p>
<h3 id="2-朴素贝叶斯法的参数估计"><a href="#2-朴素贝叶斯法的参数估计" class="headerlink" title="2.朴素贝叶斯法的参数估计"></a>2.朴素贝叶斯法的参数估计</h3><p><strong>极大似然估计</strong></p>
<p>估计先验概率：$P(Y&#x3D;c_k)&#x3D;\frac{\sum_{i&#x3D;1}^N{I(y_i&#x3D;c_k)}}{N},I为指示函数$<br>估计条件概率：$P(X^j&#x3D;a_{jl}|Y&#x3D;c_k)&#x3D;\frac{\sum_{i&#x3D;1}^{N}{I(x_i^j&#x3D;a_{jl},y_i&#x3D;c_k)}}{\sum_{i&#x3D;1}^{N}{I(y_i&#x3D;c_k)}}$<br>会出现所有估计概率值为0的情况</p>
<p><strong>贝叶斯估计</strong></p>
<p>估计先验概率：$P_\lambda(Y&#x3D;c_k)&#x3D;\frac{\sum_{i&#x3D;1}^N{I(y_i&#x3D;c_k)}+\lambda}{N+K\lambda},I为指示函数$<br>估计条件概率：$P_\lambda(X^j&#x3D;a_{jl}|Y&#x3D;c_k)&#x3D;\frac{\sum_{i&#x3D;1}^N{I(x_i^j&#x3D;a_{jl},y_i&#x3D;c_k)+\lambda}}{\sum_{i&#x3D;1}^N{I(y_i&#x3D;c_k)+S_j\lambda}}$<br>$\lambda&#x3D;0$就是<code>极大似然估计</code>，$\lambda&#x3D;1$就是<code>拉普拉斯平滑</code></p>

        
    </div>
    </div>

    <div class="article-badge">
        
        
    </div>

</div>

<footer class="article-footer">
    <div class="article-more-info">
    <div class="article-date">
  <time datetime="2022-04-10T13:17:27.000Z" itemprop="datePublished">2022-04-10</time>
</div>
    
        <div class="article-category">
        <a class="article-category-link" href="/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/">《统计学习方法》（第二版）</a>
        </div>
    
    
        <div class="article-tag">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li></ul>
        </div>
    
    </div>
</footer>

</article>

    
  
    
      <article
id="post-学习笔记/《统计学习方法》（第二版）/第2章 感知机"
class="article article-type-post"
>



<div class="article-inner">
    

    <div class="article-body">
    <header class="article-title">
        <a href="/2022/04/10/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC2%E7%AB%A0%20%E6%84%9F%E7%9F%A5%E6%9C%BA/">第2章 感知机</a>
    </header>
    <div class="article-entry post-inner-html">
        
        <h3 id="2-1-感知机模型"><a href="#2-1-感知机模型" class="headerlink" title="2.1 感知机模型"></a>2.1 感知机模型</h3><p>$$<br>f(x)&#x3D;sign(w\cdot x+b),\ sign(x)&#x3D;\begin{cases}+1,x\ge0\-1,x\lt0\end{cases}<br>$$</p>
<p>对于特征空间$\mathbf{R}^n$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距。</p>
<h3 id="2-2-感知机学习策略"><a href="#2-2-感知机学习策略" class="headerlink" title="2.2 感知机学习策略"></a>2.2 感知机学习策略</h3><ul>
<li>数据集的线性可分性</li>
<li>损失函数是参数$w$和$b$连续可导函数：点到超平面的距离 $\frac{1}{|w|}|w\cdot x_i+b|$</li>
<li>误分类数据满足：$-y_i(w\cdot x_i+b)\gt0$，$y(w\cdot x+b)$称为样本点的函数间隔</li>
</ul>
<h3 id="2-3-感知机学习算法"><a href="#2-3-感知机学习算法" class="headerlink" title="2.3 感知机学习算法"></a>2.3 感知机学习算法</h3><p><strong>原始形式</strong></p>
<blockquote>
<p><strong>输入：</strong>数据集、学习率$\eta\ (0\lt\eta\le1)$<br><strong>输出：</strong>$w,b,f(x)&#x3D;sign(w\cdot x+b)$</p>
<ol>
<li><p>选取初始值$w_0,b_0$</p>
</li>
<li><p>在训练集中选取数据$(x_i,y_i)$</p>
</li>
<li><p>如果$y_i(w\cdot x_i+b)\le0$</p>
<p>$$w\leftarrow w+\eta y_i x_i\<br>b\leftarrow b+\eta y_i$$</p>
</li>
<li><p>转至2，直到无误分类点</p>
</li>
</ol>
</blockquote>
<p>首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数，在一个过程中一次随机选取一个误分类点使其梯度下降。</p>
<p><strong>Novikoff定理</strong></p>
<ol>
<li><p>$\exists:y_i(\hat{W}<em>{opt}\cdot\hat{x}<em>i)&#x3D;y_i(W</em>{opt}\cdot x_i+b</em>{opt})\ge\gamma,|\hat{W}_{opt}|&#x3D;1$</p>
</li>
<li><p>在训练集上的误分类次数$k$满足不等式：$k\le(\frac{R}{\gamma})^2,R&#x3D;\max_{1\le i\le N}|\hat{x}_i|^2$</p>
</li>
</ol>
<p>说明误分类有上界；训练集可分时，原始形式收敛，不可分时，不收敛；2可以近似看成线段划分，即特征空间最多可以划分成K个。</p>
<p><code>当训练集可分时，感知机学习算法存在无穷多解，由于不同的初始值或不同的迭代顺序而有所不同。</code></p>
<p><strong>对偶形式</strong></p>
<blockquote>
<p><strong>输入：</strong>数据集、学习率$\eta\ (0\lt\eta\le1)$<br><strong>输出：</strong>$\alpha,b,f(x)&#x3D;sign(\sum_{j&#x3D;1}^N\alpha_j y_j x_j\cdot x+b)$</p>
<ol>
<li><p>选取初始值$a\leftarrow0,b\leftarrow0$</p>
</li>
<li><p>在训练集中选取数据$(x_i,y_i)$</p>
</li>
<li><p>如果$y_i(\sum_{j&#x3D;1}^N\alpha_j y_j x_j\cdot x_i+b)\le0$</p>
</li>
</ol>
<p>  $$\alpha_i\leftarrow\alpha_i+\eta\<br>  b\leftarrow b+\eta y_i$$</p>
<ol start="4">
<li>转至2，直到无误分类点</li>
</ol>
</blockquote>
<p><strong>基本思想：</strong>将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合，通过求解其系数而得到$w$和$b$。实例点更新次数越多，意味着它距离分类超平面越近，也就越难正确分类。对偶形式中训练实例仅以內积$x_j\cdot x_i$的形式出现（Gram矩阵）。</p>
<h3 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h3><p><strong>1. 为什么感知机不能表示异或</strong></p>
<p><strong>2. 样本集线性可分的充要条件是正实例点击所构成的凸壳$^1$与负实例所构成的凸壳互不相交</strong></p>
<p>$^1$设集合$S\subset\mathbf{R}^n$是由$\mathbf{R}^n$中的$k$个点组成的集合，即$S&#x3D;{x_1,x_2,\cdots,x_k}$ 定义$S$的凸壳$conv(S)$为：<br>$$<br>conv(S)&#x3D;\left{x&#x3D;\sum_{i&#x3D;1}^k{\lambda_i x_i}\mid\sum_{i&#x3D;1}^k{\lambda_i&#x3D;1},\lambda_i\ge0,i&#x3D;1,2,\cdots,k\right}<br>$$<br><code>最小凸多边形、点集合的边界所围成的区域、点集的线性组合</code></p>

        
    </div>
    </div>

    <div class="article-badge">
        
        
    </div>

</div>

<footer class="article-footer">
    <div class="article-more-info">
    <div class="article-date">
  <time datetime="2022-04-10T13:16:27.000Z" itemprop="datePublished">2022-04-10</time>
</div>
    
        <div class="article-category">
        <a class="article-category-link" href="/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/">《统计学习方法》（第二版）</a>
        </div>
    
    
        <div class="article-tag">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li></ul>
        </div>
    
    </div>
</footer>

</article>

    
  

  
    <nav class="pagination">
        <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next</a>
    </nav>
  
</div>

                </section>
            </section>

            
            <aside class="sidebar ">
                


<div class="widget" id="widget">
    
      
    
      
  <div class="widget-wrap widget-cate">
    <div class="widget-title"><span>Categories</span></div>
    <div class="widget-inner">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/">《统计学习方法》（第二版）</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/">专题学习</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap widget-tags">
    <div class="widget-title"><span>Tags</span></div>
    <div class="widget-inner">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B5%8B%E8%AF%95/" rel="tag">测试</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap widget-recent-posts">
    <div class="widget-title"><span>Recent Posts</span></div>
    <div class="widget-inner">
      <ul>
        
          <li>
            <a href="/2022/04/11/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E5%87%B8%E4%BC%98%E5%8C%96/">凸优化</a>
          </li>
        
          <li>
            <a href="/2022/04/11/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4/">希尔伯特空间</a>
          </li>
        
          <li>
            <a href="/2022/04/11/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0/">流形学习</a>
          </li>
        
          <li>
            <a href="/2022/04/11/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E7%A7%91%E5%AD%A6%E7%A9%BA%E9%97%B4%E7%AC%94%E8%AE%B0/">科学空间笔记</a>
          </li>
        
          <li>
            <a href="/2022/04/11/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/">高斯分布</a>
          </li>
        
      </ul>
    </div>
  </div>

    
</div>

<div id="backtop"><i class="icon icon-arrow-up"></i></div>
            </aside>
            
        </div>
    </div>

    <footer class="footer">
    <div class="footer-wave">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1440 320"><path fill="#3c4859" fill-opacity="1" d="M0,160L60,181.3C120,203,240,245,360,240C480,235,600,181,720,186.7C840,192,960,256,1080,261.3C1200,267,1320,213,1380,186.7L1440,160L1440,320L1380,320C1320,320,1200,320,1080,320C960,320,840,320,720,320C600,320,480,320,360,320C240,320,120,320,60,320L0,320Z"></path></svg>
    </div>

    <!-- Please do not remove this -->
    <!-- 开源不易，请勿删除 -->
    <div class="footer-wrap">
        <div class="footer-inner"> 
            Seeyourmind &copy; 2022<br>
            Powered By Hexo · Theme By <a href="https://linhong.me/" target="_blank">Aomori</a> · <a href="https://github.com/lh1me/hexo-theme-aomori" target="_blank">Github</a>
        </div>
    </div>

</footer>


<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>





<script src="/dist/build.js?1646451311888.js"></script>


<script src="/dist/custom.js?1646451311888.js"></script>













</body>

</html>