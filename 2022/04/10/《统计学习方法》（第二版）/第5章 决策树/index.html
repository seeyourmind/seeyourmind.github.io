<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

    
    <title>第5章 决策树 | Seeyourmind</title>

    <meta name="description" content="&lt;p&gt;决策树（decision tree）是一种基本的分类和回归方法，它可以任务是if-then规则（&lt;strong&gt;互斥且完备&lt;/strong&gt;）的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。&lt;br&gt;因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题，得到的决策树是sub-optimal的。&lt;br&gt;决策树的生成对应于模型的局部选择&lt;code&gt;局部最优&lt;/code&gt;，决策树的剪枝对应于模型的全局选择&lt;code&gt;全局最优&lt;/code&gt;。&lt;/p&gt;">
    <meta name="keywords" content="">

    

    <meta property="og:locale" content="cn,en,ja,default" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content= "第5章 决策树 | Seeyourmind"  />
    <meta property="og:description" content= "&lt;p&gt;决策树（decision tree）是一种基本的分类和回归方法，它可以任务是if-then规则（&lt;strong&gt;互斥且完备&lt;/strong&gt;）的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。&lt;br&gt;因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题，得到的决策树是sub-optimal的。&lt;br&gt;决策树的生成对应于模型的局部选择&lt;code&gt;局部最优&lt;/code&gt;，决策树的剪枝对应于模型的全局选择&lt;code&gt;全局最优&lt;/code&gt;。&lt;/p&gt;" />
    <meta property="og:url" content="http://example.com/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC5%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/index.html" />
    <meta property="og:site_name" content="" />
    <meta property="article:author" content="FZY" />
    <meta property="article:publisher" content="" />
    <meta property="og:description" content="&lt;p&gt;决策树（decision tree）是一种基本的分类和回归方法，它可以任务是if-then规则（&lt;strong&gt;互斥且完备&lt;/strong&gt;）的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。&lt;br&gt;因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题，得到的决策树是sub-optimal的。&lt;br&gt;决策树的生成对应于模型的局部选择&lt;code&gt;局部最优&lt;/code&gt;，决策树的剪枝对应于模型的全局选择&lt;code&gt;全局最优&lt;/code&gt;。&lt;/p&gt;" />
    <meta name="twitter:title" content="第5章 决策树 | Seeyourmind"/>
    <meta name="twitter:description" content="&lt;p&gt;决策树（decision tree）是一种基本的分类和回归方法，它可以任务是if-then规则（&lt;strong&gt;互斥且完备&lt;/strong&gt;）的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。&lt;br&gt;因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题，得到的决策树是sub-optimal的。&lt;br&gt;决策树的生成对应于模型的局部选择&lt;code&gt;局部最优&lt;/code&gt;，决策树的剪枝对应于模型的全局选择&lt;code&gt;全局最优&lt;/code&gt;。&lt;/p&gt;"/>
    <script type="application/ld+json">
        {
            "description": "&lt;p&gt;决策树（decision tree）是一种基本的分类和回归方法，它可以任务是if-then规则（&lt;strong&gt;互斥且完备&lt;/strong&gt;）的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。&lt;br&gt;因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题，得到的决策树是sub-optimal的。&lt;br&gt;决策树的生成对应于模型的局部选择&lt;code&gt;局部最优&lt;/code&gt;，决策树的剪枝对应于模型的全局选择&lt;code&gt;全局最优&lt;/code&gt;。&lt;/p&gt;",
            "author": { "@type": "Person", "name": "FZY" },
            "@type": "BlogPosting",
            "url": "http://example.com/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC5%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/index.html",
            "publisher": {
            "@type": "Organization",
            "logo": {
                "@type": "ImageObject",
                "url": "http://example.comundefined"
            },
            "name": "FZY"
            },
            "headline": "第5章 决策树 | Seeyourmind",
            "datePublished": "2022-04-10T13:18:27.000Z",
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "http://example.com/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC5%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/index.html"
            },
            "@context": "http://schema.org"
        }
    </script>




    

    

    

    

    
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>😉</text></svg>">
    

    
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    

    

    
<link rel="stylesheet" href="/dist/build.css?v=1646451311888.css">


    
<link rel="stylesheet" href="/dist/custom.css?v=1646451311888.css">


    <script>
        window.isPost = true
        window.aomori = {
            
            gitalk: {
                enable: true,
                clientID: "7f65031d09ca001ce168",
                clientSecret: "886e5d3f9b29dddbae3fa445680cb33d8f8b55eb",
                repo: "seeyourmind.github.io",
                owner: "seeyourmind",
                admin: ["seeyourmind",],
                distractionFreeMode: true // Facebook-like distraction free mode  // Facebook-like distraction free mode
            },
            
            
            
        }
        window.aomori_logo_typed_animated = true
        window.aomori_search_algolia = false

    </script>

    <!--将该代码放入博客模板的head中即可-->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
        }
        });
    </script>
    <!--latex数学显示公式-->
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<meta name="generator" content="Hexo 6.1.0"></head>

<body>

    <div class="container">
    <header class="header">
        <div class="header-type">
            
            <div class="header-type-inner">
                
                    <div id="typed-strings" style="display:none">
                        <p>Seeyourmind</p>
                    </div>
                    <a class="header-type-title" id="typed" href="/"></a>
                
    
                
            </div>
        </div>
        <div class="header-menu">
            <div class="header-menu-inner">
                
                <a href="/">首页</a>
                
                <a href="/archives">归档</a>
                
            </div>
            <div class="header-menu-social">
                
    <a class="social" target="_blank" href="https://github.com/seeyourmind">
        <box-icon type='logo' name='github'></box-icon>
    </a>

    <a class="social" target="_blank" href="live:.cid.1f6ac2746d42e0ee">
        <box-icon type='logo' name='skype'></box-icon>
    </a>

            </div>
        </div>

        <div class="header-menu-mobile">
            <div class="header-menu-mobile-inner" id="mobile-menu-open">
                <i class="icon icon-menu"></i>
            </div>
        </div>
    </header>

    <div class="header-menu-mobile-menu">
        <div class="header-menu-mobile-menu-bg"></div>
        <div class="header-menu-mobile-menu-wrap">
            <div class="header-menu-mobile-menu-inner">
                <div class="header-menu-mobile-menu-close" id="mobile-menu-close">
                    <i class="icon icon-cross"></i>
                </div>
                <div class="header-menu-mobile-menu-list">
                    
                    <a href="/">首页</a>
                    
                    <a href="/archives">归档</a>
                    
                </div>
            </div>
        </div>
    </div>

</div>

    <div class="container">
        <div class="main">
            <section class="inner">
                <section class="inner-main">
                    <div class="post">
    <article id="post-cl3eh2ri7001bp8tx9g5qf2tk" class="article article-type-post" itemscope
    itemprop="blogPost">

    <div class="article-inner">

        
          
        
        
        

        
        <header class="article-header">
            
  
    <h1 class="article-title" itemprop="name">
      第5章 决策树
    </h1>
  

        </header>
        

        <div class="article-more-info article-more-info-post hairline">

            <div class="article-date">
  <time datetime="2022-04-10T13:18:27.000Z" itemprop="datePublished">2022-04-10</time>
</div>

            
            <div class="article-category">
                <a class="article-category-link" href="/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/">《统计学习方法》（第二版）</a>
            </div>
            

            
            <div class="article-tag">
                <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li></ul>
            </div>
            

            
            <div class="article-busuanzi">
                <span id="busuanzi_value_page_pv">N</span> 人看过
            </div>
            

        </div>

        <div class="article-entry post-inner-html hairline" itemprop="articleBody">
            <p>决策树（decision tree）是一种基本的分类和回归方法，它可以任务是if-then规则（<strong>互斥且完备</strong>）的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。<br>因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题，得到的决策树是sub-optimal的。<br>决策树的生成对应于模型的局部选择<code>局部最优</code>，决策树的剪枝对应于模型的全局选择<code>全局最优</code>。</p>
<span id="more"></span>
<h3 id="1-特征选择"><a href="#1-特征选择" class="headerlink" title="1. 特征选择"></a>1. 特征选择</h3><p>特征现在在于选取对训练数据具有分类能力的特征，通常特征选择的准则是信息增益或信息增益比。<br><strong>熵</strong><br>熵（entropy）是表示随机变量不确定性的度量：$H(X)&#x3D;-\sum_i^N{p_i\log{p_i}},\ 0\le{H(p)\le\log{n}}$<br>条件熵（conditional entropy）表示在已知随机变量X的条件下随机变量Y的不确定性：$H(Y|X)&#x3D;\sum_i^N{p_iH(Y|X&#x3D;x_i)}$</p>
<p><strong>信息增益</strong>（Information gain）表示得知特征X的信息而使得类Y的信息的确定性减少的程度：<br>$g(D,A)&#x3D;H(D)-H(D|A)$<br>上式中$H(D)[empirical\ entropy]$表示对数据集D进行分类的不确定性，$H(D|A)[empirical\ comditional\ entropy]$表示在特征A给定的条件下对数据集D进行分类的不确定性。一般地，熵与条件熵的差称为互信息（mutual Information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p>
<p><strong>信息增益比</strong>（Information gain ratio）信息增益存在偏向于选择取值较多的特征，ratio可以避免此问题。<br>$g_R(D,A)&#x3D;\frac{g(D,A)}{H_A(D)},\ H_A(D)&#x3D;-\sum_{i&#x3D;1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|},\ n是特征A的取值个数$</p>
<h3 id="2-决策树的生成"><a href="#2-决策树的生成" class="headerlink" title="2. 决策树的生成"></a>2. 决策树的生成</h3><p><strong>ID3：</strong>核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。ID3相当于用极大似然法进行概率模型的选择。<strong>C4.5：</strong>改进ID3，并使用信息增益比。</p>
<h3 id="3-决策树的剪枝（pruning）"><a href="#3-决策树的剪枝（pruning）" class="headerlink" title="3. 决策树的剪枝（pruning）"></a>3. 决策树的剪枝（pruning）</h3><p>决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现。<br>设树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该结点有$N_t$个样本点，其中$k$类样本点有$N_{tk}$个，则决策树学习的损失函数可以定义为：<br>$C_\alpha(T)&#x3D;\sum_t^{|T|}N_tH_t(T)+\alpha|T|&#x3D;C(T)+\alpha|T|\ H_t(T)&#x3D;-\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}\C(T)&#x3D;\sum_t^{|T|}{N_tH_t(T)&#x3D;-\sum_t^{|T|}\sum_k^K{N_{tk}}\log\frac{N_{tk}}{N_t}}$<br>$C(T)$表示模型对训练数据的预测误差（模型与训练数据的拟合程度），$|T|$表示模型复杂度，$\alpha\ge0$控制两者间的影响。较大的$\alpha$促使选择较简单的模型，较小的$\alpha$促使选择较复杂的模型，$\alpha&#x3D;0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。剪枝就是当$\alpha$确定时，选择损失函数最小的模型。损失函数的极小化等价于正则化的极大似然估计。</p>
<blockquote>
<p><strong>输入：</strong>生成算法产生的整个树$T$，参数$\alpha$<br><strong>输出：</strong>修剪后的子树$T_{\alpha}$<br>1-计算每个结点的经验熵<br>2-递归地从树的叶结点向上回缩<br>3-根据回缩前B后A损失函数值$C_{\alpha}(T_A)\le C_{\alpha}(T_B)$，剪掉后损失减少则剪枝（重复）</p>
</blockquote>
<h3 id="4-CART算法"><a href="#4-CART算法" class="headerlink" title="4. CART算法"></a>4. CART算法</h3><p>分类与回归树（CART）是在给定输入随机变量X条件下输出随机变量Y的条件概率分布。（左是右否的二叉树）</p>
<blockquote>
<p><strong>生成：</strong>基于训练数据集生成决策树，生成的树要尽量大。<br><strong>剪枝：</strong>用验证集对已生成的树进行剪枝并选择最优子树，用损失函数最小作为剪枝的标准。</p>
</blockquote>
<p><strong>CART生成</strong><br><code>回归树：</code>平方误差最小化准则<br><code>分类树：</code>基尼指数最小化准则</p>
<blockquote>
<p><strong>最小二乘回归树生成算法</strong><br><strong>输入：</strong>训练数据集$D$<br><strong>输出：</strong>回归树$f(x)$<br>在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉树：1-选择最优切分变量$j$与切分点$s$，求解$\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$遍历变量$j$对固定的切分变量$j$扫描切分点$s$选择使式达到最小值的对$(j,s)$。2-用选定的$(j,s)$划分区域并决定相应的输出值$R_1(j,s)&#x3D;{x|x^{(j)}\le s},\ \ R_2(j,s)&#x3D;{x|x^{(j)}\gt s},\ \ \hat{c}<em>m&#x3D;\frac{1}{N_m}\sum</em>{x_i\in R_m(j,s)}y_i,x\in R_m$。3-重复1、2。4-将输入空间划分为$M$个区域$R_1,R_2,\cdots,R_M$生成决策树：$f(x)&#x3D;\sum_m^M\hat{c}_mI(x\in R_m)$</p>
<p><strong>基尼指数分类树生成算法</strong><br><strong>输入：</strong>训练数据集$D$，停止计算的条件<br><strong>输出：</strong>CART决策树<br>从根结点开始构建二叉树：1-设结点的训练数据集为$D$，计算现有特征对数据集的基尼指数$Gini(D)$。 根据特征$A&#x3D;a$将数据集分成$D_1$和$D_2$计算基尼指数$Gini(D,A)$。2-在所有特征与切分点对$&lt;A,a&gt;$中选择基尼指数最小的作为最优特征与最优切分点，由此生成两个子结点。3-重复1，2，直到满足停止条件。4-生成CART树。</p>
<p><code>基尼指数：</code>$K$类中样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数为：$Gini(p)&#x3D;\sum_k^K{p_k(1-p_k)}&#x3D;1-\sum_k^K{p_k^2}$。对于给定样本集合$D$，基尼指数为$Gini(D)&#x3D;1-\sum_k^K(\frac{|C_k|}{|D|})^2$，对于在特征$A$条件下的集合$D$的基尼指数为$Gini(D,A)&#x3D;\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$。基尼指数越大，样本集合的不确定性也越大（与熵类似）。<br><code>排列组合：</code>排列$A_n^m&#x3D;\frac{n!}{(n-m)!}$， 组合$C_n^m&#x3D;\pmatrix{\begin{matrix}n\m\end{matrix}}&#x3D;\frac{A_n^m}{m!}&#x3D;\frac{n!}{m!(n-m)!}&#x3D;C(n,m)&#x3D;C(n,n-m)$<br><code>二项式定理：</code>$(a+b)^n&#x3D;\sum_i^nC_n^i a^{n-i}b^i$，$C_n^i$杨辉三角<br><code>伯努利分布：</code>关于布尔变量$x\in{0,1}$的概率分布，其连续参数$p\in[0,1]$表示变量$x&#x3D;1$的概率：$p(x)&#x3D;p^x(1-p)^{1-x},E(x)&#x3D;p,\sigma&#x3D;p(1-p)$<br><code>二项分布：</code>1-实验次数固定为$n$；2-每次事件只有两种可能；3-每次成功的概率固定为$p$；4-表示成功$x$次的概率：$p(x)&#x3D;C_n^x p^x(1-p)^{n-x},E(x)&#x3D;np,\sigma&#x3D;\sqrt{np(1-p)}$<br><code>几何分布：</code>0-与二项分类相似，唯一不同在于4-表示进行$x$次尝试，取得第一次成功的概率：$p(x)&#x3D;(1-p)^{x-1}p,E(x)&#x3D;1&#x2F;p,\sigma&#x3D;(1-p)&#x2F;p^2$<br>&#96;&#96;泊松分布：&#96;1-事件独立；2-在任意相同的时间范围内，事件发生的概率相同；3-在某个时间范围内，发生某件事情$x$的概率：$p(x)&#x3D;\frac{u^xe^{-u}}{x!},E(x)&#x3D;\sigma&#x3D;u\ [p&#x3D;\frac{u}{n},p(x)&#x3D;\lim_{n\rightarrow\infty}\pmatrix{\begin{matrix}n\m\end{matrix}}p^x(1-p)^{n-x}]即在p上的极限$</p>
<p><code>伯努利扔一次硬币；二项分布是多次伯努利；泊松分布是p很小的二项，即无数次扔硬币且正面概率极小；正态分布是n很大的二项，即无数次扔硬币且硬币完全相同。</code></p>
</blockquote>
<p><strong>CART剪枝</strong></p>
<blockquote>
<p><strong>输入：</strong>CART算法完全生成的决策树$T_0$<br><strong>输出：</strong>最优决策树$T_\alpha$<br>1-设$k&#x3D;0,T&#x3D;T_0,\alpha&#x3D;+\infty$<br>2-自下而上地对各内部结点$t$计算$C(T_t)$，$|T_t|$以及$g(t)&#x3D;\frac{C(t)-C(T_t)}{|T_t|-1},\alpha&#x3D;\min(\alpha,g(t))$。以$|T_t|$为叶结点个数$t$为根节点的子树$T_t$，对训练数据的预测误差$C(T_t)$。<br>3-对$g(t)&#x3D;\alpha$的内部结点$t$进行剪枝，并对叶结点以多数表决法决定其类，得到树$T$。<br>4-设$k&#x3D;k+1,T_k&#x3D;T,\alpha_k&#x3D;\alpha$<br>5-如果$T_k$不是由根节点及两个叶结点构成的树，则回到1-$\alpha&#x3D;+\infty$；否则令$T_k&#x3D;T_n$<br>6-采用交叉验证法在子树序列$T_0,T_1,\cdots,T_n$中选取最优子树$T_{\alpha}$。</p>
</blockquote>

        </div>

    </div>

    

    

    

    
  <div class="article-copyright hairline">
    <p>
      本作品采用  <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议 (CC BY-NC-ND 4.0)</a> 进行许可。
    </p>
  </div>
  

    

    
<nav class="article-nav">
  
    <a href="/2022/04/11/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-caption">下一篇</div>
      <div class="article-nav-title">
        
          高斯分布
        
      </div>
    </a>
  
  
    <a href="/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC4%E7%AB%A0%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-caption">上一篇</div>
      <div class="article-nav-title">第4章 朴素贝叶斯法</div>
    </a>
  
</nav>


    <section class="share">
        <div class="share-title">分享</div>
        <a class="share-item" target="_blank"
            href="https://twitter.com/share?text=第5章 决策树 - Seeyourmind&url=http%3A%2F%2Fexample.com%2F2022%2F04%2F10%2F%25E3%2580%258A%25E7%25BB%259F%25E8%25AE%25A1%25E5%25AD%25A6%25E4%25B9%25A0%25E6%2596%25B9%25E6%25B3%2595%25E3%2580%258B%25EF%25BC%2588%25E7%25AC%25AC%25E4%25BA%258C%25E7%2589%2588%25EF%25BC%2589%2F%25E7%25AC%25AC5%25E7%25AB%25A0%2520%25E5%2586%25B3%25E7%25AD%2596%25E6%25A0%2591%2F">
            <box-icon type='logo' name='twitter'></box-icon>
        </a>
        <a class="share-item" target="_blank"
            href="https://www.facebook.com/sharer.php?title=第5章 决策树 - Seeyourmind&u=http%3A%2F%2Fexample.com%2F2022%2F04%2F10%2F%25E3%2580%258A%25E7%25BB%259F%25E8%25AE%25A1%25E5%25AD%25A6%25E4%25B9%25A0%25E6%2596%25B9%25E6%25B3%2595%25E3%2580%258B%25EF%25BC%2588%25E7%25AC%25AC%25E4%25BA%258C%25E7%2589%2588%25EF%25BC%2589%2F%25E7%25AC%25AC5%25E7%25AB%25A0%2520%25E5%2586%25B3%25E7%25AD%2596%25E6%25A0%2591%2F">
            <box-icon name='facebook-square' type='logo' ></box-icon>
        </a>
        <!-- <a class="share-item" target="_blank"
            href="https://service.weibo.com/share/share.php?title=第5章 决策树 - Seeyourmind&url=http://example.com/2022/04/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/%E7%AC%AC5%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/&pic=">
            <div class="n-icon n-icon-weibo"></div>
        </a> -->
    </section>

</article>








<section class="comments">
    <div id="gitalk-container"></div>
</section>









<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

</div>
                </section>
            </section>

            
            <aside class="sidebar ">
                


<div class="widget" id="widget">
    
      
  <div class="widget-wrap">
    <div class="widget-inner">
      <div class="toc post-toc-html"></div>
    </div>
  </div>

    
      
  <div class="widget-wrap widget-cate">
    <div class="widget-title"><span>Categories</span></div>
    <div class="widget-inner">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Using-Skills/">Using Skills</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/VScode%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/">VScode使用教程</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B-%E9%9B%B7%E6%98%8E/">《机器学习的数学》-雷明</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89/">《统计学习方法》（第二版）</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/">专题学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E5%90%8E%E6%84%9F/">读后感</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap widget-tags">
    <div class="widget-title"><span>Tags</span></div>
    <div class="widget-inner">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/" rel="tag">Git</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LaTeX/" rel="tag">LaTeX</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VScode-config/" rel="tag">VScode.config</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VScode-issue/" rel="tag">VScode.issue</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap widget-recent-posts">
    <div class="widget-title"><span>Recent Posts</span></div>
    <div class="widget-inner">
      <ul>
        
          <li>
            <a href="/2022/08/17/LaTex-usetech/">LaTex写作实用技巧</a>
          </li>
        
          <li>
            <a href="/2022/06/21/Learning-MathematicsOfMachineLearning-chap1/">第1章 一元函数微积分</a>
          </li>
        
          <li>
            <a href="/2022/05/15/Git-skils/">Git使用教程</a>
          </li>
        
          <li>
            <a href="/2022/05/03/VScode-Python-Remote/">VScode配置Python远程调试</a>
          </li>
        
          <li>
            <a href="/2022/05/01/PostReading-KK-lifestream/">凯文·凯利的103条人生建议</a>
          </li>
        
      </ul>
    </div>
  </div>

    
</div>

<div id="backtop"><i class="icon icon-arrow-up"></i></div>
            </aside>
            
        </div>
    </div>

    <footer class="footer">
    <div class="footer-wave">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1440 320"><path fill="#3c4859" fill-opacity="1" d="M0,160L60,181.3C120,203,240,245,360,240C480,235,600,181,720,186.7C840,192,960,256,1080,261.3C1200,267,1320,213,1380,186.7L1440,160L1440,320L1380,320C1320,320,1200,320,1080,320C960,320,840,320,720,320C600,320,480,320,360,320C240,320,120,320,60,320L0,320Z"></path></svg>
    </div>

    <!-- Please do not remove this -->
    <!-- 开源不易，请勿删除 -->
    <div class="footer-wrap">
        <div class="footer-inner"> 
            Seeyourmind &copy; 2022<br>
            Powered By Hexo · Theme By <a href="https://linhong.me/" target="_blank">Aomori</a> · <a href="https://github.com/lh1me/hexo-theme-aomori" target="_blank">Github</a>
        </div>
    </div>

</footer>


<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>





<script src="/dist/build.js?1646451311888.js"></script>


<script src="/dist/custom.js?1646451311888.js"></script>













</body>

</html>